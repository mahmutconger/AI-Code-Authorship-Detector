{
    "code": "from typing import Any, Coroutine, List, Optional, Tuple\nimport asyncio\nimport os\nfrom datetime import datetime\nimport time\nfrom llm import Llm\nfrom prompts.types import Stack\nfrom .core import generate_code_for_image\nfrom .utils import image_to_data_url\nfrom .config import EVALS_DIR\n\n\nasync def generate_code_and_time(\n    image_url: str,\n    stack: Stack,\n    model: Llm,\n    original_input_filename: str,\n    attempt_idx: int,\n) -> Tuple[str, int, Optional[str], Optional[float], Optional[Exception]]:\n    \"\"\"\n    Generates code for an image, measures the time taken, and returns identifiers\n    along with success/failure status.\n    Returns a tuple: (original_input_filename, attempt_idx, content, duration, error_object)\n    content and duration are None if an error occurs during generation.\n    \"\"\"\n    start_time = time.perf_counter()\n    try:\n        content = await generate_code_for_image(\n            image_url=image_url, stack=stack, model=model\n        )\n        end_time = time.perf_counter()\n        duration = end_time - start_time\n        return original_input_filename, attempt_idx, content, duration, None\n    except Exception as e:\n        print(\n            f\"Error during code generation for {original_input_filename} (attempt {attempt_idx}): {e}\"\n        )\n        return original_input_filename, attempt_idx, None, None, e\n\n\nasync def run_image_evals(\n    stack: Optional[Stack] = None, \n    model: Optional[str] = None, \n    n: int = 1,\n    input_files: Optional[List[str]] = None\n) -> List[str]:\n    INPUT_DIR = EVALS_DIR + \"/inputs\"\n    OUTPUT_DIR = EVALS_DIR + \"/outputs\"\n\n    # Get all evaluation image files\n    if input_files and len(input_files) > 0:\n        # Use the explicitly provided file list\n        evals = [os.path.basename(f) for f in input_files if f.endswith(\".png\")]\n    else:\n        # Use all PNG files from the input directory\n        evals = [f for f in os.listdir(INPUT_DIR) if f.endswith(\".png\")]\n\n    if not stack:\n        raise ValueError(\"No stack was provided\")\n    if not model:\n        raise ValueError(\"No model was provided\")\n\n    print(\"User selected stack:\", stack)\n    print(\"User selected model:\", model)\n    selected_model = Llm(model)\n    print(f\"Running evals for {selected_model.value} model\")\n    \n    if input_files and len(input_files) > 0:\n        print(f\"Running on {len(evals)} selected files\")\n    else:\n        print(f\"Running on all {len(evals)} files in {INPUT_DIR}\")\n\n    today = datetime.now().strftime(\"%b_%d_%Y\")\n    output_subfolder = os.path.join(\n        OUTPUT_DIR, f\"{today}_{selected_model.value}_{stack}\"\n    )\n    os.makedirs(output_subfolder, exist_ok=True)\n\n    task_coroutines: List[\n        Coroutine[\n            Any,\n            Any,\n            Tuple[str, int, Optional[str], Optional[float], Optional[Exception]],\n        ]\n    ] = []\n    for original_filename in evals:\n        # Handle both full paths and relative filenames\n        if os.path.isabs(original_filename):\n            filepath = original_filename\n            original_filename = os.path.basename(original_filename)\n        else:\n            filepath = os.path.join(INPUT_DIR, original_filename)\n            \n        data_url = await image_to_data_url(filepath)\n        for n_idx in range(n):\n            current_model_for_task = (\n                selected_model if n_idx == 0 else Llm.GPT_4O_2024_05_13\n            )\n            coro = generate_code_and_time(\n                image_url=data_url,\n                stack=stack,\n                model=current_model_for_task,\n                original_input_filename=original_filename,\n                attempt_idx=n_idx,\n            )\n            task_coroutines.append(coro)\n\n    print(f\"Processing {len(task_coroutines)} tasks...\")\n\n    output_files: List[str] = []\n    timing_data: List[str] = []\n    failed_tasks_log: List[str] = []\n\n    for future in asyncio.as_completed(task_coroutines):\n        try:\n            task_orig_fn, task_attempt_idx, generated_content, time_taken, error_obj = (\n                await future\n            )\n\n            output_html_filename_base = os.path.splitext(task_orig_fn)[0]\n            final_output_html_filename = (\n                f\"{output_html_filename_base}_{task_attempt_idx}.html\"\n            )\n            output_html_filepath = os.path.join(\n                output_subfolder, final_output_html_filename\n            )\n\n            if error_obj is not None:\n                failed_tasks_log.append(\n                    f\"Input: {task_orig_fn}, Attempt: {task_attempt_idx}, OutputFile: {final_output_html_filename}, Error: Generation failed - {str(error_obj)}\"\n                )\n            elif generated_content is not None and time_taken is not None:\n                try:\n                    with open(output_html_filepath, \"w\") as file:\n                        file.write(generated_content)\n                    timing_data.append(\n                        f\"{final_output_html_filename}: {time_taken:.2f} seconds\"\n                    )\n                    output_files.append(final_output_html_filename)\n                    print(\n                        f\"Successfully processed and wrote {final_output_html_filename}\"\n                    )\n                except Exception as e_write:\n                    failed_tasks_log.append(\n                        f\"Input: {task_orig_fn}, Attempt: {task_attempt_idx}, OutputFile: {final_output_html_filename}, Error: Writing to file failed - {str(e_write)}\"\n                    )\n            else:\n                failed_tasks_log.append(\n                    f\"Input: {task_orig_fn}, Attempt: {task_attempt_idx}, OutputFile: {final_output_html_filename}, Error: Unknown issue - content or time_taken is None without explicit error.\"\n                )\n\n        except Exception as e_as_completed:\n            print(f\"A task in as_completed failed unexpectedly: {e_as_completed}\")\n            failed_tasks_log.append(\n                f\"Critical Error: A task processing failed - {str(e_as_completed)}\"\n            )\n\n    # Write timing data for successful tasks\n    if timing_data:\n        timing_file_path = os.path.join(output_subfolder, \"generation_times.txt\")\n        try:\n            is_new_or_empty_file = (\n                not os.path.exists(timing_file_path)\n                or os.path.getsize(timing_file_path) == 0\n            )\n\n            with open(timing_file_path, \"a\") as file:\n                if is_new_or_empty_file:\n                    file.write(f\"Model: {selected_model.value}\\n\")\n                elif timing_data:\n                    file.write(\"\\n\")\n\n                file.write(\"\\n\".join(timing_data))\n            print(f\"Timing data saved to {timing_file_path}\")\n        except Exception as e:\n            print(f\"Error writing timing file {timing_file_path}: {e}\")\n\n    # Write log for failed tasks\n    if failed_tasks_log:\n        failed_log_path = os.path.join(output_subfolder, \"failed_tasks.txt\")\n        try:\n            with open(failed_log_path, \"w\") as file:\n                file.write(\"\\n\".join(failed_tasks_log))\n            print(f\"Failed tasks log saved to {failed_log_path}\")\n        except Exception as e:\n            print(f\"Error writing failed tasks log {failed_log_path}: {e}\")\n\n    return output_files",
    "source": "github_repo:abi/screenshot-to-code",
    "file": "backend/evals/runner.py",
    "license": "MIT",
    "language": "python"
}