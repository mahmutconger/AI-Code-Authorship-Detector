{
    "code": "def generate(\n    model: Transformer,\n    prompt_tokens: List[List[int]],\n    max_new_tokens: int,\n    eos_id: int,\n    temperature: float = 1.0\n) -> List[List[int]]:\n    \"\"\"\n    Generates new tokens based on the given prompt tokens using the specified model.\n\n    Args:\n        model (Transformer): The transformer model used for token generation.\n        prompt_tokens (List[List[int]]): A list of lists containing the prompt tokens for each sequence.\n        max_new_tokens (int): The maximum number of new tokens to generate.\n        eos_id (int): The end-of-sequence token ID.\n        temperature (float, optional): The temperature value for sampling. Defaults to 1.0.\n\n    Returns:\n        List[List[int]]: A list of lists containing the generated tokens for each sequence.\n    \"\"\"\n    prompt_lens = [len(t) for t in prompt_tokens]\n    assert max(prompt_lens) <= model.max_seq_len, f\"Prompt length exceeds model maximum sequence length (max_seq_len={model.max_seq_len})\"\n    total_len = min(model.max_seq_len, max_new_tokens + max(prompt_lens))\n    tokens = torch.full((len(prompt_tokens), total_len), -1, dtype=torch.long, device=\"cuda\")\n    for i, t in enumerate(prompt_tokens):\n        tokens[i, :len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n    prev_pos = 0\n    finished = torch.tensor([False] * len(prompt_tokens), device=\"cuda\")\n    prompt_mask = tokens != -1\n    for cur_pos in range(min(prompt_lens), total_len):\n        logits = model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n        if temperature > 0:\n            next_token = sample(logits, temperature)\n        else:\n            next_token = logits.argmax(dim=-1)\n        next_token = torch.where(prompt_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n        tokens[:, cur_pos] = next_token\n        finished |= torch.logical_and(~prompt_mask[:, cur_pos], next_token == eos_id)\n        prev_pos = cur_pos\n        if finished.all():\n            break\n    completion_tokens = []\n    for i, toks in enumerate(tokens.tolist()):\n        toks = toks[prompt_lens[i]:prompt_lens[i]+max_new_tokens]\n        if eos_id in toks:\n            toks = toks[:toks.index(eos_id)]\n        completion_tokens.append(toks)\n    return completion_tokens",
    "source": "github_repo:deepseek-ai/DeepSeek-V3",
    "file": "inference/generate.py",
    "license": "MIT",
    "language": "python"
}