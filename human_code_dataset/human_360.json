{
    "code": "def test_split_on_unicode():\n    multilingual_tokenizer = get_tokenizer(multilingual=True)\n\n    tokens = [8404, 871, 287, 6, 246, 526, 3210, 20378]\n    words, word_tokens = multilingual_tokenizer.split_tokens_on_unicode(tokens)\n\n    assert words == [\" elle\", \" est\", \" l\", \"'\", \"\\ufffd\", \"Ã©\", \"rit\", \"oire\"]\n    assert word_tokens == [[8404], [871], [287], [6], [246], [526], [3210], [20378]]",
    "source": "github_repo:openai/whisper",
    "file": "tests/test_tokenizer.py",
    "license": "MIT",
    "language": "python"
}