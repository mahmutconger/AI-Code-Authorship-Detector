{
    "code": "class DistributedHyperparamOptManager(HyperparamOptManager):\n    \"\"\"Manages distributed hyperparameter optimisation across many gpus.\"\"\"\n\n    def __init__(\n        self,\n        param_ranges,\n        fixed_params,\n        root_model_folder,\n        worker_number,\n        search_iterations=1000,\n        num_iterations_per_worker=5,\n        clear_serialised_params=False,\n    ):\n        \"\"\"Instantiates optimisation manager.\n\n        This hyperparameter optimisation pre-generates #search_iterations\n        hyperparameter combinations and serialises them\n        at the start. At runtime, each worker goes through their own set of\n        parameter ranges. The pregeneration\n        allows for multiple workers to run in parallel on different machines without\n        resulting in parameter overlaps.\n\n        Args:\n          param_ranges: Discrete hyperparameter range for random search.\n          fixed_params: Fixed model parameters per experiment.\n          root_model_folder: Folder to store optimisation artifacts.\n          worker_number: Worker index defining which set of hyperparameters to\n            test.\n          search_iterations: Maximum number of random search iterations.\n          num_iterations_per_worker: How many iterations are handled per worker.\n          clear_serialised_params: Whether to regenerate hyperparameter\n            combinations.\n        \"\"\"\n\n        max_workers = int(np.ceil(search_iterations / num_iterations_per_worker))\n\n        # Sanity checks\n        if worker_number > max_workers:\n            raise ValueError(\n                \"Worker number ({}) cannot be larger than the total number of workers!\".format(max_workers)\n            )\n        if worker_number > search_iterations:\n            raise ValueError(\n                \"Worker number ({}) cannot be larger than the max search iterations ({})!\".format(\n                    worker_number, search_iterations\n                )\n            )\n\n        print(\"*** Creating hyperparameter manager for worker {} ***\".format(worker_number))\n\n        hyperparam_folder = os.path.join(root_model_folder, str(worker_number))\n        super().__init__(param_ranges, fixed_params, hyperparam_folder, override_w_fixed_params=True)\n\n        serialised_ranges_folder = os.path.join(root_model_folder, \"hyperparams\")\n        if clear_serialised_params:\n            print(\"Regenerating hyperparameter list\")\n            if os.path.exists(serialised_ranges_folder):\n                shutil.rmtree(serialised_ranges_folder)\n\n        utils.create_folder_if_not_exist(serialised_ranges_folder)\n\n        self.serialised_ranges_path = os.path.join(serialised_ranges_folder, \"ranges_{}.csv\".format(search_iterations))\n        self.hyperparam_folder = hyperparam_folder  # override\n        self.worker_num = worker_number\n        self.total_search_iterations = search_iterations\n        self.num_iterations_per_worker = num_iterations_per_worker\n        self.global_hyperparam_df = self.load_serialised_hyperparam_df()\n        self.worker_search_queue = self._get_worker_search_queue()\n\n    @property\n    def optimisation_completed(self):\n        return False if self.worker_search_queue else True\n\n    def get_next_parameters(self):\n        \"\"\"Returns next dictionary of hyperparameters to optimise.\"\"\"\n        param_name = self.worker_search_queue.pop()\n\n        params = self.global_hyperparam_df.loc[param_name, :].to_dict()\n\n        # Always override!\n        for k in self.fixed_params:\n            print(\"Overriding saved {}: {}\".format(k, self.fixed_params[k]))\n\n            params[k] = self.fixed_params[k]\n\n        return params\n\n    def load_serialised_hyperparam_df(self):\n        \"\"\"Loads serialsed hyperparameter ranges from file.\n\n        Returns:\n          DataFrame containing hyperparameter combinations.\n        \"\"\"\n        print(\n            \"Loading params for {} search iterations form {}\".format(\n                self.total_search_iterations, self.serialised_ranges_path\n            )\n        )\n\n        if os.path.exists(self.serialised_ranges_folder):\n            df = pd.read_csv(self.serialised_ranges_path, index_col=0)\n        else:\n            print(\"Unable to load - regenerating search ranges instead\")\n            df = self.update_serialised_hyperparam_df()\n\n        return df\n\n    def update_serialised_hyperparam_df(self):\n        \"\"\"Regenerates hyperparameter combinations and saves to file.\n\n        Returns:\n          DataFrame containing hyperparameter combinations.\n        \"\"\"\n        search_df = self._generate_full_hyperparam_df()\n\n        print(\n            \"Serialising params for {} search iterations to {}\".format(\n                self.total_search_iterations, self.serialised_ranges_path\n            )\n        )\n\n        search_df.to_csv(self.serialised_ranges_path)\n\n        return search_df\n\n    def _generate_full_hyperparam_df(self):\n        \"\"\"Generates actual hyperparameter combinations.\n\n        Returns:\n          DataFrame containing hyperparameter combinations.\n        \"\"\"\n\n        np.random.seed(131)  # for reproducibility of hyperparam list\n\n        name_list = []\n        param_list = []\n        for _ in range(self.total_search_iterations):\n            params = super().get_next_parameters(name_list)\n\n            name = self._get_name(params)\n\n            name_list.append(name)\n            param_list.append(params)\n\n        full_search_df = pd.DataFrame(param_list, index=name_list)\n\n        return full_search_df\n\n    def clear(self):  # reset when cleared\n        \"\"\"Clears results for hyperparameter manager and resets.\"\"\"\n        super().clear()\n        self.worker_search_queue = self._get_worker_search_queue()\n\n    def load_results(self):\n        \"\"\"Load results from file and queue parameter combinations to try.\n\n        Returns:\n          Boolean indicating if results were successfully loaded.\n        \"\"\"\n        success = super().load_results()\n\n        if success:\n            self.worker_search_queue = self._get_worker_search_queue()\n\n        return success\n\n    def _get_worker_search_queue(self):\n        \"\"\"Generates the queue of param combinations for current worker.\n\n        Returns:\n          Queue of hyperparameter combinations outstanding.\n        \"\"\"\n        global_df = self.assign_worker_numbers(self.global_hyperparam_df)\n        worker_df = global_df[global_df[\"worker\"] == self.worker_num]\n\n        left_overs = [s for s in worker_df.index if s not in self.results.columns]\n\n        return Deque(left_overs)\n\n    def assign_worker_numbers(self, df):\n        \"\"\"Updates parameter combinations with the index of the worker used.\n\n        Args:\n          df: DataFrame of parameter combinations.\n\n        Returns:\n          Updated DataFrame with worker number.\n        \"\"\"\n        output = df.copy()\n\n        n = self.total_search_iterations\n        batch_size = self.num_iterations_per_worker\n\n        max_worker_num = int(np.ceil(n / batch_size))\n\n        worker_idx = np.concatenate([np.tile(i + 1, self.num_iterations_per_worker) for i in range(max_worker_num)])\n\n        output[\"worker\"] = worker_idx[: len(output)]\n\n        return output",
    "source": "github_repo:microsoft/qlib",
    "file": "examples/benchmarks/TFT/libs/hyperparam_opt.py",
    "license": "MIT",
    "language": "python"
}