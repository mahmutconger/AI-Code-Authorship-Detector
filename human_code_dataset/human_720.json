{
    "code": "class Squash(nn.Module):\n    \"\"\"\n    ## Squash\n\n    This is **squashing** function from paper, given by equation $(1)$.\n\n    $$\\mathbf{v}_j = \\frac{{\\lVert \\mathbf{s}_j \\rVert}^2}{1 + {\\lVert \\mathbf{s}_j \\rVert}^2}\n     \\frac{\\mathbf{s}_j}{\\lVert \\mathbf{s}_j \\rVert}$$\n\n    $\\frac{\\mathbf{s}_j}{\\lVert \\mathbf{s}_j \\rVert}$\n    normalizes the length of all the capsules, whilst\n    $\\frac{{\\lVert \\mathbf{s}_j \\rVert}^2}{1 + {\\lVert \\mathbf{s}_j \\rVert}^2}$\n    shrinks the capsules that have a length smaller than one .\n    \"\"\"\n\n    def __init__(self, epsilon=1e-8):\n        super().__init__()\n        self.epsilon = epsilon\n\n    def forward(self, s: torch.Tensor):\n        \"\"\"\n        The shape of `s` is `[batch_size, n_capsules, n_features]`\n        \"\"\"\n\n        # ${\\lVert \\mathbf{s}_j \\rVert}^2$\n        s2 = (s ** 2).sum(dim=-1, keepdims=True)\n\n        # We add an epsilon when calculating $\\lVert \\mathbf{s}_j \\rVert$ to make sure it doesn't become zero.\n        # If this becomes zero it starts giving out `nan` values and training fails.\n        # $$\\mathbf{v}_j = \\frac{{\\lVert \\mathbf{s}_j \\rVert}^2}{1 + {\\lVert \\mathbf{s}_j \\rVert}^2}\n        # \\frac{\\mathbf{s}_j}{\\sqrt{{\\lVert \\mathbf{s}_j \\rVert}^2 + \\epsilon}}$$\n        return (s2 / (1 + s2)) * (s / torch.sqrt(s2 + self.epsilon))",
    "source": "github_repo:labmlai/annotated_deep_learning_paper_implementations",
    "file": "labml_nn/capsule_networks/__init__.py",
    "license": "MIT",
    "language": "python"
}