{
    "code": "def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: list[str] | None = None,\n        run_manager: CallbackManagerForLLMRun | None = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        \"\"\"Override the _generate method to implement the chat model logic.\n\n        This can be a call to an API, a call to a local model, or any other\n        implementation that generates a response to the input prompt.\n\n        Args:\n            messages: the prompt composed of a list of messages.\n            stop: a list of strings on which the model should stop generating.\n                If generation stops due to a stop token, the stop token itself\n                SHOULD BE INCLUDED as part of the output. This is not enforced\n                across models right now, but it's a good practice to follow since\n                it makes it much easier to parse the output of the model\n                downstream and understand why generation stopped.\n            run_manager: A run manager with callbacks for the LLM.\n        \"\"\"\n        # Replace this with actual logic to generate a response from a list\n        # of messages.\n        last_message = messages[-1]\n        tokens = last_message.content[: self.parrot_buffer_length]\n        ct_input_tokens = sum(len(message.content) for message in messages)\n        ct_output_tokens = len(tokens)\n        message = AIMessage(\n            content=tokens,\n            additional_kwargs={},  # Used to add additional payload to the message\n            response_metadata={  # Use for response metadata\n                \"time_in_seconds\": 3,\n                \"model_name\": self.model_name,\n            },\n            usage_metadata={\n                \"input_tokens\": ct_input_tokens,\n                \"output_tokens\": ct_output_tokens,\n                \"total_tokens\": ct_input_tokens + ct_output_tokens,\n            },\n        )\n        ##\n\n        generation = ChatGeneration(message=message)\n        return ChatResult(generations=[generation])",
    "source": "github_repo:langchain-ai/langchain",
    "file": "libs/cli/langchain_cli/integration_template/integration_template/chat_models.py",
    "license": "MIT",
    "language": "python"
}