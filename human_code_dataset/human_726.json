{
    "code": "def forward(self, u: torch.Tensor):\n        \"\"\"\n        The shape of `u` is `[batch_size, n_capsules, n_features]`.\n        These are the capsules from the lower layer.\n        \"\"\"\n\n        # $$\\hat{\\mathbf{u}}_{j|i} = \\mathbf{W}_{ij} \\mathbf{u}_i$$\n        # Here $j$ is used to index capsules in this layer, whilst $i$ is\n        # used to index capsules in the layer below (previous).\n        u_hat = torch.einsum('ijnm,bin->bijm', self.weight, u)\n\n        # Initial logits $b_{ij}$ are the log prior probabilities that capsule $i$\n        # should be coupled with $j$.\n        # We initialize these at zero\n        b = u.new_zeros(u.shape[0], self.in_caps, self.out_caps)\n\n        v = None\n\n        # Iterate\n        for i in range(self.iterations):\n            # routing softmax $$c_{ij} = \\frac{\\exp({b_{ij}})}{\\sum_k\\exp({b_{ik}})}$$\n            c = self.softmax(b)\n            # $$\\mathbf{s}_j = \\sum_i{c_{ij} \\hat{\\mathbf{u}}_{j|i}}$$\n            s = torch.einsum('bij,bijm->bjm', c, u_hat)\n            # $$\\mathbf{v}_j = squash(\\mathbf{s}_j)$$\n            v = self.squash(s)\n            # $$a_{ij} = \\mathbf{v}_j \\cdot \\hat{\\mathbf{u}}_{j|i}$$\n            a = torch.einsum('bjm,bijm->bij', v, u_hat)\n            # $$b_{ij} \\gets b_{ij} + \\mathbf{v}_j \\cdot \\hat{\\mathbf{u}}_{j|i}$$\n            b = b + a\n\n        return v",
    "source": "github_repo:labmlai/annotated_deep_learning_paper_implementations",
    "file": "labml_nn/capsule_networks/__init__.py",
    "license": "MIT",
    "language": "python"
}