{
    "code": "import asyncio\n\nfrom llama_index.core.llama_dataset import download_llama_dataset\nfrom llama_index.core.llama_pack import download_llama_pack\nfrom llama_index.core import VectorStoreIndex\n\n\nasync def main():\n    # DOWNLOAD LLAMADATASET\n    rag_dataset, documents = download_llama_dataset(\n        \"BraintrustCodaHelpDeskDataset\", \"./braintrust_codahdd\"\n    )\n\n    # BUILD BASIC RAG PIPELINE\n    index = VectorStoreIndex.from_documents(documents=documents)\n    query_engine = index.as_query_engine()\n\n    # EVALUATE WITH PACK\n    RagEvaluatorPack = download_llama_pack(\"RagEvaluatorPack\", \"./pack_stuff\")\n    rag_evaluator = RagEvaluatorPack(query_engine=query_engine, rag_dataset=rag_dataset)\n\n    ############################################################################\n    # NOTE: If have a lower tier subscription for OpenAI API like Usage Tier 1 #\n    # then you'll need to use different batch_size and sleep_time_in_seconds.  #\n    # For Usage Tier 1, settings that seemed to work well were batch_size=5,   #\n    # and sleep_time_in_seconds=15 (as of December 2023.)                      #\n    ############################################################################\n    benchmark_df = await rag_evaluator.arun(\n        batch_size=20,  # batches the number of openai api calls to make\n        sleep_time_in_seconds=1,  # number of seconds sleep before making an api call\n    )\n    print(benchmark_df)\n\n\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main)",
    "source": "github_repo:run-llama/llama_index",
    "file": "llama-datasets/braintrust_coda/llamaindex_baseline.py",
    "license": "MIT",
    "language": "python"
}