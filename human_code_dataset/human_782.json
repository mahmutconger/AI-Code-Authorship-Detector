{
    "code": "def eval(self, id: str, result: str) -> Tuple[str, bool]:\n        \"\"\"\n        Evaluate the prediction against the true label.\n\n        Args:\n            id (str): The identifier for the question.\n            result (str): The original prediction result.\n\n        Returns:\n            Tuple[str, bool]: A tuple containing the final prediction and a boolean indicating\n                              whether it matches the true label.\n        \"\"\"\n        true_label = self.get_answer(id)[\"common_answers\"]  # Retrieve the true label for comparison\n        nest_asyncio.apply()  # Apply nested asyncio to allow for async calls\n        result = json.loads(str(result).split(\"Current Plan\")[1].split(\"## Current Task\")[0])[-1][\"result\"].strip()\n        cleaned_prediction = result.replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\")  # Clean the prediction string\n\n        # Use the decorated function to handle exceptions while parsing the cleaned prediction\n        parsed_result = self.parse_cleaned_prediction(cleaned_prediction, true_label)\n        if parsed_result[1]:  # If the parsed prediction is valid\n            return parsed_result  # Return the valid prediction\n\n        # If the cleaned prediction is not valid, attempt to asynchronously reformat it\n        prediction = self.async_reformat_prediction(id, result)\n\n        pred_dict = parse_prediction(prediction)  # Parse the reformatted prediction\n        if pred_dict is not None and compare_predictions(pred_dict, true_label):\n            return prediction, True  # Return if the reformatted prediction matches the true label\n\n        return prediction, False  # Return the final prediction with a False match",
    "source": "github_repo:FoundationAgents/MetaGPT",
    "file": "examples/di/InfiAgent-DABench/DABench.py",
    "license": "MIT",
    "language": "python"
}