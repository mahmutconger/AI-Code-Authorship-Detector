{
    "code": "def evaluate(\n    dataset,\n    embed_model,\n    top_k=5,\n    verbose=False,\n):\n    corpus = dataset.corpus\n    queries = dataset.queries\n    relevant_docs = dataset.relevant_docs\n\n    embed_model = embed_model or Settings.embed_model\n    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n    index = VectorStoreIndex(\n        nodes, embed_model=embed_model, show_progress=True\n    )\n    retriever = index.as_retriever(similarity_top_k=top_k)\n\n    eval_results = []\n    for query_id, query in tqdm(queries.items()):\n        retrieved_nodes = retriever.retrieve(query)\n        retrieved_ids = [node.node.node_id for node in retrieved_nodes]\n        expected_id = relevant_docs[query_id][0]\n\n        rank = None\n        for idx, id in enumerate(retrieved_ids):\n            if id == expected_id:\n                rank = idx + 1\n                break\n\n        is_hit = rank is not None  # assume 1 relevant doc\n        mrr = 0 if rank is None else 1 / rank\n\n        eval_result = {\n            \"is_hit\": is_hit,\n            \"mrr\": mrr,\n            \"retrieved\": retrieved_ids,\n            \"expected\": expected_id,\n            \"query\": query_id,\n        }\n        eval_results.append(eval_result)\n    return eval_results",
    "source": "github_repo:run-llama/llama_index",
    "file": "docs/examples/finetuning/embeddings/eval_utils.py",
    "license": "MIT",
    "language": "python"
}