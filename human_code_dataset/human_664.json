{
    "code": "def main():\n    parser = ArgumentParser(\n        formatter_class=RawDescriptionHelpFormatter,\n        description=f\"{__longname__} (Version {__version__})\",\n    )\n    parser.add_argument(\n        \"--version\",\n        action=\"version\",\n        version=f\"{__shortname__} v{__version__}\",\n        help=\"Display version information and dependencies.\",\n    )\n    parser.add_argument(\n        \"--verbose\",\n        \"-v\",\n        \"-d\",\n        \"--debug\",\n        action=\"store_true\",\n        dest=\"verbose\",\n        default=False,\n        help=\"Display extra debugging information and metrics.\",\n    )\n    parser.add_argument(\n        \"--folderoutput\",\n        \"-fo\",\n        dest=\"folderoutput\",\n        help=\"If using multiple usernames, the output of the results will be saved to this folder.\",\n    )\n    parser.add_argument(\n        \"--output\",\n        \"-o\",\n        dest=\"output\",\n        help=\"If using single username, the output of the result will be saved to this file.\",\n    )\n    parser.add_argument(\n        \"--csv\",\n        action=\"store_true\",\n        dest=\"csv\",\n        default=False,\n        help=\"Create Comma-Separated Values (CSV) File.\",\n    )\n    parser.add_argument(\n        \"--xlsx\",\n        action=\"store_true\",\n        dest=\"xlsx\",\n        default=False,\n        help=\"Create the standard file for the modern Microsoft Excel spreadsheet (xlsx).\",\n    )\n    parser.add_argument(\n        \"--site\",\n        action=\"append\",\n        metavar=\"SITE_NAME\",\n        dest=\"site_list\",\n        default=[],\n        help=\"Limit analysis to just the listed sites. Add multiple options to specify more than one site.\",\n    )\n    parser.add_argument(\n        \"--proxy\",\n        \"-p\",\n        metavar=\"PROXY_URL\",\n        action=\"store\",\n        dest=\"proxy\",\n        default=None,\n        help=\"Make requests over a proxy. e.g. socks5://127.0.0.1:1080\",\n    )\n    parser.add_argument(\n        \"--dump-response\",\n        action=\"store_true\",\n        dest=\"dump_response\",\n        default=False,\n        help=\"Dump the HTTP response to stdout for targeted debugging.\",\n    )\n    parser.add_argument(\n        \"--json\",\n        \"-j\",\n        metavar=\"JSON_FILE\",\n        dest=\"json_file\",\n        default=None,\n        help=\"Load data from a JSON file or an online, valid, JSON file. Upstream PR numbers also accepted.\",\n    )\n    parser.add_argument(\n        \"--timeout\",\n        action=\"store\",\n        metavar=\"TIMEOUT\",\n        dest=\"timeout\",\n        type=timeout_check,\n        default=60,\n        help=\"Time (in seconds) to wait for response to requests (Default: 60)\",\n    )\n    parser.add_argument(\n        \"--print-all\",\n        action=\"store_true\",\n        dest=\"print_all\",\n        default=False,\n        help=\"Output sites where the username was not found.\",\n    )\n    parser.add_argument(\n        \"--print-found\",\n        action=\"store_true\",\n        dest=\"print_found\",\n        default=True,\n        help=\"Output sites where the username was found (also if exported as file).\",\n    )\n    parser.add_argument(\n        \"--no-color\",\n        action=\"store_true\",\n        dest=\"no_color\",\n        default=False,\n        help=\"Don't color terminal output\",\n    )\n    parser.add_argument(\n        \"username\",\n        nargs=\"+\",\n        metavar=\"USERNAMES\",\n        action=\"store\",\n        help=\"One or more usernames to check with social networks. Check similar usernames using {?} (replace to '_', '-', '.').\",\n    )\n    parser.add_argument(\n        \"--browse\",\n        \"-b\",\n        action=\"store_true\",\n        dest=\"browse\",\n        default=False,\n        help=\"Browse to all results on default browser.\",\n    )\n\n    parser.add_argument(\n        \"--local\",\n        \"-l\",\n        action=\"store_true\",\n        default=False,\n        help=\"Force the use of the local data.json file.\",\n    )\n\n    parser.add_argument(\n        \"--nsfw\",\n        action=\"store_true\",\n        default=False,\n        help=\"Include checking of NSFW sites from default list.\",\n    )\n\n    # TODO deprecated in favor of --txt, retained for workflow compatibility, to be removed\n    # in future release\n    parser.add_argument(\n        \"--no-txt\",\n        action=\"store_true\",\n        dest=\"no_txt\",\n        default=False,\n        help=\"Disable creation of a txt file - WILL BE DEPRECATED\",\n    )\n\n    parser.add_argument(\n        \"--txt\",\n        action=\"store_true\",\n        dest=\"output_txt\",\n        default=False,\n        help=\"Enable creation of a txt file\",\n    )\n\n    parser.add_argument(\n        \"--ignore-exclusions\",\n        action=\"store_true\",\n        dest=\"ignore_exclusions\",\n        default=False,\n        help=\"Ignore upstream exclusions (may return more false positives)\",\n    )\n\n    args = parser.parse_args()\n\n    # If the user presses CTRL-C, exit gracefully without throwing errors\n    signal.signal(signal.SIGINT, handler)\n\n    # Check for newer version of Sherlock. If it exists, let the user know about it\n    try:\n        latest_release_raw = requests.get(forge_api_latest_release, timeout=10).text\n        latest_release_json = json_loads(latest_release_raw)\n        latest_remote_tag = latest_release_json[\"tag_name\"]\n\n        if latest_remote_tag[1:] != __version__:\n            print(\n                f\"Update available! {__version__} --> {latest_remote_tag[1:]}\"\n                f\"\\n{latest_release_json['html_url']}\"\n            )\n\n    except Exception as error:\n        print(f\"A problem occurred while checking for an update: {error}\")\n\n    # Make prompts\n    if args.proxy is not None:\n        print(\"Using the proxy: \" + args.proxy)\n\n    if args.no_color:\n        # Disable color output.\n        init(strip=True, convert=False)\n    else:\n        # Enable color output.\n        init(autoreset=True)\n\n    # Check if both output methods are entered as input.\n    if args.output is not None and args.folderoutput is not None:\n        print(\"You can only use one of the output methods.\")\n        sys.exit(1)\n\n    # Check validity for single username output.\n    if args.output is not None and len(args.username) != 1:\n        print(\"You can only use --output with a single username\")\n        sys.exit(1)\n\n    # Create object with all information about sites we are aware of.\n    try:\n        if args.local:\n            sites = SitesInformation(\n                os.path.join(os.path.dirname(__file__), \"resources/data.json\"),\n                honor_exclusions=False,\n            )\n        else:\n            json_file_location = args.json_file\n            if args.json_file:\n                # If --json parameter is a number, interpret it as a pull request number\n                if args.json_file.isnumeric():\n                    pull_number = args.json_file\n                    pull_url = f\"https://api.github.com/repos/sherlock-project/sherlock/pulls/{pull_number}\"\n                    pull_request_raw = requests.get(pull_url, timeout=10).text\n                    pull_request_json = json_loads(pull_request_raw)\n\n                    # Check if it's a valid pull request\n                    if \"message\" in pull_request_json:\n                        print(f\"ERROR: Pull request #{pull_number} not found.\")\n                        sys.exit(1)\n\n                    head_commit_sha = pull_request_json[\"head\"][\"sha\"]\n                    json_file_location = f\"https://raw.githubusercontent.com/sherlock-project/sherlock/{head_commit_sha}/sherlock_project/resources/data.json\"\n\n            sites = SitesInformation(\n                data_file_path=json_file_location,\n                honor_exclusions=not args.ignore_exclusions,\n                do_not_exclude=args.site_list,\n            )\n    except Exception as error:\n        print(f\"ERROR:  {error}\")\n        sys.exit(1)\n\n    if not args.nsfw:\n        sites.remove_nsfw_sites(do_not_remove=args.site_list)\n\n    # Create original dictionary from SitesInformation() object.\n    # Eventually, the rest of the code will be updated to use the new object\n    # directly, but this will glue the two pieces together.\n    site_data_all = {site.name: site.information for site in sites}\n    if args.site_list == []:\n        # Not desired to look at a sub-set of sites\n        site_data = site_data_all\n    else:\n        # User desires to selectively run queries on a sub-set of the site list.\n        # Make sure that the sites are supported & build up pruned site database.\n        site_data = {}\n        site_missing = []\n        for site in args.site_list:\n            counter = 0\n            for existing_site in site_data_all:\n                if site.lower() == existing_site.lower():\n                    site_data[existing_site] = site_data_all[existing_site]\n                    counter += 1\n            if counter == 0:\n                # Build up list of sites not supported for future error message.\n                site_missing.append(f\"'{site}'\")\n\n        if site_missing:\n            print(f\"Error: Desired sites not found: {', '.join(site_missing)}.\")\n\n        if not site_data:\n            sys.exit(1)\n\n    # Create notify object for query results.\n    query_notify = QueryNotifyPrint(\n        result=None, verbose=args.verbose, print_all=args.print_all, browse=args.browse\n    )\n\n    # Run report on all specified users.\n    all_usernames = []\n    for username in args.username:\n        if check_for_parameter(username):\n            for name in multiple_usernames(username):\n                all_usernames.append(name)\n        else:\n            all_usernames.append(username)\n    for username in all_usernames:\n        results = sherlock(\n            username,\n            site_data,\n            query_notify,\n            dump_response=args.dump_response,\n            proxy=args.proxy,\n            timeout=args.timeout,\n        )\n\n        if args.output:\n            result_file = args.output\n        elif args.folderoutput:\n            # The usernames results should be stored in a targeted folder.\n            # If the folder doesn't exist, create it first\n            os.makedirs(args.folderoutput, exist_ok=True)\n            result_file = os.path.join(args.folderoutput, f\"{username}.txt\")\n        else:\n            result_file = f\"{username}.txt\"\n\n        if args.output_txt:\n            with open(result_file, \"w\", encoding=\"utf-8\") as file:\n                exists_counter = 0\n                for website_name in results:\n                    dictionary = results[website_name]\n                    if dictionary.get(\"status\").status == QueryStatus.CLAIMED:\n                        exists_counter += 1\n                        file.write(dictionary[\"url_user\"] + \"\\n\")\n                file.write(f\"Total Websites Username Detected On : {exists_counter}\\n\")\n\n        if args.csv:\n            result_file = f\"{username}.csv\"\n            if args.folderoutput:\n                # The usernames results should be stored in a targeted folder.\n                # If the folder doesn't exist, create it first\n                os.makedirs(args.folderoutput, exist_ok=True)\n                result_file = os.path.join(args.folderoutput, result_file)\n\n            with open(result_file, \"w\", newline=\"\", encoding=\"utf-8\") as csv_report:\n                writer = csv.writer(csv_report)\n                writer.writerow(\n                    [\n                        \"username\",\n                        \"name\",\n                        \"url_main\",\n                        \"url_user\",\n                        \"exists\",\n                        \"http_status\",\n                        \"response_time_s\",\n                    ]\n                )\n                for site in results:\n                    if (\n                        args.print_found\n                        and not args.print_all\n                        and results[site][\"status\"].status != QueryStatus.CLAIMED\n                    ):\n                        continue\n\n                    response_time_s = results[site][\"status\"].query_time\n                    if response_time_s is None:\n                        response_time_s = \"\"\n                    writer.writerow(\n                        [\n                            username,\n                            site,\n                            results[site][\"url_main\"],\n                            results[site][\"url_user\"],\n                            str(results[site][\"status\"].status),\n                            results[site][\"http_status\"],\n                            response_time_s,\n                        ]\n                    )\n        if args.xlsx:\n            usernames = []\n            names = []\n            url_main = []\n            url_user = []\n            exists = []\n            http_status = []\n            response_time_s = []\n\n            for site in results:\n                if (\n                    args.print_found\n                    and not args.print_all\n                    and results[site][\"status\"].status != QueryStatus.CLAIMED\n                ):\n                    continue\n\n                if response_time_s is None:\n                    response_time_s.append(\"\")\n                else:\n                    response_time_s.append(results[site][\"status\"].query_time)\n                usernames.append(username)\n                names.append(site)\n                url_main.append(results[site][\"url_main\"])\n                url_user.append(results[site][\"url_user\"])\n                exists.append(str(results[site][\"status\"].status))\n                http_status.append(results[site][\"http_status\"])\n\n            DataFrame = pd.DataFrame(\n                {\n                    \"username\": usernames,\n                    \"name\": names,\n                    \"url_main\": [f'=HYPERLINK(\\\"{u}\\\")' for u in url_main],\n                    \"url_user\": [f'=HYPERLINK(\\\"{u}\\\")' for u in url_user],\n                    \"exists\": exists,\n                    \"http_status\": http_status,\n                    \"response_time_s\": response_time_s,\n                }\n            )\n            DataFrame.to_excel(f\"{username}.xlsx\", sheet_name=\"sheet1\", index=False)\n\n        print()\n    query_notify.finish()",
    "source": "github_repo:sherlock-project/sherlock",
    "file": "sherlock_project/sherlock.py",
    "license": "MIT",
    "language": "python"
}