{
    "code": "def __iter__(self) -> Iterator[T_co]:\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            random.seed(self.epoch + self.seed)\n            shuffled_bucket = []\n            for buc in self.id_buckets:\n                buc_copy = buc.copy()\n                shuffle(buc_copy)\n                shuffled_bucket.append(buc_copy)\n            grouped_batch_size = self.batch_size * self.num_replicas\n            shuffled_bucket = list(itertools.chain(*shuffled_bucket))\n            n_batch = int(math.ceil(len(shuffled_bucket) / grouped_batch_size))\n            batches = [shuffled_bucket[b * grouped_batch_size : (b + 1) * grouped_batch_size] for b in range(n_batch)]\n            shuffle(batches)\n            indices = list(itertools.chain(*batches))\n        else:\n            # type: ignore[arg-type]\n            indices = list(range(len(self.dataset)))\n\n        if not self.drop_last:\n            # add extra samples to make it evenly divisible\n            padding_size = self.total_size - len(indices)\n            if padding_size <= len(indices):\n                indices += indices[:padding_size]\n            else:\n                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]\n        else:\n            # remove tail of data to make it evenly divisible.\n            indices = indices[: self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)",
    "source": "github_repo:RVC-Boss/GPT-SoVITS",
    "file": "GPT_SoVITS/AR/data/bucket_sampler.py",
    "license": "MIT",
    "language": "python"
}