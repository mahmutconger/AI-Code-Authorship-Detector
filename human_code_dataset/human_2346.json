{
    "code": "class Alpha158Formatter(GenericDataFormatter):\n    \"\"\"Defines and formats data for the Alpha158 dataset.\n\n    Attributes:\n      column_definition: Defines input and data type of column used in the\n        experiment.\n      identifiers: Entity identifiers used in experiments.\n    \"\"\"\n\n    _column_definition = [\n        (\"instrument\", DataTypes.CATEGORICAL, InputTypes.ID),\n        (\"LABEL0\", DataTypes.REAL_VALUED, InputTypes.TARGET),\n        (\"date\", DataTypes.DATE, InputTypes.TIME),\n        (\"month\", DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),\n        (\"day_of_week\", DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),\n        # Selected features\n        (\"RESI5\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"WVMA5\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"RSQR5\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"KLEN\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"RSQR10\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"CORR5\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"CORD5\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"CORR10\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"ROC60\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"RESI10\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"VSTD5\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"RSQR60\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"CORR60\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"WVMA60\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"STD5\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"RSQR20\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"CORD60\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"CORD10\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"CORR20\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"KLOW\", DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n        (\"const\", DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n    ]\n\n    def __init__(self):\n        \"\"\"Initialises formatter.\"\"\"\n\n        self.identifiers = None\n        self._real_scalers = None\n        self._cat_scalers = None\n        self._target_scaler = None\n        self._num_classes_per_cat_input = None\n\n    def split_data(self, df, valid_boundary=2016, test_boundary=2018):\n        \"\"\"Splits data frame into training-validation-test data frames.\n\n        This also calibrates scaling object, and transforms data for each split.\n\n        Args:\n          df: Source data frame to split.\n          valid_boundary: Starting year for validation data\n          test_boundary: Starting year for test data\n\n        Returns:\n          Tuple of transformed (train, valid, test) data.\n        \"\"\"\n\n        print(\"Formatting train-valid-test splits.\")\n\n        index = df[\"year\"]\n        train = df.loc[index < valid_boundary]\n        valid = df.loc[(index >= valid_boundary) & (index < test_boundary)]\n        test = df.loc[index >= test_boundary]\n\n        self.set_scalers(train)\n\n        return (self.transform_inputs(data) for data in [train, valid, test])\n\n    def set_scalers(self, df):\n        \"\"\"Calibrates scalers using the data supplied.\n\n        Args:\n          df: Data to use to calibrate scalers.\n        \"\"\"\n        print(\"Setting scalers with training data...\")\n\n        column_definitions = self.get_column_definition()\n        id_column = utils.get_single_col_by_input_type(InputTypes.ID, column_definitions)\n        target_column = utils.get_single_col_by_input_type(InputTypes.TARGET, column_definitions)\n\n        # Extract identifiers in case required\n        self.identifiers = list(df[id_column].unique())\n\n        # Format real scalers\n        real_inputs = utils.extract_cols_from_data_type(\n            DataTypes.REAL_VALUED, column_definitions, {InputTypes.ID, InputTypes.TIME}\n        )\n\n        data = df[real_inputs].values\n        self._real_scalers = sklearn.preprocessing.StandardScaler().fit(data)\n        self._target_scaler = sklearn.preprocessing.StandardScaler().fit(\n            df[[target_column]].values\n        )  # used for predictions\n\n        # Format categorical scalers\n        categorical_inputs = utils.extract_cols_from_data_type(\n            DataTypes.CATEGORICAL, column_definitions, {InputTypes.ID, InputTypes.TIME}\n        )\n\n        categorical_scalers = {}\n        num_classes = []\n        for col in categorical_inputs:\n            # Set all to str so that we don't have mixed integer/string columns\n            srs = df[col].apply(str)\n            categorical_scalers[col] = sklearn.preprocessing.LabelEncoder().fit(srs.values)\n            num_classes.append(srs.nunique())\n\n        # Set categorical scaler outputs\n        self._cat_scalers = categorical_scalers\n        self._num_classes_per_cat_input = num_classes\n\n    def transform_inputs(self, df):\n        \"\"\"Performs feature transformations.\n\n        This includes both feature engineering, preprocessing and normalisation.\n\n        Args:\n          df: Data frame to transform.\n\n        Returns:\n          Transformed data frame.\n\n        \"\"\"\n        output = df.copy()\n\n        if self._real_scalers is None and self._cat_scalers is None:\n            raise ValueError(\"Scalers have not been set!\")\n\n        column_definitions = self.get_column_definition()\n\n        real_inputs = utils.extract_cols_from_data_type(\n            DataTypes.REAL_VALUED, column_definitions, {InputTypes.ID, InputTypes.TIME}\n        )\n        categorical_inputs = utils.extract_cols_from_data_type(\n            DataTypes.CATEGORICAL, column_definitions, {InputTypes.ID, InputTypes.TIME}\n        )\n\n        # Format real inputs\n        output[real_inputs] = self._real_scalers.transform(df[real_inputs].values)\n\n        # Format categorical inputs\n        for col in categorical_inputs:\n            string_df = df[col].apply(str)\n            output[col] = self._cat_scalers[col].transform(string_df)\n\n        return output\n\n    def format_predictions(self, predictions):\n        \"\"\"Reverts any normalisation to give predictions in original scale.\n\n        Args:\n          predictions: Dataframe of model predictions.\n\n        Returns:\n          Data frame of unnormalised predictions.\n        \"\"\"\n        output = predictions.copy()\n\n        column_names = predictions.columns\n\n        for col in column_names:\n            if col not in {\"forecast_time\", \"identifier\"}:\n                # Using [col] is for aligning with the format when fitting\n                output[col] = self._target_scaler.inverse_transform(predictions[[col]])\n\n        return output\n\n    # Default params\n    def get_fixed_params(self):\n        \"\"\"Returns fixed model parameters for experiments.\"\"\"\n\n        fixed_params = {\n            \"total_time_steps\": 6 + 6,\n            \"num_encoder_steps\": 6,\n            \"num_epochs\": 100,\n            \"early_stopping_patience\": 10,\n            \"multiprocessing_workers\": 5,\n        }\n\n        return fixed_params\n\n    def get_default_model_params(self):\n        \"\"\"Returns default optimised model parameters.\"\"\"\n\n        model_params = {\n            \"dropout_rate\": 0.4,\n            \"hidden_layer_size\": 160,\n            \"learning_rate\": 0.0001,\n            \"minibatch_size\": 128,\n            \"max_gradient_norm\": 0.0135,\n            \"num_heads\": 1,\n            \"stack_size\": 1,\n        }\n\n        return model_params",
    "source": "github_repo:microsoft/qlib",
    "file": "examples/benchmarks/TFT/data_formatters/qlib_Alpha158.py",
    "license": "MIT",
    "language": "python"
}