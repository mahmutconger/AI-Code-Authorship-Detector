{
    "code": "def render_prompts_lm(item, continuation_delimiter, fewshot_examples=None):\n    \"\"\"\n    Render complete prompt for a language modeling task.\n    Notice that we manually trim the context in the template,\n    which in some datasets seems to have trailing whitespace (which we don't want).\n    \"\"\"\n    template_str = \"\"\"\n{%- for example in fewshot_examples -%}\n{{ example.context | trim }}{{ continuation_delimiter }}{{ example.continuation }}\n\n{% endfor -%}\n{{ item.context | trim }}{{ continuation_delimiter }}{% if include_continuation %}{{ item.continuation }}{% endif %}\"\"\".strip()\n    template = Template(template_str)\n    fewshot_examples = fewshot_examples or []\n    context = {\n        'fewshot_examples': fewshot_examples,\n        'continuation_delimiter': continuation_delimiter,\n        'item': item\n    }\n    # Return two prompts: without and with the continuation\n    prompt_without = template.render(include_continuation=False, **context)\n    prompt_with = template.render(include_continuation=True, **context)\n    # Due to the way the data seems to be stored, I think I need to strip in the case of LM here.\n    # Otherwise we may get trailing whitespaces in prompt_without (which get absorbed into the next\n    # token in prompt_with), meaning we don't get a nice and clean prefix in the token space\n    # to detect the final continuation. Tokenizers...\n    prompt_without = prompt_without.strip()\n    return [prompt_without, prompt_with]",
    "source": "github_repo:karpathy/nanochat",
    "file": "nanochat/core_eval.py",
    "license": "MIT",
    "language": "python"
}