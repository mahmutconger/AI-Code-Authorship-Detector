{
    "code": "class Router(nn.Module):\n    \"\"\"\n    ## Routing Algorithm\n\n    This is the routing mechanism described in the paper.\n    You can use multiple routing layers in your models.\n\n    This combines calculating $\\mathbf{s}_j$ for this layer and\n    the routing algorithm described in *Procedure 1*.\n    \"\"\"\n\n    def __init__(self, in_caps: int, out_caps: int, in_d: int, out_d: int, iterations: int):\n        \"\"\"\n        `in_caps` is the number of capsules, and `in_d` is the number of features per capsule from the layer below.\n        `out_caps` and `out_d` are the same for this layer.\n\n        `iterations` is the number of routing iterations, symbolized by $r$ in the paper.\n        \"\"\"\n        super().__init__()\n        self.in_caps = in_caps\n        self.out_caps = out_caps\n        self.iterations = iterations\n        self.softmax = nn.Softmax(dim=1)\n        self.squash = Squash()\n\n        # This is the weight matrix $\\mathbf{W}_{ij}$. It maps each capsule in the\n        # lower layer to each capsule in this layer\n        self.weight = nn.Parameter(torch.randn(in_caps, out_caps, in_d, out_d), requires_grad=True)\n\n    def forward(self, u: torch.Tensor):\n        \"\"\"\n        The shape of `u` is `[batch_size, n_capsules, n_features]`.\n        These are the capsules from the lower layer.\n        \"\"\"\n\n        # $$\\hat{\\mathbf{u}}_{j|i} = \\mathbf{W}_{ij} \\mathbf{u}_i$$\n        # Here $j$ is used to index capsules in this layer, whilst $i$ is\n        # used to index capsules in the layer below (previous).\n        u_hat = torch.einsum('ijnm,bin->bijm', self.weight, u)\n\n        # Initial logits $b_{ij}$ are the log prior probabilities that capsule $i$\n        # should be coupled with $j$.\n        # We initialize these at zero\n        b = u.new_zeros(u.shape[0], self.in_caps, self.out_caps)\n\n        v = None\n\n        # Iterate\n        for i in range(self.iterations):\n            # routing softmax $$c_{ij} = \\frac{\\exp({b_{ij}})}{\\sum_k\\exp({b_{ik}})}$$\n            c = self.softmax(b)\n            # $$\\mathbf{s}_j = \\sum_i{c_{ij} \\hat{\\mathbf{u}}_{j|i}}$$\n            s = torch.einsum('bij,bijm->bjm', c, u_hat)\n            # $$\\mathbf{v}_j = squash(\\mathbf{s}_j)$$\n            v = self.squash(s)\n            # $$a_{ij} = \\mathbf{v}_j \\cdot \\hat{\\mathbf{u}}_{j|i}$$\n            a = torch.einsum('bjm,bijm->bij', v, u_hat)\n            # $$b_{ij} \\gets b_{ij} + \\mathbf{v}_j \\cdot \\hat{\\mathbf{u}}_{j|i}$$\n            b = b + a\n\n        return v",
    "source": "github_repo:labmlai/annotated_deep_learning_paper_implementations",
    "file": "labml_nn/capsule_networks/__init__.py",
    "license": "MIT",
    "language": "python"
}