{
    "code": "class DecodingTask:\n    inference: Inference\n    sequence_ranker: SequenceRanker\n    decoder: TokenDecoder\n    logit_filters: List[LogitFilter]\n\n    def __init__(self, model: \"Whisper\", options: DecodingOptions):\n        self.model = model\n\n        language = options.language or \"en\"\n        tokenizer = get_tokenizer(\n            model.is_multilingual,\n            num_languages=model.num_languages,\n            language=language,\n            task=options.task,\n        )\n        self.tokenizer: Tokenizer = tokenizer\n        self.options: DecodingOptions = self._verify_options(options)\n\n        self.n_group: int = options.beam_size or options.best_of or 1\n        self.n_ctx: int = model.dims.n_text_ctx\n        self.sample_len: int = options.sample_len or model.dims.n_text_ctx // 2\n\n        self.sot_sequence: Tuple[int] = tokenizer.sot_sequence\n        if self.options.without_timestamps:\n            self.sot_sequence = tokenizer.sot_sequence_including_notimestamps\n\n        self.initial_tokens: Tuple[int] = self._get_initial_tokens()\n        self.sample_begin: int = len(self.initial_tokens)\n        self.sot_index: int = self.initial_tokens.index(tokenizer.sot)\n\n        # inference: implements the forward pass through the decoder, including kv caching\n        self.inference = PyTorchInference(model, len(self.initial_tokens))\n\n        # sequence ranker: implements how to rank a group of sampled sequences\n        self.sequence_ranker = MaximumLikelihoodRanker(options.length_penalty)\n\n        # decoder: implements how to select the next tokens, given the autoregressive distribution\n        if options.beam_size is not None:\n            self.decoder = BeamSearchDecoder(\n                options.beam_size, tokenizer.eot, self.inference, options.patience\n            )\n        else:\n            self.decoder = GreedyDecoder(options.temperature, tokenizer.eot)\n\n        # logit filters: applies various rules to suppress or penalize certain tokens\n        self.logit_filters = []\n        if self.options.suppress_blank:\n            self.logit_filters.append(SuppressBlank(self.tokenizer, self.sample_begin))\n        if self.options.suppress_tokens:\n            self.logit_filters.append(SuppressTokens(self._get_suppress_tokens()))\n        if not options.without_timestamps:\n            precision = CHUNK_LENGTH / model.dims.n_audio_ctx  # usually 0.02 seconds\n            max_initial_timestamp_index = None\n            if options.max_initial_timestamp:\n                max_initial_timestamp_index = round(\n                    self.options.max_initial_timestamp / precision\n                )\n            self.logit_filters.append(\n                ApplyTimestampRules(\n                    tokenizer, self.sample_begin, max_initial_timestamp_index\n                )\n            )\n\n    def _verify_options(self, options: DecodingOptions) -> DecodingOptions:\n        if options.beam_size is not None and options.best_of is not None:\n            raise ValueError(\"beam_size and best_of can't be given together\")\n        if options.temperature == 0:\n            if options.best_of is not None:\n                raise ValueError(\"best_of with greedy sampling (T=0) is not compatible\")\n        if options.patience is not None and options.beam_size is None:\n            raise ValueError(\"patience requires beam_size to be given\")\n        if options.length_penalty is not None and not (\n            0 <= options.length_penalty <= 1\n        ):\n            raise ValueError(\"length_penalty (alpha) should be a value between 0 and 1\")\n\n        return options\n\n    def _get_initial_tokens(self) -> Tuple[int]:\n        tokens = list(self.sot_sequence)\n\n        if prefix := self.options.prefix:\n            prefix_tokens = (\n                self.tokenizer.encode(\" \" + prefix.strip())\n                if isinstance(prefix, str)\n                else prefix\n            )\n            if self.sample_len is not None:\n                max_prefix_len = self.n_ctx // 2 - self.sample_len\n                prefix_tokens = prefix_tokens[-max_prefix_len:]\n            tokens = tokens + prefix_tokens\n\n        if prompt := self.options.prompt:\n            prompt_tokens = (\n                self.tokenizer.encode(\" \" + prompt.strip())\n                if isinstance(prompt, str)\n                else prompt\n            )\n            tokens = (\n                [self.tokenizer.sot_prev]\n                + prompt_tokens[-(self.n_ctx // 2 - 1) :]\n                + tokens\n            )\n\n        return tuple(tokens)\n\n    def _get_suppress_tokens(self) -> Tuple[int]:\n        suppress_tokens = self.options.suppress_tokens\n\n        if isinstance(suppress_tokens, str):\n            suppress_tokens = [int(t) for t in suppress_tokens.split(\",\")]\n\n        if -1 in suppress_tokens:\n            suppress_tokens = [t for t in suppress_tokens if t >= 0]\n            suppress_tokens.extend(self.tokenizer.non_speech_tokens)\n        elif suppress_tokens is None or len(suppress_tokens) == 0:\n            suppress_tokens = []  # interpret empty string as an empty list\n        else:\n            assert isinstance(suppress_tokens, list), \"suppress_tokens must be a list\"\n\n        suppress_tokens.extend(\n            [\n                self.tokenizer.transcribe,\n                self.tokenizer.translate,\n                self.tokenizer.sot,\n                self.tokenizer.sot_prev,\n                self.tokenizer.sot_lm,\n            ]\n        )\n        if self.tokenizer.no_speech is not None:\n            # no-speech probability is collected separately\n            suppress_tokens.append(self.tokenizer.no_speech)\n\n        return tuple(sorted(set(suppress_tokens)))\n\n    def _get_audio_features(self, mel: Tensor):\n        if self.options.fp16:\n            mel = mel.half()\n\n        if mel.shape[-2:] == (\n            self.model.dims.n_audio_ctx,\n            self.model.dims.n_audio_state,\n        ):\n            # encoded audio features are given; skip audio encoding\n            audio_features = mel\n        else:\n            audio_features = self.model.encoder(mel)\n\n        if audio_features.dtype != (\n            torch.float16 if self.options.fp16 else torch.float32\n        ):\n            return TypeError(\n                f\"audio_features has an incorrect dtype: {audio_features.dtype}\"\n            )\n\n        return audio_features\n\n    def _detect_language(self, audio_features: Tensor, tokens: Tensor):\n        languages = [self.options.language] * audio_features.shape[0]\n        lang_probs = None\n\n        if self.options.language is None or self.options.task == \"lang_id\":\n            lang_tokens, lang_probs = self.model.detect_language(\n                audio_features, self.tokenizer\n            )\n            languages = [max(probs, key=probs.get) for probs in lang_probs]\n            if self.options.language is None:\n                tokens[:, self.sot_index + 1] = lang_tokens  # write language tokens\n\n        return languages, lang_probs\n\n    def _main_loop(self, audio_features: Tensor, tokens: Tensor):\n        n_batch = tokens.shape[0]\n        sum_logprobs: Tensor = torch.zeros(n_batch, device=audio_features.device)\n        no_speech_probs = [np.nan] * n_batch\n\n        try:\n            for i in range(self.sample_len):\n                logits = self.inference.logits(tokens, audio_features)\n\n                if (\n                    i == 0 and self.tokenizer.no_speech is not None\n                ):  # save no_speech_probs\n                    probs_at_sot = logits[:, self.sot_index].float().softmax(dim=-1)\n                    no_speech_probs = probs_at_sot[:, self.tokenizer.no_speech].tolist()\n\n                # now we need to consider the logits at the last token only\n                logits = logits[:, -1]\n\n                # apply the logit filters, e.g. for suppressing or applying penalty to\n                for logit_filter in self.logit_filters:\n                    logit_filter.apply(logits, tokens)\n\n                # expand the tokens tensor with the selected next tokens\n                tokens, completed = self.decoder.update(tokens, logits, sum_logprobs)\n\n                if completed or tokens.shape[-1] > self.n_ctx:\n                    break\n        finally:\n            self.inference.cleanup_caching()\n\n        return tokens, sum_logprobs, no_speech_probs\n\n    @torch.no_grad()\n    def run(self, mel: Tensor) -> List[DecodingResult]:\n        self.decoder.reset()\n        tokenizer: Tokenizer = self.tokenizer\n        n_audio: int = mel.shape[0]\n\n        audio_features: Tensor = self._get_audio_features(mel)  # encoder forward pass\n        tokens: Tensor = torch.tensor([self.initial_tokens]).repeat(n_audio, 1)\n\n        # detect language if requested, overwriting the language token\n        languages, language_probs = self._detect_language(audio_features, tokens)\n        if self.options.task == \"lang_id\":\n            return [\n                DecodingResult(\n                    audio_features=features, language=language, language_probs=probs\n                )\n                for features, language, probs in zip(\n                    audio_features, languages, language_probs\n                )\n            ]\n\n        # repeat text tensors by the group size, for beam search or best-of-n sampling\n        tokens = tokens.repeat_interleave(self.n_group, dim=0).to(audio_features.device)\n\n        # call the main sampling loop\n        tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n\n        # reshape the tensors to have (n_audio, n_group) as the first two dimensions\n        audio_features = audio_features[:: self.n_group]\n        no_speech_probs = no_speech_probs[:: self.n_group]\n        assert audio_features.shape[0] == len(no_speech_probs) == n_audio\n\n        tokens = tokens.reshape(n_audio, self.n_group, -1)\n        sum_logprobs = sum_logprobs.reshape(n_audio, self.n_group)\n\n        # get the final candidates for each group, and slice between the first sampled token and EOT\n        tokens, sum_logprobs = self.decoder.finalize(tokens, sum_logprobs)\n        tokens: List[List[Tensor]] = [\n            [t[self.sample_begin : (t == tokenizer.eot).nonzero()[0, 0]] for t in s]\n            for s in tokens\n        ]\n\n        # select the top-ranked sample in each group\n        selected = self.sequence_ranker.rank(tokens, sum_logprobs)\n        tokens: List[List[int]] = [t[i].tolist() for i, t in zip(selected, tokens)]\n        texts: List[str] = [tokenizer.decode(t).strip() for t in tokens]\n\n        sum_logprobs: List[float] = [lp[i] for i, lp in zip(selected, sum_logprobs)]\n        avg_logprobs: List[float] = [\n            lp / (len(t) + 1) for t, lp in zip(tokens, sum_logprobs)\n        ]\n\n        fields = (\n            texts,\n            languages,\n            tokens,\n            audio_features,\n            avg_logprobs,\n            no_speech_probs,\n        )\n        if len(set(map(len, fields))) != 1:\n            raise RuntimeError(f\"inconsistent result lengths: {list(map(len, fields))}\")\n\n        return [\n            DecodingResult(\n                audio_features=features,\n                language=language,\n                tokens=tokens,\n                text=text,\n                avg_logprob=avg_logprob,\n                no_speech_prob=no_speech_prob,\n                temperature=self.options.temperature,\n                compression_ratio=compression_ratio(text),\n            )\n            for text, language, tokens, features, avg_logprob, no_speech_prob in zip(\n                *fields\n            )\n        ]",
    "source": "github_repo:openai/whisper",
    "file": "whisper/decoding.py",
    "license": "MIT",
    "language": "python"
}