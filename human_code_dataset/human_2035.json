{
    "code": "class FFN(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        filter_channels,\n        kernel_size,\n        p_dropout=0.0,\n        activation=None,\n        causal=False,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.filter_channels = filter_channels\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.activation = activation\n        self.causal = causal\n\n        if causal:\n            self.padding = self._causal_padding\n        else:\n            self.padding = self._same_padding\n\n        self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size)\n        self.conv_2 = nn.Conv1d(filter_channels, out_channels, kernel_size)\n        self.drop = nn.Dropout(p_dropout)\n\n    def forward(self, x, x_mask):\n        x = self.conv_1(self.padding(x * x_mask))\n        if self.activation == \"gelu\":\n            x = x * torch.sigmoid(1.702 * x)\n        else:\n            x = torch.relu(x)\n        x = self.drop(x)\n        x = self.conv_2(self.padding(x * x_mask))\n        return x * x_mask\n\n    def _causal_padding(self, x):\n        if self.kernel_size == 1:\n            return x\n        pad_l = self.kernel_size - 1\n        pad_r = 0\n        padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n        x = F.pad(x, commons.convert_pad_shape(padding))\n        return x\n\n    def _same_padding(self, x):\n        if self.kernel_size == 1:\n            return x\n        pad_l = (self.kernel_size - 1) // 2\n        pad_r = self.kernel_size // 2\n        padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n        x = F.pad(x, commons.convert_pad_shape(padding))\n        return x",
    "source": "github_repo:myshell-ai/OpenVoice",
    "file": "openvoice/attentions.py",
    "license": "MIT",
    "language": "python"
}