{
    "code": "class Config:\n    def __init__(self):\n        self.device = \"cuda:0\"\n        self.is_half = True\n        self.use_jit = False\n        self.n_cpu = 0\n        self.gpu_name = None\n        self.json_config = self.load_config_json()\n        self.gpu_mem = None\n        (\n            self.python_cmd,\n            self.listen_port,\n            self.iscolab,\n            self.noparallel,\n            self.noautoopen,\n            self.dml,\n        ) = self.arg_parse()\n        self.instead = \"\"\n        self.preprocess_per = 3.7\n        self.x_pad, self.x_query, self.x_center, self.x_max = self.device_config()\n\n    @staticmethod\n    def load_config_json() -> dict:\n        d = {}\n        for config_file in version_config_list:\n            p = f\"configs/inuse/{config_file}\"\n            if not os.path.exists(p):\n                shutil.copy(f\"configs/{config_file}\", p)\n            with open(f\"configs/inuse/{config_file}\", \"r\") as f:\n                d[config_file] = json.load(f)\n        return d\n\n    @staticmethod\n    def arg_parse() -> tuple:\n        exe = sys.executable or \"python\"\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\"--port\", type=int, default=7865, help=\"Listen port\")\n        parser.add_argument(\"--pycmd\", type=str, default=exe, help=\"Python command\")\n        parser.add_argument(\"--colab\", action=\"store_true\", help=\"Launch in colab\")\n        parser.add_argument(\n            \"--noparallel\", action=\"store_true\", help=\"Disable parallel processing\"\n        )\n        parser.add_argument(\n            \"--noautoopen\",\n            action=\"store_true\",\n            help=\"Do not open in browser automatically\",\n        )\n        parser.add_argument(\n            \"--dml\",\n            action=\"store_true\",\n            help=\"torch_dml\",\n        )\n        cmd_opts = parser.parse_args()\n\n        cmd_opts.port = cmd_opts.port if 0 <= cmd_opts.port <= 65535 else 7865\n\n        return (\n            cmd_opts.pycmd,\n            cmd_opts.port,\n            cmd_opts.colab,\n            cmd_opts.noparallel,\n            cmd_opts.noautoopen,\n            cmd_opts.dml,\n        )\n\n    # has_mps is only available in nightly pytorch (for now) and MasOS 12.3+.\n    # check `getattr` and try it for compatibility\n    @staticmethod\n    def has_mps() -> bool:\n        if not torch.backends.mps.is_available():\n            return False\n        try:\n            torch.zeros(1).to(torch.device(\"mps\"))\n            return True\n        except Exception:\n            return False\n\n    @staticmethod\n    def has_xpu() -> bool:\n        if hasattr(torch, \"xpu\") and torch.xpu.is_available():\n            return True\n        else:\n            return False\n\n    def use_fp32_config(self):\n        for config_file in version_config_list:\n            self.json_config[config_file][\"train\"][\"fp16_run\"] = False\n            with open(f\"configs/inuse/{config_file}\", \"r\") as f:\n                strr = f.read().replace(\"true\", \"false\")\n            with open(f\"configs/inuse/{config_file}\", \"w\") as f:\n                f.write(strr)\n            logger.info(\"overwrite \" + config_file)\n        self.preprocess_per = 3.0\n        logger.info(\"overwrite preprocess_per to %d\" % (self.preprocess_per))\n\n    def device_config(self) -> tuple:\n        if torch.cuda.is_available():\n            if self.has_xpu():\n                self.device = self.instead = \"xpu:0\"\n                self.is_half = True\n            i_device = int(self.device.split(\":\")[-1])\n            self.gpu_name = torch.cuda.get_device_name(i_device)\n            if (\n                (\"16\" in self.gpu_name and \"V100\" not in self.gpu_name.upper())\n                or \"P40\" in self.gpu_name.upper()\n                or \"P10\" in self.gpu_name.upper()\n                or \"1060\" in self.gpu_name\n                or \"1070\" in self.gpu_name\n                or \"1080\" in self.gpu_name\n            ):\n                logger.info(\"Found GPU %s, force to fp32\", self.gpu_name)\n                self.is_half = False\n                self.use_fp32_config()\n            else:\n                logger.info(\"Found GPU %s\", self.gpu_name)\n            self.gpu_mem = int(\n                torch.cuda.get_device_properties(i_device).total_memory\n                / 1024\n                / 1024\n                / 1024\n                + 0.4\n            )\n            if self.gpu_mem <= 4:\n                self.preprocess_per = 3.0\n        elif self.has_mps():\n            logger.info(\"No supported Nvidia GPU found\")\n            self.device = self.instead = \"mps\"\n            self.is_half = False\n            self.use_fp32_config()\n        else:\n            logger.info(\"No supported Nvidia GPU found\")\n            self.device = self.instead = \"cpu\"\n            self.is_half = False\n            self.use_fp32_config()\n\n        if self.n_cpu == 0:\n            self.n_cpu = cpu_count()\n\n        if self.is_half:\n            # 6G显存配置\n            x_pad = 3\n            x_query = 10\n            x_center = 60\n            x_max = 65\n        else:\n            # 5G显存配置\n            x_pad = 1\n            x_query = 6\n            x_center = 38\n            x_max = 41\n\n        if self.gpu_mem is not None and self.gpu_mem <= 4:\n            x_pad = 1\n            x_query = 5\n            x_center = 30\n            x_max = 32\n        if self.dml:\n            logger.info(\"Use DirectML instead\")\n            if (\n                os.path.exists(\n                    \"runtime\\Lib\\site-packages\\onnxruntime\\capi\\DirectML.dll\"\n                )\n                == False\n            ):\n                try:\n                    os.rename(\n                        \"runtime\\Lib\\site-packages\\onnxruntime\",\n                        \"runtime\\Lib\\site-packages\\onnxruntime-cuda\",\n                    )\n                except:\n                    pass\n                try:\n                    os.rename(\n                        \"runtime\\Lib\\site-packages\\onnxruntime-dml\",\n                        \"runtime\\Lib\\site-packages\\onnxruntime\",\n                    )\n                except:\n                    pass\n            # if self.device != \"cpu\":\n            import torch_directml\n\n            self.device = torch_directml.device(torch_directml.default_device())\n            self.is_half = False\n        else:\n            if self.instead:\n                logger.info(f\"Use {self.instead} instead\")\n            if (\n                os.path.exists(\n                    \"runtime\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_providers_cuda.dll\"\n                )\n                == False\n            ):\n                try:\n                    os.rename(\n                        \"runtime\\Lib\\site-packages\\onnxruntime\",\n                        \"runtime\\Lib\\site-packages\\onnxruntime-dml\",\n                    )\n                except:\n                    pass\n                try:\n                    os.rename(\n                        \"runtime\\Lib\\site-packages\\onnxruntime-cuda\",\n                        \"runtime\\Lib\\site-packages\\onnxruntime\",\n                    )\n                except:\n                    pass\n        logger.info(\n            \"Half-precision floating-point: %s, device: %s\"\n            % (self.is_half, self.device)\n        )\n        return x_pad, x_query, x_center, x_max",
    "source": "github_repo:RVC-Project/Retrieval-based-Voice-Conversion-WebUI",
    "file": "configs/config.py",
    "license": "MIT",
    "language": "python"
}