{
    "code": "def build_model(checkpoint_dir, step, device, phase):\n    \"\"\"\n    A bunch of repetitive code to build a model from a given checkpoint.\n    Returns:\n    - base model - uncompiled, not wrapped in DDP\n    - tokenizer\n    - meta data saved during base model training\n    \"\"\"\n    assert phase in [\"train\", \"eval\"], f\"Invalid phase: {phase}\"\n    model_data, optimizer_data, meta_data = load_checkpoint(checkpoint_dir, step, device, load_optimizer=False)\n    if device.type in {\"cpu\", \"mps\"}:\n        # Convert bfloat16 tensors to float for CPU inference\n        model_data = {\n            k: v.float() if v.dtype == torch.bfloat16 else v\n            for k, v in model_data.items()\n        }\n    # Hack: fix torch compile issue, which prepends all keys with _orig_mod.\n    model_data = {k.removeprefix(\"_orig_mod.\"): v for k, v in model_data.items()}\n    model_config_kwargs = meta_data[\"model_config\"]\n    log0(f\"Building model with config: {model_config_kwargs}\")\n    model_config = GPTConfig(**model_config_kwargs)\n    with torch.device(\"meta\"):\n        model = GPT(model_config)\n    # Load the model state\n    model.to_empty(device=device)\n    model.init_weights() # note: this is dumb, but we need to init the rotary embeddings. TODO: fix model re-init\n    model.load_state_dict(model_data, strict=True, assign=True)\n    # Put the model in the right training phase / mode\n    if phase == \"eval\":\n        model.eval()\n    else:\n        model.train()\n    # Load the Tokenizer\n    tokenizer = get_tokenizer()\n    # Sanity check: compatibility between model and tokenizer\n    assert tokenizer.get_vocab_size() == model_config_kwargs[\"vocab_size\"]\n    return model, tokenizer, meta_data",
    "source": "github_repo:karpathy/nanochat",
    "file": "nanochat/checkpoint_manager.py",
    "license": "MIT",
    "language": "python"
}