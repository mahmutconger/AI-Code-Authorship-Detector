{
    "code": "def infer_panel_batch_infer(\n        self,\n        x: List[torch.LongTensor],  #####全部文本token\n        x_lens: torch.LongTensor,\n        prompts: torch.LongTensor,  ####参考音频token\n        bert_feature: List[torch.LongTensor],\n        top_k: int = -100,\n        top_p: int = 100,\n        early_stop_num: int = -1,\n        temperature: float = 1.0,\n        repetition_penalty: float = 1.35,\n        **kwargs,\n    ):\n        if prompts is None:\n            print(\"Warning: Prompt free is not supported batch_infer! switch to naive_infer\")\n            return self.infer_panel_naive_batched(\n                x,\n                x_lens,\n                prompts,\n                bert_feature,\n                top_k=top_k,\n                top_p=top_p,\n                early_stop_num=early_stop_num,\n                temperature=temperature,\n                **kwargs,\n            )\n\n        max_len = kwargs.get(\"max_len\", x_lens.max())\n        x_list = []\n        for x_item, bert_item in zip(x, bert_feature):\n            # max_len = max(max_len, x_item.shape[0], bert_item.shape[1])\n            x_item = self.ar_text_embedding(x_item.unsqueeze(0))\n            x_item = x_item + self.bert_proj(bert_item.transpose(0, 1).unsqueeze(0))\n            x_item = self.ar_text_position(x_item).squeeze(0)\n            # x_item = F.pad(x_item,(0,0,0,max_len-x_item.shape[0]),value=0) if x_item.shape[0]<max_len else x_item  ### padding right\n            x_item = (\n                F.pad(x_item, (0, 0, max_len - x_item.shape[0], 0), value=0) if x_item.shape[0] < max_len else x_item\n            )  ### padding left\n            x_list.append(x_item)\n        x: torch.Tensor = torch.stack(x_list, dim=0)\n\n        # AR Decoder\n        y = prompts\n\n        x_len = x.shape[1]\n        stop = False\n\n        k_cache = None\n        v_cache = None\n        ###################  first step ##########################\n        assert y is not None, \"Error: Prompt free is not supported batch_infer!\"\n        ref_free = False\n\n        y_emb = self.ar_audio_embedding(y)\n        y_len = y_emb.shape[1]\n        prefix_len = y.shape[1]\n        y_lens = torch.LongTensor([y_emb.shape[1]] * y_emb.shape[0]).to(x.device)\n        y_pos = self.ar_audio_position(y_emb)\n        xy_pos = torch.concat([x, y_pos], dim=1)\n\n        ##### create mask #####\n        bsz = x.shape[0]\n        src_len = x_len + y_len\n        y_paddind_mask = make_pad_mask_left(y_lens, y_len)\n        x_paddind_mask = make_pad_mask_left(x_lens, max_len)\n\n        # (bsz, x_len + y_len)\n        padding_mask = torch.concat([x_paddind_mask, y_paddind_mask], dim=1)\n\n        x_mask = F.pad(\n            torch.zeros(x_len, x_len, dtype=torch.bool, device=x.device),\n            (0, y_len),\n            value=True,\n        )\n\n        y_mask = F.pad(  ###yy的右上1扩展到左边xy的0,(y,x+y)\n            torch.triu(torch.ones(y_len, y_len, dtype=torch.bool, device=x.device), diagonal=1),\n            (x_len, 0),\n            value=False,\n        )\n\n        causal_mask = torch.concat([x_mask, y_mask], dim=0).view(1, src_len, src_len).repeat(bsz, 1, 1).to(x.device)\n        # padding_mask = padding_mask.unsqueeze(1) * padding_mask.unsqueeze(2) ### [b, x+y, x+y]\n        ### 上面是错误的，会导致padding的token被\"看见\"\n\n        # 正确的padding_mask应该是：\n        # |   pad_len   |  x_len  |  y_len  |\n        # [[PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],  前3行按理说也应该被mask掉，但是为了防止计算attention时不出现nan，还是保留了，不影响结果\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6]]\n\n        padding_mask = padding_mask.view(bsz, 1, src_len).repeat(1, src_len, 1)\n\n        attn_mask: torch.Tensor = causal_mask.logical_or(padding_mask)\n        attn_mask = attn_mask.unsqueeze(1).expand(-1, self.num_head, -1, -1).bool()\n\n        # 正确的attn_mask应该是这样的：\n        # |   pad_len   |  x_len  |  y_len  |\n        # [[PAD, PAD, PAD, 1, 2, 3, EOS, EOS, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3, EOS, EOS, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3, EOS, EOS, EOS],  前3行按理说也应该被mask掉，但是为了防止计算attention时不出现nan，还是保留了，不影响结果\n        # [PAD, PAD, PAD, 1, 2, 3, EOS, EOS, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3, EOS, EOS, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3, EOS, EOS, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3,   4, EOS, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3,   4,   5, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3,   4,   5,   6]]\n\n        ###### decode #####\n        y_list = [None] * y.shape[0]\n        batch_idx_map = list(range(y.shape[0]))\n        idx_list = [None] * y.shape[0]\n        for idx in tqdm(range(1500)):\n            if idx == 0:\n                xy_dec, k_cache, v_cache = self.t2s_transformer.process_prompt(xy_pos, attn_mask, None)\n            else:\n                xy_dec, k_cache, v_cache = self.t2s_transformer.decode_next_token(xy_pos, k_cache, v_cache, attn_mask)\n            logits = self.ar_predict_layer(xy_dec[:, -1])\n\n            if idx == 0:\n                attn_mask = F.pad(attn_mask[:, :, -1].unsqueeze(-2), (0, 1), value=False)\n                logits = logits[:, :-1]\n            else:\n                attn_mask = F.pad(attn_mask, (0, 1), value=False)\n\n            samples = sample(\n                logits, y, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty, temperature=temperature\n            )[0]\n\n            y = torch.concat([y, samples], dim=1)\n\n            ####### 移除batch中已经生成完毕的序列,进一步优化计算量\n            tokens = torch.argmax(logits, dim=-1)\n            reserved_idx_of_batch_for_y = None\n            if (self.EOS in samples[:, 0]) or (self.EOS in tokens):  ###如果生成到EOS，则停止\n                l1 = samples[:, 0] == self.EOS\n                l2 = tokens == self.EOS\n                l = l1.logical_or(l2)\n                removed_idx_of_batch_for_y = torch.where(l == True)[0].tolist()\n                reserved_idx_of_batch_for_y = torch.where(l == False)[0]\n                # batch_indexs = torch.tensor(batch_idx_map, device=y.device)[removed_idx_of_batch_for_y]\n                for i in removed_idx_of_batch_for_y:\n                    batch_index = batch_idx_map[i]\n                    idx_list[batch_index] = idx\n                    y_list[batch_index] = y[i, :-1]\n\n                batch_idx_map = [batch_idx_map[i] for i in reserved_idx_of_batch_for_y.tolist()]\n\n            # 只保留batch中未生成完毕的序列\n            if reserved_idx_of_batch_for_y is not None:\n                # index = torch.LongTensor(batch_idx_map).to(y.device)\n                y = torch.index_select(y, dim=0, index=reserved_idx_of_batch_for_y)\n                attn_mask = torch.index_select(attn_mask, dim=0, index=reserved_idx_of_batch_for_y)\n                if k_cache is not None:\n                    for i in range(len(k_cache)):\n                        k_cache[i] = torch.index_select(k_cache[i], dim=0, index=reserved_idx_of_batch_for_y)\n                        v_cache[i] = torch.index_select(v_cache[i], dim=0, index=reserved_idx_of_batch_for_y)\n\n            if (early_stop_num != -1 and (y.shape[1] - prefix_len) > early_stop_num) or idx == 1499:\n                print(\"use early stop num:\", early_stop_num)\n                stop = True\n                for i, batch_index in enumerate(batch_idx_map):\n                    batch_index = batch_idx_map[i]\n                    idx_list[batch_index] = idx\n                    y_list[batch_index] = y[i, :-1]\n\n            if None not in idx_list:\n                stop = True\n\n            if stop:\n                if y.shape[1] == 0:\n                    y = torch.concat([y, torch.zeros_like(samples)], dim=1)\n                    print(\"bad zero prediction\")\n                print(f\"T2S Decoding EOS [{prefix_len} -> {y.shape[1]}]\")\n                break\n\n            ####################### update next step ###################################\n            y_emb = self.ar_audio_embedding(y[:, -1:])\n            xy_pos = y_emb * self.ar_audio_position.x_scale + self.ar_audio_position.alpha * self.ar_audio_position.pe[\n                :, y_len + idx\n            ].to(dtype=y_emb.dtype, device=y_emb.device)\n\n        if None in idx_list:\n            for i in range(x.shape[0]):\n                if idx_list[i] is None:\n                    idx_list[i] = 1500 - 1  ###如果没有生成到EOS，就用最大长度代替\n\n        if ref_free:\n            return y_list, [0] * x.shape[0]\n        # print(idx_list)\n        return y_list, idx_list",
    "source": "github_repo:RVC-Boss/GPT-SoVITS",
    "file": "GPT_SoVITS/AR/models/t2s_model.py",
    "license": "MIT",
    "language": "python"
}