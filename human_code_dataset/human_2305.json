{
    "code": "class AlphaDataset:\n    \"\"\"Alpha dataset template class\"\"\"\n\n    def __init__(\n        self,\n        df: pl.DataFrame,\n        train_period: tuple[str, str],\n        valid_period: tuple[str, str],\n        test_period: tuple[str, str],\n        process_type: str = \"append\"\n    ) -> None:\n        \"\"\"Constructor\"\"\"\n        self.df: pl.DataFrame = df\n\n        # DataFrames for processed data\n        self.result_df: pl.DataFrame\n        self.raw_df: pl.DataFrame\n        self.infer_df: pl.DataFrame\n        self.learn_df: pl.DataFrame\n\n        # New version\n        self.data_periods: dict[Segment, tuple[str, str]] = {\n            Segment.TRAIN: train_period,\n            Segment.VALID: valid_period,\n            Segment.TEST: test_period\n        }\n\n        self.feature_expressions: dict[str, str | pl.expr.expr.Expr] = {}\n        self.feature_results: dict[str, pl.DataFrame] = {}\n        self.label_expression: str = \"\"\n\n        self.process_type: str = process_type\n        self.infer_processors: list = []\n        self.learn_processors: list = []\n\n    def add_feature(\n        self,\n        name: str,\n        expression: str | pl.expr.expr.Expr | None = None,\n        result: pl.DataFrame | None = None\n    ) -> None:\n        \"\"\"\n        Add a feature expression\n        \"\"\"\n        if expression is not None and result is not None:\n            raise ValueError(\"Only one of 'expression' or 'result' can be provided\")\n\n        if expression is not None:\n            self.feature_expressions[name] = expression\n        elif result is not None:\n            self.feature_results[name] = result\n\n    def set_label(self, expression: str) -> None:\n        \"\"\"\n        Set the label expression\n        \"\"\"\n        self.label_expression = expression\n\n    def add_processor(self, task: str, processor: Callable[[pl.DataFrame], None]) -> None:\n        \"\"\"\n        Add a feature preprocessor\n        \"\"\"\n        if task == \"infer\":\n            self.infer_processors.append(processor)\n        else:\n            self.learn_processors.append(processor)\n\n    def prepare_data(self, filters: dict | None = None, max_workers: int | None = None) -> None:\n        \"\"\"\n        Generate required data\n        \"\"\"\n        # List for feature data results\n        results: list = []\n\n        # Iterate through expressions for calculation\n        expressions: list[tuple[str, str | pl.expr.expr.Expr]] = list(self.feature_expressions.items())\n\n        if self.label_expression:\n            expressions.append((\"label\", self.label_expression))\n\n        # Create process pool\n        logger.info(\"开始计算表达式因子特征\")\n\n        args: list[tuple] = [(self.df, name, expression) for name, expression in expressions]\n\n        context: BaseContext = get_context(\"spawn\")\n\n        with context.Pool(processes=max_workers) as pool:\n            # Calculate all expressions in parallel\n            it = pool.imap(calculate_feature, args)\n\n            # Collect results\n            for result in tqdm(it, total=len(args)):\n                results.append(result)\n\n        self.result_df = self.df.with_columns(results)\n\n        # Merge result data factor features\n        logger.info(\"开始合并结果数据因子特征\")\n\n        label_exist: bool = \"label\" in self.result_df\n        for name, feature_result in tqdm(self.feature_results.items()):\n            feature_result = feature_result.rename({\"data\": name})\n            self.result_df = self.result_df.join(feature_result, on=[\"datetime\", \"vt_symbol\"], how=\"left\")\n\n        if label_exist:\n            # Put label at the last column\n            cols: list = [col for col in self.result_df.columns if col != \"label\"] + [\"label\"]\n            self.result_df = self.result_df.select(cols).sort([\"datetime\", \"vt_symbol\"])\n\n        # Generate raw data\n        raw_df = self.result_df.fill_null(float(\"nan\"))\n\n        if filters:\n            logger.info(\"开始筛选成分股数据\")\n\n            filtered_df = pl.DataFrame()\n\n            for vt_symbol, ranges in tqdm(filters.items(), total=len(filters)):\n                for start, end in ranges:\n                    temp_df = raw_df.filter(\n                        (pl.col(\"vt_symbol\") == vt_symbol) & (pl.col(\"datetime\") >= pl.lit(start)) & (pl.col(\"datetime\") <= pl.lit(end))\n                    )\n                    filtered_df = pl.concat([filtered_df, temp_df])\n\n            raw_df = filtered_df\n\n        # Only keep feature columns\n        select_columns: list[str] = [\"datetime\", \"vt_symbol\"] + raw_df.columns[self.df.width:]\n        self.raw_df = raw_df.select(select_columns).sort([\"datetime\", \"vt_symbol\"])\n\n        self.infer_df = self.raw_df\n        self.learn_df = self.raw_df\n\n    def process_data(self) -> None:\n        \"\"\"\n        Process data\n        \"\"\"\n        # Generate inference data\n        for processor in self.infer_processors:\n            self.infer_df = processor(df=self.infer_df)\n\n        # Generate learning data\n        if self.process_type == \"append\":\n            self.learn_df = self.infer_df\n\n        for processor in self.learn_processors:\n            self.learn_df = processor(df=self.learn_df)\n\n    def fetch_raw(self, segment: Segment) -> pl.DataFrame:\n        \"\"\"\n        Get raw data for a specific segment\n        \"\"\"\n        start, end = self.data_periods[segment]\n        return query_by_time(self.raw_df, start, end)\n\n    def fetch_infer(self, segment: Segment) -> pl.DataFrame:\n        \"\"\"\n        Get inference data for a specific segment\n        \"\"\"\n        start, end = self.data_periods[segment]\n        return query_by_time(self.infer_df, start, end)\n\n    def fetch_learn(self, segment: Segment) -> pl.DataFrame:\n        \"\"\"\n        Get learning data for a specific segment\n        \"\"\"\n        start, end = self.data_periods[segment]\n        return query_by_time(self.learn_df, start, end)\n\n    def show_feature_performance(self, name: str) -> None:\n        \"\"\"\n        Perform performance analysis for a feature\n        \"\"\"\n        starts: list[datetime] = []\n        ends: list[datetime] = []\n\n        for period in self.data_periods.values():\n            starts.append(to_datetime(period[0]))\n            ends.append(to_datetime(period[1]))\n\n        start: datetime = min(starts)\n        end: datetime = max(ends)\n\n        # Select range\n        result_df: pl.DataFrame = query_by_time(self.result_df, start, end)\n        learn_df: pl.DataFrame = query_by_time(self.learn_df, start, end)\n\n        merged_df = (\n            result_df\n            .select([\"datetime\", \"vt_symbol\", \"close\"])\n            .join(\n                learn_df.select([\"datetime\", \"vt_symbol\", name]),\n                on=[\"datetime\", \"vt_symbol\"],\n                how=\"inner\"\n            )\n        )\n\n        # Fill NaN and drop nulls\n        merged_df = merged_df.fill_nan(None).drop_nulls()\n\n        # Extract feature\n        feature_df: pd.DataFrame = merged_df.select([\"datetime\", \"vt_symbol\", name]).to_pandas()\n        feature_df.set_index([\"datetime\", \"vt_symbol\"], inplace=True)\n\n        feature_s: pd.Series = feature_df[name]\n\n        # Extract price\n        price_df: pd.DataFrame = merged_df.select([\"datetime\", \"vt_symbol\", \"close\"]).to_pandas()\n        price_df = price_df.pivot(index=\"datetime\", columns=\"vt_symbol\", values=\"close\")\n\n        # Merge data\n        clean_data: pd.DataFrame = get_clean_factor_and_forward_returns(feature_s, price_df, quantiles=10)\n\n        # Perform analysis\n        create_full_tear_sheet(clean_data)\n\n    def show_signal_performance(self, signal: pl.DataFrame) -> None:\n        \"\"\"\n        Perform performance analysis for prediction signals\n        \"\"\"\n        # Get signal start and end times\n        start: datetime = cast(datetime, signal[\"datetime\"].min())\n        end: datetime = cast(datetime, signal[\"datetime\"].max())\n\n        # Select range\n        df: pl.DataFrame = query_by_time(self.result_df, start, end)\n\n        # Extract feature\n        signal_df: pd.DataFrame = signal.to_pandas()\n        signal_df.set_index([\"datetime\", \"vt_symbol\"], inplace=True)\n        signal_s: pd.Series = signal_df[\"signal\"]\n\n        # Extract price\n        price_df: pd.DataFrame = df.select([\"datetime\", \"vt_symbol\", \"close\"]).to_pandas()\n        price_df = price_df.pivot(index=\"datetime\", columns=\"vt_symbol\", values=\"close\")\n\n        # Merge data\n        clean_data: pd.DataFrame = get_clean_factor_and_forward_returns(\n            signal_s,\n            price_df,\n            max_loss=1.0,\n            quantiles=10\n        )\n\n        # Perform analysis\n        create_full_tear_sheet(clean_data)",
    "source": "github_repo:vnpy/vnpy",
    "file": "vnpy/alpha/dataset/template.py",
    "license": "MIT",
    "language": "python"
}