{
    "code": "class KVCache:\n    \"\"\"\n    Works hand-in-hand with the GPT model to maintain the KV cache.\n    Note that the .pos advances automatically after the last layer of the Transformer inserts.\n    \"\"\"\n\n    def __init__(self, batch_size, num_heads, seq_len, head_dim, num_layers):\n        # Each of K/V is of shape (B, H, T, D) and we have one per layer of the Transformer.\n        self.kv_shape = (num_layers, 2, batch_size, num_heads, seq_len, head_dim)\n        self.kv_cache = None\n        self.pos = 0 # current position in time in the cache\n\n    def reset(self):\n        self.pos = 0\n\n    def get_pos(self):\n        return self.pos\n\n    def prefill(self, other):\n        \"\"\"\n        Prefill given another KV cache. Optionally expand along batch dim.\n        This is used when we do batch 1 prefill and then want to generate\n        multiple samples in parallel from there.\n        \"\"\"\n        # 1) validate the shapes\n        assert self.kv_cache is None, \"Cannot prefill a non-empty KV cache\"\n        assert other.kv_cache is not None, \"Cannot prefill with a None KV cache\"\n        for ix, (dim1, dim2) in enumerate(zip(self.kv_shape, other.kv_shape)):\n            # ix 0: num_layers, 1: k/v, 2: batch_size, 3: num_heads, 4: seq_len, 5: head_dim\n            if ix in [0, 1, 3, 5]:\n                # num_layers, k/v, num_heads, head_dim must match\n                assert dim1 == dim2, f\"Dim {ix} mismatch: {dim1} != {dim2}\"\n            elif ix == 2:\n                # batch_size can be expanded\n                assert dim1 == dim2 or dim2 == 1, f\"Batch dim mismatch: {dim1} != {dim2}\"\n            elif ix == 4:\n                # seq_len: self must be longer than other\n                assert dim1 >= dim2, f\"Seq len mismatch: {dim1} < {dim2}\"\n        # 2) initialize the cache\n        dtype, device = other.kv_cache.dtype, other.kv_cache.device\n        self.kv_cache = torch.empty(self.kv_shape, dtype=dtype, device=device)\n        # 3) copy the data over\n        self.kv_cache[:, :, :, :, :other.pos, :] = other.kv_cache\n        # 4) update the pos\n        self.pos = other.pos\n\n    def insert_kv(self, layer_idx, k, v):\n        # Lazy initialize the cache here because we need to know the dtype/device\n        if self.kv_cache is None:\n            self.kv_cache = torch.empty(self.kv_shape, dtype=k.dtype, device=k.device)\n        # Insert new keys/values to the cache and return the full cache so far\n        B, H, T_add, D = k.size()\n        t0, t1 = self.pos, self.pos + T_add\n        # Dynamically grow the cache if needed\n        if t1 > self.kv_cache.size(4):\n            t_needed = t1 + 1024 # as much as we need plus buffer of 1024\n            t_needed = (t_needed + 1023) & ~1023 # then round up to the nearest multiple of 1024\n            additional_shape = list(self.kv_cache.shape)\n            additional_shape[4] = t_needed - self.kv_cache.size(4)\n            additional_cache = torch.empty(additional_shape, dtype=k.dtype, device=k.device)\n            self.kv_cache = torch.cat([self.kv_cache, additional_cache], dim=4).contiguous()\n            self.kv_shape = self.kv_cache.shape\n        # Insert k, v into the cache\n        self.kv_cache[layer_idx, 0, :, :, t0:t1] = k\n        self.kv_cache[layer_idx, 1, :, :, t0:t1] = v\n        # Return the full cached keys/values up to current position (as a view)\n        key_view = self.kv_cache[layer_idx, 0, :, :, :t1]\n        value_view = self.kv_cache[layer_idx, 1, :, :, :t1]\n        # Increment pos after the last layer of the Transformer processes\n        if layer_idx == self.kv_cache.size(0) - 1:\n            self.pos = t1\n        return key_view, value_view",
    "source": "github_repo:karpathy/nanochat",
    "file": "nanochat/engine.py",
    "license": "MIT",
    "language": "python"
}