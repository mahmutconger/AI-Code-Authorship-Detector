{
    "code": "def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        * `x` is the input of shape `[batch_size, n_elems]`\n\n        This outputs a tuple of four tensors:\n\n        1. $p_1 \\dots p_N$ in a tensor of shape `[N, batch_size]`\n        2. $\\hat{y}_1 \\dots \\hat{y}_N$ in a tensor of shape `[N, batch_size]` - the log probabilities of the parity being $1$\n        3. $p_m$ of shape `[batch_size]`\n        4. $\\hat{y}_m$ of shape `[batch_size]` where the computation was halted at step $m$\n        \"\"\"\n\n        #\n        batch_size = x.shape[0]\n\n        # We get initial state $h_1 = s_h(x)$\n        h = x.new_zeros((x.shape[0], self.n_hidden))\n        h = self.gru(x, h)\n\n        # Lists to store $p_1 \\dots p_N$ and $\\hat{y}_1 \\dots \\hat{y}_N$\n        p = []\n        y = []\n        # $\\prod_{j=1}^{n-1} (1 - \\lambda_j)$\n        un_halted_prob = h.new_ones((batch_size,))\n\n        # A vector to maintain which samples has halted computation\n        halted = h.new_zeros((batch_size,))\n        # $p_m$ and $\\hat{y}_m$ where the computation was halted at step $m$\n        p_m = h.new_zeros((batch_size,))\n        y_m = h.new_zeros((batch_size,))\n\n        # Iterate for $N$ steps\n        for n in range(1, self.max_steps + 1):\n            # The halting probability $\\lambda_N = 1$ for the last step\n            if n == self.max_steps:\n                lambda_n = h.new_ones(h.shape[0])\n            # $\\lambda_n = s_\\lambda(h_n)$\n            else:\n                lambda_n = self.lambda_prob(self.lambda_layer(h))[:, 0]\n            # $\\hat{y}_n = s_y(h_n)$\n            y_n = self.output_layer(h)[:, 0]\n\n            # $$p_n = \\lambda_n \\prod_{j=1}^{n-1} (1 - \\lambda_j)$$\n            p_n = un_halted_prob * lambda_n\n            # Update $\\prod_{j=1}^{n-1} (1 - \\lambda_j)$\n            un_halted_prob = un_halted_prob * (1 - lambda_n)\n\n            # Halt based on halting probability $\\lambda_n$\n            halt = torch.bernoulli(lambda_n) * (1 - halted)\n\n            # Collect $p_n$ and $\\hat{y}_n$\n            p.append(p_n)\n            y.append(y_n)\n\n            # Update $p_m$ and $\\hat{y}_m$ based on what was halted at current step $n$\n            p_m = p_m * (1 - halt) + p_n * halt\n            y_m = y_m * (1 - halt) + y_n * halt\n\n            # Update halted samples\n            halted = halted + halt\n            # Get next state $h_{n+1} = s_h(x, h_n)$\n            h = self.gru(x, h)\n\n            # Stop the computation if all samples have halted\n            if self.is_halt and halted.sum() == batch_size:\n                break\n\n        #\n        return torch.stack(p), torch.stack(y), p_m, y_m",
    "source": "github_repo:labmlai/annotated_deep_learning_paper_implementations",
    "file": "labml_nn/adaptive_computation/ponder_net/__init__.py",
    "license": "MIT",
    "language": "python"
}