{
    "code": "class DABench:\n    def __init__(\n        self,\n        questions_file: Path = Path(DABENCH_PATH) / \"da-dev-questions.jsonl\",\n        answers_file: Path = Path(DABENCH_PATH) / \"da-dev-labels.jsonl\",\n        template: str = \"\",\n    ):\n        \"\"\"\n        Initializes the DABench instance with questions and answers.\n\n        This constructor loads questions and answers from specified JSONL files.\n        It also sets a template for formatting prompts. If no template is provided,\n        a default template is used.\n\n        Args:\n            questions_file (Path): The path to the JSONL file containing questions.\n            answers_file (Path): The path to the JSONL file containing answers.\n            template (str): A string template for formatting prompts.\n        \"\"\"\n\n        self.questions = {\n            int(line[\"id\"]): line for line in load_jsonl(questions_file)\n        }  # Load questions from the specified file\n        self.answers = {\n            int(line[\"id\"]): line for line in load_jsonl(answers_file)\n        }  # Load answers from the specified file\n        self.template = template if template else DABENCH  # Set the template, defaulting if necessary\n\n    def get_question(self, question_id: str) -> dict:\n        \"\"\"\n        Retrieve the question associated with the given ID.\n\n        This method looks up a question by its unique identifier. If the question\n        is found, it returns the question data; otherwise, it returns a message\n        indicating that the question was not found.\n\n        Args:\n            question_id (str): The unique identifier for the question.\n\n        Returns:\n            dict: The question data if found, otherwise a \"Question not found.\" message.\n        \"\"\"\n        return self.questions.get(question_id, \"Question not found.\")  # Return the question or an error message\n\n    def generate_formatted_prompt(self, question_id: str) -> str:\n        \"\"\"\n        Generate a formatted prompt for the specified question ID.\n\n        This method retrieves the question data and formats it using the specified\n        template. The formatted prompt includes the question, constraints, format,\n        file name, and level, allowing for a structured output.\n\n        Args:\n            question_id (str): The unique identifier for the question.\n\n        Returns:\n            str: A formatted prompt string based on the question data.\n        \"\"\"\n        temp = self.get_question(question_id)  # Retrieve the question data\n        return self.template.format(\n            question=temp[\"question\"],\n            constraints=temp[\"constraints\"],\n            format=temp[\"format\"],\n            file_name=str(DABENCH_PATH) + \"/da-dev-tables/\" + temp[\"file_name\"],\n            level=temp[\"level\"],\n        )  # Format and return the prompt\n\n    def get_answer(self, answer_id: str) -> list:\n        \"\"\"\n        Retrieve the answer list associated with the given ID.\n\n        This method looks up an answer by its unique identifier. If the answer\n        is found, it returns the answer data; otherwise, it returns a message\n        indicating that the answer was not found.\n\n        Args:\n            answer_id (str): The unique identifier for the answer.\n\n        Returns:\n            list: The answer data if found, otherwise an \"Answer not found.\" message.\n        \"\"\"\n        return self.answers.get(answer_id, \"Answer not found.\")  # Return the answer or an error message\n\n    @handle_exception(exception_msg=\"Error parsing cleaned prediction\", default_return=(None, False))\n    def parse_cleaned_prediction(self, cleaned_prediction: str, true_label: Any) -> Tuple[str, bool]:\n        \"\"\"\n        Parse the cleaned prediction and compare it with the true label.\n\n        Args:\n            cleaned_prediction (str): The cleaned prediction string.\n            true_label (Any): The true label to compare against.\n\n        Returns:\n            Tuple[str, bool]: A tuple containing the cleaned prediction and a boolean indicating\n                              whether it matches the true label.\n        \"\"\"\n        if cleaned_prediction:  # Ensure the cleaned prediction is not empty\n            pred_dict = parse_prediction(cleaned_prediction)  # Parse the prediction\n            if pred_dict is not None and compare_predictions(pred_dict, true_label):\n                return cleaned_prediction, True  # Return if the prediction matches the true label\n        return cleaned_prediction, False  # Return the cleaned prediction with a False match\n\n    @handle_exception(exception_msg=\"Error during async reformat\", default_return=(None, False))\n    def async_reformat_prediction(self, id: str, result: str) -> str:\n        \"\"\"\n        Reformat the prediction asynchronously and extract the answer.\n\n        Args:\n            id (str): The identifier for the question.\n            result (str): The original prediction result.\n\n        Returns:\n            str: The reformatted prediction or the original prediction if extraction fails.\n        \"\"\"\n        question = self.get_question(id)[\"question\"]  # Retrieve the question based on the ID\n        question_format = self.get_question(id)[\"format\"]  # Get the format of the question\n        prediction = asyncio.run(reformat(question, question_format, result))  # Asynchronously reformat the prediction\n\n        # Attempt to extract the answer from the reformatted prediction\n        answer_part = prediction.split(\"Answer{{\") if \"Answer{{\" in prediction else []\n        if len(answer_part) > 1:\n            return answer_part[1].split(\"}}\")[0].strip()  # Return the extracted answer\n\n        return prediction  # If extraction fails, return the original prediction\n\n    def eval(self, id: str, result: str) -> Tuple[str, bool]:\n        \"\"\"\n        Evaluate the prediction against the true label.\n\n        Args:\n            id (str): The identifier for the question.\n            result (str): The original prediction result.\n\n        Returns:\n            Tuple[str, bool]: A tuple containing the final prediction and a boolean indicating\n                              whether it matches the true label.\n        \"\"\"\n        true_label = self.get_answer(id)[\"common_answers\"]  # Retrieve the true label for comparison\n        nest_asyncio.apply()  # Apply nested asyncio to allow for async calls\n        result = json.loads(str(result).split(\"Current Plan\")[1].split(\"## Current Task\")[0])[-1][\"result\"].strip()\n        cleaned_prediction = result.replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\")  # Clean the prediction string\n\n        # Use the decorated function to handle exceptions while parsing the cleaned prediction\n        parsed_result = self.parse_cleaned_prediction(cleaned_prediction, true_label)\n        if parsed_result[1]:  # If the parsed prediction is valid\n            return parsed_result  # Return the valid prediction\n\n        # If the cleaned prediction is not valid, attempt to asynchronously reformat it\n        prediction = self.async_reformat_prediction(id, result)\n\n        pred_dict = parse_prediction(prediction)  # Parse the reformatted prediction\n        if pred_dict is not None and compare_predictions(pred_dict, true_label):\n            return prediction, True  # Return if the reformatted prediction matches the true label\n\n        return prediction, False  # Return the final prediction with a False match\n\n    @handle_exception(exception_msg=\"Error evaluating single prediction\", default_return={})\n    def single_eval(self, id: str, prediction: str) -> dict:\n        \"\"\"\n        Evaluate the prediction against the true label for a single question.\n        just using in eval_all\n\n        Args:\n            id (str): The identifier for the question.\n            prediction (str): The prediction string to evaluate.\n\n        Returns:\n            dict: A dictionary indicating the correctness of each metric.\n        \"\"\"\n        true_label = self.get_answer(id)[\"common_answers\"]  # Retrieve the true label for the question\n        prediction = prediction.replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\")  # Clean the prediction string\n        pred_dict = parse_prediction(prediction)  # Parse the prediction into a dictionary\n\n        # Initialize the correctness dictionary with False values for each metric\n        correctness = {metric: False for metric, _ in true_label}\n\n        # Check each metric's prediction against the true label\n        for metric, true_value in true_label:\n            try:\n                true_value = float(true_value)  # Attempt to convert the true value to float\n            except ValueError:\n                true_value = true_value.replace(\",\", \"\")  # Handle non-numeric values\n\n            if metric in pred_dict:\n                # Consider the prediction correct if it's within a small tolerance\n                if (\n                    isinstance(true_value, (int, float))\n                    and isinstance(pred_dict[metric], (int, float))\n                    and abs(pred_dict[metric] - true_value) < 1e-6\n                ):\n                    correctness[metric] = True  # Mark as correct if within tolerance\n\n                if isinstance(true_value, str) and (\n                    metric not in pred_dict or str(pred_dict[metric]).lower() != str(true_value).lower()\n                ):\n                    correctness[metric] = True  # Mark as correct for string comparison\n\n        return correctness  # Return the correctness dictionary\n\n    def eval_all(self, id_list: list, predictions: list) -> dict:\n        \"\"\"\n        Evaluate all predictions and calculate accuracy rates.\n\n        Args:\n            id_list (list): A list of question identifiers.\n            predictions (list): A list of prediction strings corresponding to the questions.\n\n        Returns:\n            dict: A dictionary containing accuracy rates by question and sub-question.\n        \"\"\"\n        results = []  # Initialize a list to store results for each question\n\n        # Evaluate each prediction against its corresponding question ID\n        for id, prediction in zip(id_list, predictions):\n            correct = self.single_eval(id, prediction)  # Evaluate the single prediction\n            results.append({\"id\": id, \"correctness\": correct})  # Append the result to the list\n\n        # Calculate the three accuracy rates based on the results\n        accuracy_by_question = evaluate_accuracy_by_question(results)\n        accuracy_by_sub_question = evaluate_accuracy_by_sub_question(results)\n        proportional_accuracy_by_sub_question = evaluate_accuracy_proportional_by_sub_question_adjusted(results)\n\n        return {\n            \"accuracy_by_question\": accuracy_by_question,\n            \"accuracy_by_sub_question\": accuracy_by_sub_question,\n            \"proportional_accuracy_by_sub_question\": proportional_accuracy_by_sub_question,\n        }",
    "source": "github_repo:FoundationAgents/MetaGPT",
    "file": "examples/di/InfiAgent-DABench/DABench.py",
    "license": "MIT",
    "language": "python"
}