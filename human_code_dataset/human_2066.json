{
    "code": "class Tokenizer(object):\n\n    def __init__(self, dictionary=DEFAULT_DICT):\n        self.lock = threading.RLock()\n        if dictionary == DEFAULT_DICT:\n            self.dictionary = dictionary\n        else:\n            self.dictionary = _get_abs_path(dictionary)\n        self.FREQ = {}\n        self.total = 0\n        self.user_word_tag_tab = {}\n        self.initialized = False\n        self.tmp_dir = None\n        self.cache_file = None\n\n    def __repr__(self):\n        return '<Tokenizer dictionary=%r>' % self.dictionary\n\n    @staticmethod\n    def gen_pfdict(f):\n        lfreq = {}\n        ltotal = 0\n        f_name = resolve_filename(f)\n        for lineno, line in enumerate(f, 1):\n            try:\n                line = line.strip().decode('utf-8')\n                word, freq = line.split(' ')[:2]\n                freq = int(freq)\n                lfreq[word] = freq\n                ltotal += freq\n                for ch in xrange(len(word)):\n                    wfrag = word[:ch + 1]\n                    if wfrag not in lfreq:\n                        lfreq[wfrag] = 0\n            except ValueError:\n                raise ValueError(\n                    'invalid dictionary entry in %s at Line %s: %s' % (f_name, lineno, line))\n        f.close()\n        return lfreq, ltotal\n\n    def initialize(self, dictionary=None):\n        if dictionary:\n            abs_path = _get_abs_path(dictionary)\n            if self.dictionary == abs_path and self.initialized:\n                return\n            else:\n                self.dictionary = abs_path\n                self.initialized = False\n        else:\n            abs_path = self.dictionary\n\n        with self.lock:\n            try:\n                with DICT_WRITING[abs_path]:\n                    pass\n            except KeyError:\n                pass\n            if self.initialized:\n                return\n\n            default_logger.debug(\"Building prefix dict from %s ...\" % (abs_path or 'the default dictionary'))\n            t1 = time.time()\n            if self.cache_file:\n                cache_file = self.cache_file\n            # default dictionary\n            elif abs_path == DEFAULT_DICT:\n                cache_file = \"jieba.cache\"\n            # custom dictionary\n            else:\n                cache_file = \"jieba.u%s.cache\" % md5(\n                    abs_path.encode('utf-8', 'replace')).hexdigest()\n            cache_file = os.path.join(\n                self.tmp_dir or tempfile.gettempdir(), cache_file)\n            # prevent absolute path in self.cache_file\n            tmpdir = os.path.dirname(cache_file)\n\n            load_from_cache_fail = True\n            if os.path.isfile(cache_file) and (abs_path == DEFAULT_DICT or\n                                               os.path.getmtime(cache_file) > os.path.getmtime(abs_path)):\n                default_logger.debug(\n                    \"Loading model from cache %s\" % cache_file)\n                try:\n                    with open(cache_file, 'rb') as cf:\n                        self.FREQ, self.total = marshal.load(cf)\n                    load_from_cache_fail = False\n                except Exception:\n                    load_from_cache_fail = True\n\n            if load_from_cache_fail:\n                wlock = DICT_WRITING.get(abs_path, threading.RLock())\n                DICT_WRITING[abs_path] = wlock\n                with wlock:\n                    self.FREQ, self.total = self.gen_pfdict(self.get_dict_file())\n                    default_logger.debug(\n                        \"Dumping model to file cache %s\" % cache_file)\n                    try:\n                        # prevent moving across different filesystems\n                        fd, fpath = tempfile.mkstemp(dir=tmpdir)\n                        with os.fdopen(fd, 'wb') as temp_cache_file:\n                            marshal.dump(\n                                (self.FREQ, self.total), temp_cache_file)\n                        _replace_file(fpath, cache_file)\n                    except Exception:\n                        default_logger.exception(\"Dump cache file failed.\")\n\n                try:\n                    del DICT_WRITING[abs_path]\n                except KeyError:\n                    pass\n\n            self.initialized = True\n            default_logger.debug(\n                \"Loading model cost %.3f seconds.\" % (time.time() - t1))\n            default_logger.debug(\"Prefix dict has been built successfully.\")\n\n    def check_initialized(self):\n        if not self.initialized:\n            self.initialize()\n\n    def calc(self, sentence, DAG, route):\n        N = len(sentence)\n        route[N] = (0, 0)\n        logtotal = log(self.total)\n        for idx in xrange(N - 1, -1, -1):\n            route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) -\n                              logtotal + route[x + 1][0], x) for x in DAG[idx])\n\n    def get_DAG(self, sentence):\n        self.check_initialized()\n        DAG = {}\n        N = len(sentence)\n        for k in xrange(N):\n            tmplist = []\n            i = k\n            frag = sentence[k]\n            while i < N and frag in self.FREQ:\n                if self.FREQ[frag]:\n                    tmplist.append(i)\n                i += 1\n                frag = sentence[k:i + 1]\n            if not tmplist:\n                tmplist.append(k)\n            DAG[k] = tmplist\n        return DAG\n\n    def __cut_all(self, sentence):\n        dag = self.get_DAG(sentence)\n        old_j = -1\n        eng_scan = 0\n        eng_buf = u''\n        for k, L in iteritems(dag):\n            if eng_scan == 1 and not re_eng.match(sentence[k]):\n                eng_scan = 0\n                yield eng_buf\n            if len(L) == 1 and k > old_j:\n                word = sentence[k:L[0] + 1]\n                if re_eng.match(word):\n                    if eng_scan == 0:\n                        eng_scan = 1\n                        eng_buf = word\n                    else:\n                        eng_buf += word\n                if eng_scan == 0:\n                    yield word\n                old_j = L[0]\n            else:\n                for j in L:\n                    if j > k:\n                        yield sentence[k:j + 1]\n                        old_j = j\n        if eng_scan == 1:\n            yield eng_buf\n\n    def __cut_DAG_NO_HMM(self, sentence):\n        DAG = self.get_DAG(sentence)\n        route = {}\n        self.calc(sentence, DAG, route)\n        x = 0\n        N = len(sentence)\n        buf = ''\n        while x < N:\n            y = route[x][1] + 1\n            l_word = sentence[x:y]\n            if re_eng.match(l_word) and len(l_word) == 1:\n                buf += l_word\n                x = y\n            else:\n                if buf:\n                    yield buf\n                    buf = ''\n                yield l_word\n                x = y\n        if buf:\n            yield buf\n            buf = ''\n\n    def __cut_DAG(self, sentence):\n        DAG = self.get_DAG(sentence)\n        route = {}\n        self.calc(sentence, DAG, route)\n        x = 0\n        buf = ''\n        N = len(sentence)\n        while x < N:\n            y = route[x][1] + 1\n            l_word = sentence[x:y]\n            if y - x == 1:\n                buf += l_word\n            else:\n                if buf:\n                    if len(buf) == 1:\n                        yield buf\n                        buf = ''\n                    else:\n                        if not self.FREQ.get(buf):\n                            recognized = finalseg.cut(buf)\n                            for t in recognized:\n                                yield t\n                        else:\n                            for elem in buf:\n                                yield elem\n                        buf = ''\n                yield l_word\n            x = y\n\n        if buf:\n            if len(buf) == 1:\n                yield buf\n            elif not self.FREQ.get(buf):\n                recognized = finalseg.cut(buf)\n                for t in recognized:\n                    yield t\n            else:\n                for elem in buf:\n                    yield elem\n\n    def cut(self, sentence, cut_all=False, HMM=True, use_paddle=False):\n        \"\"\"\n        The main function that segments an entire sentence that contains\n        Chinese characters into separated words.\n\n        Parameter:\n            - sentence: The str(unicode) to be segmented.\n            - cut_all: Model type. True for full pattern, False for accurate pattern.\n            - HMM: Whether to use the Hidden Markov Model.\n        \"\"\"\n        is_paddle_installed = check_paddle_install['is_paddle_installed']\n        sentence = strdecode(sentence)\n        if use_paddle and is_paddle_installed:\n            # if sentence is null, it will raise core exception in paddle.\n            if sentence is None or len(sentence) == 0:\n                return\n            import jieba.lac_small.predict as predict\n            results = predict.get_sent(sentence)\n            for sent in results:\n                if sent is None:\n                    continue\n                yield sent\n            return\n        re_han = re_han_default\n        re_skip = re_skip_default\n        if cut_all:\n            cut_block = self.__cut_all\n        elif HMM:\n            cut_block = self.__cut_DAG\n        else:\n            cut_block = self.__cut_DAG_NO_HMM\n        blocks = re_han.split(sentence)\n        for blk in blocks:\n            if not blk:\n                continue\n            if re_han.match(blk):\n                for word in cut_block(blk):\n                    yield word\n            else:\n                tmp = re_skip.split(blk)\n                for x in tmp:\n                    if re_skip.match(x):\n                        yield x\n                    elif not cut_all:\n                        for xx in x:\n                            yield xx\n                    else:\n                        yield x\n\n    def cut_for_search(self, sentence, HMM=True):\n        \"\"\"\n        Finer segmentation for search engines.\n        \"\"\"\n        words = self.cut(sentence, HMM=HMM)\n        for w in words:\n            if len(w) > 2:\n                for i in xrange(len(w) - 1):\n                    gram2 = w[i:i + 2]\n                    if self.FREQ.get(gram2):\n                        yield gram2\n            if len(w) > 3:\n                for i in xrange(len(w) - 2):\n                    gram3 = w[i:i + 3]\n                    if self.FREQ.get(gram3):\n                        yield gram3\n            yield w\n\n    def lcut(self, *args, **kwargs):\n        return list(self.cut(*args, **kwargs))\n\n    def lcut_for_search(self, *args, **kwargs):\n        return list(self.cut_for_search(*args, **kwargs))\n\n    _lcut = lcut\n    _lcut_for_search = lcut_for_search\n\n    def _lcut_no_hmm(self, sentence):\n        return self.lcut(sentence, False, False)\n\n    def _lcut_all(self, sentence):\n        return self.lcut(sentence, True)\n\n    def _lcut_for_search_no_hmm(self, sentence):\n        return self.lcut_for_search(sentence, False)\n\n    def get_dict_file(self):\n        if self.dictionary == DEFAULT_DICT:\n            return get_module_res(DEFAULT_DICT_NAME)\n        else:\n            return open(self.dictionary, 'rb')\n\n    def load_userdict(self, f):\n        '''\n        Load personalized dict to improve detect rate.\n\n        Parameter:\n            - f : A plain text file contains words and their ocurrences.\n                  Can be a file-like object, or the path of the dictionary file,\n                  whose encoding must be utf-8.\n\n        Structure of dict file:\n        word1 freq1 word_type1\n        word2 freq2 word_type2\n        ...\n        Word type may be ignored\n        '''\n        self.check_initialized()\n        if isinstance(f, string_types):\n            f_name = f\n            f = open(f, 'rb')\n        else:\n            f_name = resolve_filename(f)\n        for lineno, ln in enumerate(f, 1):\n            line = ln.strip()\n            if not isinstance(line, text_type):\n                try:\n                    line = line.decode('utf-8').lstrip('\\ufeff')\n                except UnicodeDecodeError:\n                    raise ValueError('dictionary file %s must be utf-8' % f_name)\n            if not line:\n                continue\n            # match won't be None because there's at least one character\n            word, freq, tag = re_userdict.match(line).groups()\n            if freq is not None:\n                freq = freq.strip()\n            if tag is not None:\n                tag = tag.strip()\n            self.add_word(word, freq, tag)\n\n    def add_word(self, word, freq=None, tag=None):\n        \"\"\"\n        Add a word to dictionary.\n\n        freq and tag can be omitted, freq defaults to be a calculated value\n        that ensures the word can be cut out.\n        \"\"\"\n        self.check_initialized()\n        word = strdecode(word)\n        freq = int(freq) if freq is not None else self.suggest_freq(word, False)\n        self.FREQ[word] = freq\n        self.total += freq\n        if tag:\n            self.user_word_tag_tab[word] = tag\n        for ch in xrange(len(word)):\n            wfrag = word[:ch + 1]\n            if wfrag not in self.FREQ:\n                self.FREQ[wfrag] = 0\n        if freq == 0:\n            finalseg.add_force_split(word)\n\n    def del_word(self, word):\n        \"\"\"\n        Convenient function for deleting a word.\n        \"\"\"\n        self.add_word(word, 0)\n\n    def suggest_freq(self, segment, tune=False):\n        \"\"\"\n        Suggest word frequency to force the characters in a word to be\n        joined or splitted.\n\n        Parameter:\n            - segment : The segments that the word is expected to be cut into,\n                        If the word should be treated as a whole, use a str.\n            - tune : If True, tune the word frequency.\n\n        Note that HMM may affect the final result. If the result doesn't change,\n        set HMM=False.\n        \"\"\"\n        self.check_initialized()\n        ftotal = float(self.total)\n        freq = 1\n        if isinstance(segment, string_types):\n            word = segment\n            for seg in self.cut(word, HMM=False):\n                freq *= self.FREQ.get(seg, 1) / ftotal\n            freq = max(int(freq * self.total) + 1, self.FREQ.get(word, 1))\n        else:\n            segment = tuple(map(strdecode, segment))\n            word = ''.join(segment)\n            for seg in segment:\n                freq *= self.FREQ.get(seg, 1) / ftotal\n            freq = min(int(freq * self.total), self.FREQ.get(word, 0))\n        if tune:\n            self.add_word(word, freq)\n        return freq\n\n    def tokenize(self, unicode_sentence, mode=\"default\", HMM=True):\n        \"\"\"\n        Tokenize a sentence and yields tuples of (word, start, end)\n\n        Parameter:\n            - sentence: the str(unicode) to be segmented.\n            - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n            - HMM: whether to use the Hidden Markov Model.\n        \"\"\"\n        if not isinstance(unicode_sentence, text_type):\n            raise ValueError(\"jieba: the input parameter should be unicode.\")\n        start = 0\n        if mode == 'default':\n            for w in self.cut(unicode_sentence, HMM=HMM):\n                width = len(w)\n                yield (w, start, start + width)\n                start += width\n        else:\n            for w in self.cut(unicode_sentence, HMM=HMM):\n                width = len(w)\n                if len(w) > 2:\n                    for i in xrange(len(w) - 1):\n                        gram2 = w[i:i + 2]\n                        if self.FREQ.get(gram2):\n                            yield (gram2, start + i, start + i + 2)\n                if len(w) > 3:\n                    for i in xrange(len(w) - 2):\n                        gram3 = w[i:i + 3]\n                        if self.FREQ.get(gram3):\n                            yield (gram3, start + i, start + i + 3)\n                yield (w, start, start + width)\n                start += width\n\n    def set_dictionary(self, dictionary_path):\n        with self.lock:\n            abs_path = _get_abs_path(dictionary_path)\n            if not os.path.isfile(abs_path):\n                raise Exception(\"jieba: file does not exist: \" + abs_path)\n            self.dictionary = abs_path\n            self.initialized = False",
    "source": "github_repo:fxsjy/jieba",
    "file": "jieba/__init__.py",
    "license": "MIT",
    "language": "python"
}