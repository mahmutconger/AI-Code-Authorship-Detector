{
    "code": "class DistributedBucketSampler(Sampler[T_co]):\n    r\"\"\"\n    sort the dataset wrt. input length\n    divide samples into buckets\n    sort within buckets\n    divide buckets into batches\n    sort batches\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: Dataset,\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = True,\n        seed: int = 0,\n        drop_last: bool = False,\n        batch_size: int = 32,\n    ) -> None:\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            num_replicas = dist.get_world_size() if torch.cuda.is_available() else 1\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(\"Requires distributed package to be available\")\n            rank = dist.get_rank() if torch.cuda.is_available() else 0\n            if torch.cuda.is_available():\n                torch.cuda.set_device(rank)\n        if rank >= num_replicas or rank < 0:\n            raise ValueError(\"Invalid rank {}, rank should be in the interval [0, {}]\".format(rank, num_replicas - 1))\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.drop_last = drop_last\n        # If the dataset length is evenly divisible by # of replicas, then there\n        # is no need to drop any data, since the dataset will be split equally.\n        if self.drop_last and len(self.dataset) % self.num_replicas != 0:  # type: ignore[arg-type]\n            # Split to nearest available length that is evenly divisible.\n            # This is to ensure each rank receives the same amount of data when\n            # using this Sampler.\n            self.num_samples = math.ceil(\n                (len(self.dataset) - self.num_replicas) / self.num_replicas,  # type: ignore[arg-type]\n            )\n        else:\n            self.num_samples = math.ceil(\n                len(self.dataset) / self.num_replicas,\n            )  # type: ignore[arg-type]\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n        self.seed = seed\n        self.batch_size = batch_size\n        self.id_with_length = self._get_sample_lengths()\n        self.id_buckets = self.make_buckets(bucket_width=2.0)\n\n    def _get_sample_lengths(self):\n        id_with_lengths = []\n        for i in range(len(self.dataset)):\n            id_with_lengths.append((i, self.dataset.get_sample_length(i)))\n        id_with_lengths.sort(key=lambda x: x[1])\n        return id_with_lengths\n\n    def make_buckets(self, bucket_width: float = 2.0):\n        buckets = []\n        cur = []\n        max_sec = bucket_width\n        for id, sec in self.id_with_length:\n            if sec < max_sec:\n                cur.append(id)\n            else:\n                buckets.append(cur)\n                cur = [id]\n                max_sec += bucket_width\n        if len(cur) > 0:\n            buckets.append(cur)\n        return buckets\n\n    def __iter__(self) -> Iterator[T_co]:\n        if self.shuffle:\n            # deterministically shuffle based on epoch and seed\n            g = torch.Generator()\n            g.manual_seed(self.seed + self.epoch)\n            random.seed(self.epoch + self.seed)\n            shuffled_bucket = []\n            for buc in self.id_buckets:\n                buc_copy = buc.copy()\n                shuffle(buc_copy)\n                shuffled_bucket.append(buc_copy)\n            grouped_batch_size = self.batch_size * self.num_replicas\n            shuffled_bucket = list(itertools.chain(*shuffled_bucket))\n            n_batch = int(math.ceil(len(shuffled_bucket) / grouped_batch_size))\n            batches = [shuffled_bucket[b * grouped_batch_size : (b + 1) * grouped_batch_size] for b in range(n_batch)]\n            shuffle(batches)\n            indices = list(itertools.chain(*batches))\n        else:\n            # type: ignore[arg-type]\n            indices = list(range(len(self.dataset)))\n\n        if not self.drop_last:\n            # add extra samples to make it evenly divisible\n            padding_size = self.total_size - len(indices)\n            if padding_size <= len(indices):\n                indices += indices[:padding_size]\n            else:\n                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]\n        else:\n            # remove tail of data to make it evenly divisible.\n            indices = indices[: self.total_size]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank : self.total_size : self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self) -> int:\n        return self.num_samples\n\n    def set_epoch(self, epoch: int) -> None:\n        r\"\"\"\n        Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas\n        use a different random ordering for each epoch. Otherwise, the next iteration of this\n        sampler will yield the same ordering.\n\n        Args:\n            epoch (int): Epoch number.\n        \"\"\"\n        self.epoch = epoch",
    "source": "github_repo:RVC-Boss/GPT-SoVITS",
    "file": "GPT_SoVITS/AR/data/bucket_sampler.py",
    "license": "MIT",
    "language": "python"
}