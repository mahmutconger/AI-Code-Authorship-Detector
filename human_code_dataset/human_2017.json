{
    "code": "class BaseSpeakerTTS(OpenVoiceBaseClass):\n    language_marks = {\n        \"english\": \"EN\",\n        \"chinese\": \"ZH\",\n    }\n\n    @staticmethod\n    def get_text(text, hps, is_symbol):\n        text_norm = text_to_sequence(text, hps.symbols, [] if is_symbol else hps.data.text_cleaners)\n        if hps.data.add_blank:\n            text_norm = commons.intersperse(text_norm, 0)\n        text_norm = torch.LongTensor(text_norm)\n        return text_norm\n\n    @staticmethod\n    def audio_numpy_concat(segment_data_list, sr, speed=1.):\n        audio_segments = []\n        for segment_data in segment_data_list:\n            audio_segments += segment_data.reshape(-1).tolist()\n            audio_segments += [0] * int((sr * 0.05)/speed)\n        audio_segments = np.array(audio_segments).astype(np.float32)\n        return audio_segments\n\n    @staticmethod\n    def split_sentences_into_pieces(text, language_str):\n        texts = utils.split_sentence(text, language_str=language_str)\n        print(\" > Text splitted to sentences.\")\n        print('\\n'.join(texts))\n        print(\" > ===========================\")\n        return texts\n\n    def tts(self, text, output_path, speaker, language='English', speed=1.0):\n        mark = self.language_marks.get(language.lower(), None)\n        assert mark is not None, f\"language {language} is not supported\"\n\n        texts = self.split_sentences_into_pieces(text, mark)\n\n        audio_list = []\n        for t in texts:\n            t = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', t)\n            t = f'[{mark}]{t}[{mark}]'\n            stn_tst = self.get_text(t, self.hps, False)\n            device = self.device\n            speaker_id = self.hps.speakers[speaker]\n            with torch.no_grad():\n                x_tst = stn_tst.unsqueeze(0).to(device)\n                x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n                sid = torch.LongTensor([speaker_id]).to(device)\n                audio = self.model.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=0.667, noise_scale_w=0.6,\n                                    length_scale=1.0 / speed)[0][0, 0].data.cpu().float().numpy()\n            audio_list.append(audio)\n        audio = self.audio_numpy_concat(audio_list, sr=self.hps.data.sampling_rate, speed=speed)\n\n        if output_path is None:\n            return audio\n        else:\n            soundfile.write(output_path, audio, self.hps.data.sampling_rate)",
    "source": "github_repo:myshell-ai/OpenVoice",
    "file": "openvoice/api.py",
    "license": "MIT",
    "language": "python"
}