{
    "code": "def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: list[str] | None = None,\n        run_manager: CallbackManagerForLLMRun | None = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        \"\"\"Stream the output of the model.\n\n        This method should be implemented if the model can generate output\n        in a streaming fashion. If the model does not support streaming,\n        do not implement it. In that case streaming requests will be automatically\n        handled by the _generate method.\n\n        Args:\n            messages: the prompt composed of a list of messages.\n            stop: a list of strings on which the model should stop generating.\n                If generation stops due to a stop token, the stop token itself\n                SHOULD BE INCLUDED as part of the output. This is not enforced\n                across models right now, but it's a good practice to follow since\n                it makes it much easier to parse the output of the model\n                downstream and understand why generation stopped.\n            run_manager: A run manager with callbacks for the LLM.\n        \"\"\"\n        last_message = messages[-1]\n        tokens = str(last_message.content[: self.parrot_buffer_length])\n        ct_input_tokens = sum(len(message.content) for message in messages)\n\n        for token in tokens:\n            usage_metadata = UsageMetadata(\n                {\n                    \"input_tokens\": ct_input_tokens,\n                    \"output_tokens\": 1,\n                    \"total_tokens\": ct_input_tokens + 1,\n                }\n            )\n            ct_input_tokens = 0\n            chunk = ChatGenerationChunk(\n                message=AIMessageChunk(content=token, usage_metadata=usage_metadata)\n            )\n\n            if run_manager:\n                # This is optional in newer versions of LangChain\n                # The on_llm_new_token will be called automatically\n                run_manager.on_llm_new_token(token, chunk=chunk)\n\n            yield chunk\n\n        # Let's add some other information (e.g., response metadata)\n        chunk = ChatGenerationChunk(\n            message=AIMessageChunk(\n                content=\"\",\n                response_metadata={\"time_in_sec\": 3, \"model_name\": self.model_name},\n            )\n        )\n        if run_manager:\n            # This is optional in newer versions of LangChain\n            # The on_llm_new_token will be called automatically\n            run_manager.on_llm_new_token(token, chunk=chunk)\n        yield chunk",
    "source": "github_repo:langchain-ai/langchain",
    "file": "libs/cli/langchain_cli/integration_template/integration_template/chat_models.py",
    "license": "MIT",
    "language": "python"
}