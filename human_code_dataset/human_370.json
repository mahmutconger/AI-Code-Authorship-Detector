{
    "code": "class DecodingOptions:\n    # whether to perform X->X \"transcribe\" or X->English \"translate\"\n    task: str = \"transcribe\"\n\n    # language that the audio is in; uses detected language if None\n    language: Optional[str] = None\n\n    # sampling-related options\n    temperature: float = 0.0\n    sample_len: Optional[int] = None  # maximum number of tokens to sample\n    best_of: Optional[int] = None  # number of independent sample trajectories, if t > 0\n    beam_size: Optional[int] = None  # number of beams in beam search, if t == 0\n    patience: Optional[float] = None  # patience in beam search (arxiv:2204.05424)\n\n    # \"alpha\" in Google NMT, or None for length norm, when ranking generations\n    # to select which to return among the beams or best-of-N samples\n    length_penalty: Optional[float] = None\n\n    # text or tokens to feed as the prompt or the prefix; for more info:\n    # https://github.com/openai/whisper/discussions/117#discussioncomment-3727051\n    prompt: Optional[Union[str, List[int]]] = None  # for the previous context\n    prefix: Optional[Union[str, List[int]]] = None  # to prefix the current context\n\n    # list of tokens ids (or comma-separated token ids) to suppress\n    # \"-1\" will suppress a set of symbols as defined in `tokenizer.non_speech_tokens()`\n    suppress_tokens: Optional[Union[str, Iterable[int]]] = \"-1\"\n    suppress_blank: bool = True  # this will suppress blank outputs\n\n    # timestamp sampling options\n    without_timestamps: bool = False  # use <|notimestamps|> to sample text tokens only\n    max_initial_timestamp: Optional[float] = 1.0\n\n    # implementation details\n    fp16: bool = True  # use fp16 for most of the calculation",
    "source": "github_repo:openai/whisper",
    "file": "whisper/decoding.py",
    "license": "MIT",
    "language": "python"
}