{
    "code": "class Expert(nn.Module):\n    \"\"\"\n    Expert layer for Mixture-of-Experts (MoE) models.\n\n    Attributes:\n        w1 (nn.Module): Linear layer for input-to-hidden transformation.\n        w2 (nn.Module): Linear layer for hidden-to-output transformation.\n        w3 (nn.Module): Additional linear layer for feature transformation.\n    \"\"\"\n    def __init__(self, dim: int, inter_dim: int):\n        \"\"\"\n        Initializes the Expert layer.\n\n        Args:\n            dim (int): Input and output dimensionality.\n            inter_dim (int): Hidden layer dimensionality.\n        \"\"\"\n        super().__init__()\n        self.w1 = Linear(dim, inter_dim)\n        self.w2 = Linear(inter_dim, dim)\n        self.w3 = Linear(dim, inter_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the Expert layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after expert computation.\n        \"\"\"\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))",
    "source": "github_repo:deepseek-ai/DeepSeek-V3",
    "file": "inference/model.py",
    "license": "MIT",
    "language": "python"
}