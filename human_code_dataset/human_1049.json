{
    "code": "def init_batch(self):\n        semantic_data_len = len(self.semantic_data)\n        phoneme_data_len = len(self.phoneme_data.keys())\n        print(\"semantic_data_len:\", semantic_data_len)\n        print(\"phoneme_data_len:\", phoneme_data_len)\n        print(self.semantic_data)\n        idx = 0\n        num_not_in = 0\n        num_deleted_bigger = 0\n        num_deleted_ps = 0\n        for i in range(semantic_data_len):\n            # 先依次遍历\n            # get str\n            item_name = self.semantic_data.iloc[i, 0]\n            # print(self.phoneme_data)\n            try:\n                phoneme, word2ph, text = self.phoneme_data[item_name]\n            except Exception:\n                traceback.print_exc()\n                # print(f\"{item_name} not in self.phoneme_data !\")\n                num_not_in += 1\n                continue\n\n            semantic_str = self.semantic_data.iloc[i, 1]\n            # get token list\n            semantic_ids = [int(idx) for idx in semantic_str.split(\" \")]\n            # (T), 是否需要变成 (1, T) -> 不需要，因为需要求 len\n            # 过滤掉太长的样本\n            if (\n                len(semantic_ids) > self.max_sec * self.hz\n            ):  #########1###根据token个数推测总时长过滤时长60s（config里）#40*25=1k\n                num_deleted_bigger += 1\n                continue\n            # (T, ), 这个速度不会很慢，所以可以在一开始就处理，无需在 __getitem__ 里面单个处理####\n            phoneme = phoneme.split(\" \")\n\n            try:\n                phoneme_ids = cleaned_text_to_sequence(phoneme, version)\n            except:\n                traceback.print_exc()\n                # print(f\"{item_name} not in self.phoneme_data !\")\n                num_not_in += 1\n                continue\n            # if len(phoneme_ids) >400:###########2：改为恒定限制为semantic/2.5就行\n            if len(phoneme_ids) > self.max_sec * self.hz / 2.5:  ###########2：改为恒定限制为semantic/2.5就行\n                num_deleted_ps += 1\n                continue\n            # if len(semantic_ids) > 1000:###########3\n            #     num_deleted_bigger += 1\n            #     continue\n\n            ps_ratio = len(phoneme_ids) / (len(semantic_ids) / self.hz)\n\n            if ps_ratio > self.max_ps_ratio or ps_ratio < self.min_ps_ratio:  ##########4#3~25#每秒多少个phone\n                num_deleted_ps += 1\n                # print(item_name)\n                continue\n\n            self.semantic_phoneme.append((semantic_ids, phoneme_ids))\n            idx += 1\n            self.item_names.append(item_name)\n\n        min_num = 100  # 20直接不补#30补了也不存ckpt\n        leng = len(self.semantic_phoneme)\n        if leng < min_num:\n            tmp1 = self.semantic_phoneme\n            tmp2 = self.item_names\n            self.semantic_phoneme = []\n            self.item_names = []\n            for _ in range(max(2, int(min_num / leng))):\n                self.semantic_phoneme += tmp1\n                self.item_names += tmp2\n        if num_not_in > 0:\n            print(f\"there are {num_not_in} semantic datas not in phoneme datas\")\n        if num_deleted_bigger > 0:\n            print(\n                f\"deleted {num_deleted_bigger} audios who's duration are bigger than {self.max_sec} seconds\",\n            )\n        if num_deleted_ps > 0:\n            # 4702 for LibriTTS, LirbriTTS 是标注数据, 是否需要筛？=> 需要，有值为 100 的极端值\n            print(\n                f\"deleted {num_deleted_ps} audios who's phoneme/sec are bigger than {self.max_ps_ratio} or smaller than {self.min_ps_ratio}\",\n            )\n        \"\"\"\n        there are 31 semantic datas not in phoneme datas\n        deleted 34 audios who's duration are bigger than 54 seconds\n        deleted 3190 audios who's phoneme/sec are bigger than 25 or smaller than 3\n        dataset.__len__(): 366463\n\n        \"\"\"\n        # 345410 for LibriTTS\n        print(\"dataset.__len__():\", self.__len__())",
    "source": "github_repo:RVC-Boss/GPT-SoVITS",
    "file": "GPT_SoVITS/AR/data/dataset.py",
    "license": "MIT",
    "language": "python"
}