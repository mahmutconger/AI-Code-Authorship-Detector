{
    "code": "import time\nfrom typing import Awaitable, Callable, List\nfrom openai import AsyncOpenAI\nfrom openai.types.chat import ChatCompletionMessageParam, ChatCompletionChunk\nfrom llm import Completion\n\n\nasync def stream_openai_response(\n    messages: List[ChatCompletionMessageParam],\n    api_key: str,\n    base_url: str | None,\n    callback: Callable[[str], Awaitable[None]],\n    model_name: str,\n) -> Completion:\n    start_time = time.time()\n    client = AsyncOpenAI(api_key=api_key, base_url=base_url)\n\n    # Base parameters\n    params = {\n        \"model\": model_name,\n        \"messages\": messages,\n        \"timeout\": 600,\n    }\n\n    # O1 doesn't support streaming or temperature\n    if model_name not in [\"o1-2024-12-17\", \"o4-mini-2025-04-16\", \"o3-2025-04-16\"]:\n        params[\"temperature\"] = 0\n        params[\"stream\"] = True\n\n    # 4.1 series\n    if model_name in [\n        \"gpt-4.1-2025-04-14\",\n        \"gpt-4.1-mini-2025-04-14\",\n        \"gpt-4.1-nano-2025-04-14\",\n    ]:\n        params[\"temperature\"] = 0\n        params[\"stream\"] = True\n        params[\"max_tokens\"] = 20000\n\n    if model_name == \"gpt-4o-2024-05-13\":\n        params[\"max_tokens\"] = 4096\n\n    if model_name == \"gpt-4o-2024-11-20\":\n        params[\"max_tokens\"] = 16384\n\n    if model_name == \"o1-2024-12-17\":\n        params[\"max_completion_tokens\"] = 20000\n\n    if model_name in [\"o4-mini-2025-04-16\", \"o3-2025-04-16\"]:\n        params[\"max_completion_tokens\"] = 20000\n        params[\"stream\"] = True\n        params[\"reasoning_effort\"] = \"high\"\n\n    # O1 doesn't support streaming\n    if model_name == \"o1-2024-12-17\":\n        response = await client.chat.completions.create(**params)  # type: ignore\n        full_response = response.choices[0].message.content  # type: ignore\n    else:\n        stream = await client.chat.completions.create(**params)  # type: ignore\n        full_response = \"\"\n        async for chunk in stream:  # type: ignore\n            assert isinstance(chunk, ChatCompletionChunk)\n            if (\n                chunk.choices\n                and len(chunk.choices) > 0\n                and chunk.choices[0].delta\n                and chunk.choices[0].delta.content\n            ):\n                content = chunk.choices[0].delta.content or \"\"\n                full_response += content\n                await callback(content)\n\n    await client.close()\n\n    completion_time = time.time() - start_time\n    return {\"duration\": completion_time, \"code\": full_response}",
    "source": "github_repo:abi/screenshot-to-code",
    "file": "backend/models/openai_client.py",
    "license": "MIT",
    "language": "python"
}