{
    "code": "def evaluate_accuracy_proportional_by_sub_question_adjusted(results: dict) -> float:\n    \"\"\"\n    Adjust the score based on the number of sub-questions in each result.\n    This function is referenced from https://github.com/InfiAgent/InfiAgent/blob/main/examples/DA-Agent/eval_closed_form.py\n    This function calculates a score for each result by considering the number of sub-questions\n    it contains. Each sub-question is assigned a score of 1 divided by the number of sub-questions.\n    The total score for each result is computed as the sum of all correct sub-questions multiplied\n    by the score per sub-question. Finally, it returns the average score across all results.\n\n    Args:\n        results (dict): A collection of results where each result may contain a 'correctness' field.\n\n    Returns:\n        float: The average score across all results, rounded to four decimal places.\n               Returns 0 if there are no results.\n    \"\"\"\n    total_score = 0\n    for result in results:\n        if \"correctness\" in result:\n            sub_question_count = len(result[\"correctness\"])\n            score_per_sub_question = 1 / sub_question_count if sub_question_count > 0 else 0\n            question_score = sum(result[\"correctness\"].values()) * score_per_sub_question\n            total_score += question_score\n    return round(total_score / len(results), 4) if results else 0",
    "source": "github_repo:FoundationAgents/MetaGPT",
    "file": "examples/di/InfiAgent-DABench/DABench.py",
    "license": "MIT",
    "language": "python"
}