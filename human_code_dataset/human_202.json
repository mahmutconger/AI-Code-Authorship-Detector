{
    "code": "def main(fp8_path, bf16_path):\n    \"\"\"\n    Converts FP8 weights to BF16 and saves the converted weights.\n\n    This function reads FP8 weights from the specified directory, converts them to BF16,\n    and saves the converted weights to another specified directory. It also updates the\n    model index file to reflect the changes.\n\n    Args:\n    fp8_path (str): The path to the directory containing the FP8 weights and model index file.\n    bf16_path (str): The path to the directory where the converted BF16 weights will be saved.\n\n    Raises:\n    KeyError: If a required scale_inv tensor is missing for a weight.\n\n    Notes:\n    - The function assumes that the FP8 weights are stored in safetensor files.\n    - The function caches loaded safetensor files to optimize memory usage.\n    - The function updates the model index file to remove references to scale_inv tensors.\n    \"\"\"\n    torch.set_default_dtype(torch.bfloat16)\n    os.makedirs(bf16_path, exist_ok=True)\n    model_index_file = os.path.join(fp8_path, \"model.safetensors.index.json\")\n    with open(model_index_file, \"r\") as f:\n        model_index = json.load(f)\n    weight_map = model_index[\"weight_map\"]\n    \n    # Cache for loaded safetensor files\n    loaded_files = {}\n    fp8_weight_names = []\n\n    # Helper function to get tensor from the correct file\n    def get_tensor(tensor_name):\n        \"\"\"\n        Retrieves a tensor from the cached safetensor files or loads it from disk if not cached.\n\n        Args:\n            tensor_name (str): The name of the tensor to retrieve.\n\n        Returns:\n            torch.Tensor: The retrieved tensor.\n\n        Raises:\n            KeyError: If the tensor does not exist in the safetensor file.\n        \"\"\"\n        file_name = weight_map[tensor_name]\n        if file_name not in loaded_files:\n            file_path = os.path.join(fp8_path, file_name)\n            loaded_files[file_name] = load_file(file_path, device=\"cuda\")\n        return loaded_files[file_name][tensor_name]\n\n    safetensor_files = list(glob(os.path.join(fp8_path, \"*.safetensors\")))\n    safetensor_files.sort()\n    for safetensor_file in tqdm(safetensor_files):\n        file_name = os.path.basename(safetensor_file)\n        current_state_dict = load_file(safetensor_file, device=\"cuda\")\n        loaded_files[file_name] = current_state_dict\n        \n        new_state_dict = {}\n        for weight_name, weight in current_state_dict.items():\n            if weight_name.endswith(\"_scale_inv\"):\n                continue\n            elif weight.element_size() == 1:  # FP8 weight\n                scale_inv_name = f\"{weight_name}_scale_inv\"\n                try:\n                    # Get scale_inv from the correct file\n                    scale_inv = get_tensor(scale_inv_name)\n                    fp8_weight_names.append(weight_name)\n                    new_state_dict[weight_name] = weight_dequant(weight, scale_inv)\n                except KeyError:\n                    print(f\"Warning: Missing scale_inv tensor for {weight_name}, skipping conversion\")\n                    new_state_dict[weight_name] = weight\n            else:\n                new_state_dict[weight_name] = weight\n                \n        new_safetensor_file = os.path.join(bf16_path, file_name)\n        save_file(new_state_dict, new_safetensor_file)\n        \n        # Memory management: keep only the 2 most recently used files\n        if len(loaded_files) > 2:\n            oldest_file = next(iter(loaded_files))\n            del loaded_files[oldest_file]\n            torch.cuda.empty_cache()\n    \n    # Update model index\n    new_model_index_file = os.path.join(bf16_path, \"model.safetensors.index.json\")\n    for weight_name in fp8_weight_names:\n        scale_inv_name = f\"{weight_name}_scale_inv\"\n        if scale_inv_name in weight_map:\n            weight_map.pop(scale_inv_name)\n    with open(new_model_index_file, \"w\") as f:\n        json.dump({\"metadata\": {}, \"weight_map\": weight_map}, f, indent=2)",
    "source": "github_repo:deepseek-ai/DeepSeek-V3",
    "file": "inference/fp8_cast_bf16.py",
    "license": "MIT",
    "language": "python"
}