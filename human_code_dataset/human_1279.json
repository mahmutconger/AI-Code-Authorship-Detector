{
    "code": "import asyncio\n\nfrom llama_index.core.llama_dataset import download_llama_dataset\nfrom llama_index.core.llama_pack import download_llama_pack\nfrom llama_index.core.evaluation import PairwiseComparisonEvaluator\nfrom llama_index.llms import OpenAI, Gemini\nfrom llama_index.core import ServiceContext\nimport pandas as pd\n\n\nasync def main():\n    # DOWNLOAD LLAMADATASET\n    pairwise_evaluator_dataset, _ = download_llama_dataset(\n        \"MtBenchHumanJudgementDataset\", \"./mt_bench_data\"\n    )\n\n    # DEFINE EVALUATORS\n    gpt_4_context = ServiceContext.from_defaults(\n        llm=OpenAI(temperature=0, model=\"gpt-4\"),\n    )\n\n    gpt_3p5_context = ServiceContext.from_defaults(\n        llm=OpenAI(temperature=0, model=\"gpt-3.5-turbo\"),\n    )\n\n    gemini_pro_context = ServiceContext.from_defaults(\n        llm=Gemini(model=\"models/gemini-pro\", temperature=0)\n    )\n\n    evaluators = {\n        \"gpt-4\": PairwiseComparisonEvaluator(service_context=gpt_4_context),\n        \"gpt-3.5\": PairwiseComparisonEvaluator(service_context=gpt_3p5_context),\n        \"gemini-pro\": PairwiseComparisonEvaluator(service_context=gemini_pro_context),\n    }\n\n    # EVALUATE WITH PACK\n    ############################################################################\n    # NOTE: If have a lower tier subscription for OpenAI API like Usage Tier 1 #\n    # then you'll need to use different batch_size and sleep_time_in_seconds.  #\n    # For Usage Tier 1, settings that seemed to work well were batch_size=5,   #\n    # and sleep_time_in_seconds=15 (as of December 2023.)                      #\n    ############################################################################\n    EvaluatorBenchmarkerPack = download_llama_pack(\"EvaluatorBenchmarkerPack\", \"./pack\")\n    evaluator_benchmarker = EvaluatorBenchmarkerPack(\n        evaluator=evaluators[\"gpt-3.5\"],\n        eval_dataset=pairwise_evaluator_dataset,\n        show_progress=True,\n    )\n    gpt_3p5_benchmark_df = await evaluator_benchmarker.arun(\n        batch_size=100, sleep_time_in_seconds=0\n    )\n\n    evaluator_benchmarker = EvaluatorBenchmarkerPack(\n        evaluator=evaluators[\"gpt-4\"],\n        eval_dataset=pairwise_evaluator_dataset,\n        show_progress=True,\n    )\n    gpt_4_benchmark_df = await evaluator_benchmarker.arun(\n        batch_size=100, sleep_time_in_seconds=0\n    )\n\n    evaluator_benchmarker = EvaluatorBenchmarkerPack(\n        evaluator=evaluators[\"gemini-pro\"],\n        eval_dataset=pairwise_evaluator_dataset,\n        show_progress=True,\n    )\n    gemini_pro_benchmark_df = await evaluator_benchmarker.arun(\n        batch_size=5, sleep_time_in_seconds=0.5\n    )\n\n    benchmark_df = pd.concat(\n        [\n            gpt_3p5_benchmark_df,\n            gpt_4_benchmark_df,\n            gemini_pro_benchmark_df,\n        ],\n        axis=0,\n    )\n    print(benchmark_df)\n\n\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(main)",
    "source": "github_repo:run-llama/llama_index",
    "file": "llama-datasets/mt_bench_humanjudgement/baselines.py",
    "license": "MIT",
    "language": "python"
}