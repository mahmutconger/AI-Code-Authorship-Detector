{
    "code": "def __init__(\n        self,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size=1,\n        p_dropout=0.0,\n        window_size=4,\n        isflow=True,\n        **kwargs\n    ):\n        super().__init__()\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.window_size = window_size\n        # if isflow:\n        #  cond_layer = torch.nn.Conv1d(256, 2*hidden_channels*n_layers, 1)\n        #  self.cond_pre = torch.nn.Conv1d(hidden_channels, 2*hidden_channels, 1)\n        #  self.cond_layer = weight_norm(cond_layer, name='weight')\n        #  self.gin_channels = 256\n        self.cond_layer_idx = self.n_layers\n        if \"gin_channels\" in kwargs:\n            self.gin_channels = kwargs[\"gin_channels\"]\n            if self.gin_channels != 0:\n                self.spk_emb_linear = nn.Linear(self.gin_channels, self.hidden_channels)\n                # vits2 says 3rd block, so idx is 2 by default\n                self.cond_layer_idx = (\n                    kwargs[\"cond_layer_idx\"] if \"cond_layer_idx\" in kwargs else 2\n                )\n                # logging.debug(self.gin_channels, self.cond_layer_idx)\n                assert (\n                    self.cond_layer_idx < self.n_layers\n                ), \"cond_layer_idx should be less than n_layers\"\n        self.drop = nn.Dropout(p_dropout)\n        self.attn_layers = nn.ModuleList()\n        self.norm_layers_1 = nn.ModuleList()\n        self.ffn_layers = nn.ModuleList()\n        self.norm_layers_2 = nn.ModuleList()\n\n        for i in range(self.n_layers):\n            self.attn_layers.append(\n                MultiHeadAttention(\n                    hidden_channels,\n                    hidden_channels,\n                    n_heads,\n                    p_dropout=p_dropout,\n                    window_size=window_size,\n                )\n            )\n            self.norm_layers_1.append(LayerNorm(hidden_channels))\n            self.ffn_layers.append(\n                FFN(\n                    hidden_channels,\n                    hidden_channels,\n                    filter_channels,\n                    kernel_size,\n                    p_dropout=p_dropout,\n                )\n            )\n            self.norm_layers_2.append(LayerNorm(hidden_channels))",
    "source": "github_repo:myshell-ai/OpenVoice",
    "file": "openvoice/attentions.py",
    "license": "MIT",
    "language": "python"
}