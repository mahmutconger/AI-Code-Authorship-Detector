{
    "code": "class Decoder(nn.Module):\n    def __init__(\n        self,\n        hidden_channels,\n        filter_channels,\n        n_heads,\n        n_layers,\n        kernel_size=1,\n        p_dropout=0.0,\n        proximal_bias=False,\n        proximal_init=True,\n        **kwargs\n    ):\n        super().__init__()\n        self.hidden_channels = hidden_channels\n        self.filter_channels = filter_channels\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        self.kernel_size = kernel_size\n        self.p_dropout = p_dropout\n        self.proximal_bias = proximal_bias\n        self.proximal_init = proximal_init\n\n        self.drop = nn.Dropout(p_dropout)\n        self.self_attn_layers = nn.ModuleList()\n        self.norm_layers_0 = nn.ModuleList()\n        self.encdec_attn_layers = nn.ModuleList()\n        self.norm_layers_1 = nn.ModuleList()\n        self.ffn_layers = nn.ModuleList()\n        self.norm_layers_2 = nn.ModuleList()\n        for i in range(self.n_layers):\n            self.self_attn_layers.append(\n                MultiHeadAttention(\n                    hidden_channels,\n                    hidden_channels,\n                    n_heads,\n                    p_dropout=p_dropout,\n                    proximal_bias=proximal_bias,\n                    proximal_init=proximal_init,\n                )\n            )\n            self.norm_layers_0.append(LayerNorm(hidden_channels))\n            self.encdec_attn_layers.append(\n                MultiHeadAttention(\n                    hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout\n                )\n            )\n            self.norm_layers_1.append(LayerNorm(hidden_channels))\n            self.ffn_layers.append(\n                FFN(\n                    hidden_channels,\n                    hidden_channels,\n                    filter_channels,\n                    kernel_size,\n                    p_dropout=p_dropout,\n                    causal=True,\n                )\n            )\n            self.norm_layers_2.append(LayerNorm(hidden_channels))\n\n    def forward(self, x, x_mask, h, h_mask):\n        \"\"\"\n        x: decoder input\n        h: encoder output\n        \"\"\"\n        self_attn_mask = commons.subsequent_mask(x_mask.size(2)).to(\n            device=x.device, dtype=x.dtype\n        )\n        encdec_attn_mask = h_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n        x = x * x_mask\n        for i in range(self.n_layers):\n            y = self.self_attn_layers[i](x, x, self_attn_mask)\n            y = self.drop(y)\n            x = self.norm_layers_0[i](x + y)\n\n            y = self.encdec_attn_layers[i](x, h, encdec_attn_mask)\n            y = self.drop(y)\n            x = self.norm_layers_1[i](x + y)\n\n            y = self.ffn_layers[i](x, x_mask)\n            y = self.drop(y)\n            x = self.norm_layers_2[i](x + y)\n        x = x * x_mask\n        return x",
    "source": "github_repo:myshell-ai/OpenVoice",
    "file": "openvoice/attentions.py",
    "license": "MIT",
    "language": "python"
}