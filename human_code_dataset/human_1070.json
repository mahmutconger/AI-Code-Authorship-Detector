{
    "code": "def process_prompt(\n        self,\n        x: torch.Tensor,\n        attn_mask: torch.Tensor,\n        padding_mask: Optional[torch.Tensor] = None,\n        torch_sdpa: bool = True,\n    ):\n        q, k, v = F.linear(self.to_mask(x, padding_mask), self.qkv_w, self.qkv_b).chunk(3, dim=-1)\n\n        batch_size = q.shape[0]\n        q_len = q.shape[1]\n        kv_len = k.shape[1]\n\n        q = self.to_mask(q, padding_mask)\n        k_cache = self.to_mask(k, padding_mask)\n        v_cache = self.to_mask(v, padding_mask)\n\n        q = q.view(batch_size, q_len, self.num_heads, -1).transpose(1, 2)\n        k = k_cache.view(batch_size, kv_len, self.num_heads, -1).transpose(1, 2)\n        v = v_cache.view(batch_size, kv_len, self.num_heads, -1).transpose(1, 2)\n\n        if torch_sdpa:\n            attn = F.scaled_dot_product_attention(q, k, v, ~attn_mask)\n        else:\n            attn = scaled_dot_product_attention(q, k, v, attn_mask)\n\n        attn = attn.transpose(1, 2).reshape(batch_size, q_len, -1)\n        attn = F.linear(self.to_mask(attn, padding_mask), self.out_w, self.out_b)\n\n        x = x + attn\n        x = F.layer_norm(x, [self.hidden_dim], self.norm_w1, self.norm_b1, self.norm_eps1)\n        x = x + self.mlp.forward(x)\n        x = F.layer_norm(\n            x,\n            [self.hidden_dim],\n            self.norm_w2,\n            self.norm_b2,\n            self.norm_eps2,\n        )\n        return x, k_cache, v_cache",
    "source": "github_repo:RVC-Boss/GPT-SoVITS",
    "file": "GPT_SoVITS/AR/models/t2s_model.py",
    "license": "MIT",
    "language": "python"
}