{
    "code": "def main(\n    project_path: str = typer.Argument(\".\", help=\"path\"),\n    model: str = typer.Option(\n        os.environ.get(\"MODEL_NAME\", \"gpt-4o\"), \"--model\", \"-m\", help=\"model id string\"\n    ),\n    temperature: float = typer.Option(\n        0.1,\n        \"--temperature\",\n        \"-t\",\n        help=\"Controls randomness: lower values for more focused, deterministic outputs\",\n    ),\n    improve_mode: bool = typer.Option(\n        False,\n        \"--improve\",\n        \"-i\",\n        help=\"Improve an existing project by modifying the files.\",\n    ),\n    lite_mode: bool = typer.Option(\n        False,\n        \"--lite\",\n        \"-l\",\n        help=\"Lite mode: run a generation using only the main prompt.\",\n    ),\n    clarify_mode: bool = typer.Option(\n        False,\n        \"--clarify\",\n        \"-c\",\n        help=\"Clarify mode - discuss specification with AI before implementation.\",\n    ),\n    self_heal_mode: bool = typer.Option(\n        False,\n        \"--self-heal\",\n        \"-sh\",\n        help=\"Self-heal mode - fix the code by itself when it fails.\",\n    ),\n    azure_endpoint: str = typer.Option(\n        \"\",\n        \"--azure\",\n        \"-a\",\n        help=\"\"\"Endpoint for your Azure OpenAI Service (https://xx.openai.azure.com).\n            In that case, the given model is the deployment name chosen in the Azure AI Studio.\"\"\",\n    ),\n    use_custom_preprompts: bool = typer.Option(\n        False,\n        \"--use-custom-preprompts\",\n        help=\"\"\"Use your project's custom preprompts instead of the default ones.\n          Copies all original preprompts to the project's workspace if they don't exist there.\"\"\",\n    ),\n    llm_via_clipboard: bool = typer.Option(\n        False,\n        \"--llm-via-clipboard\",\n        help=\"Use the clipboard to communicate with the AI.\",\n    ),\n    verbose: bool = typer.Option(\n        False, \"--verbose\", \"-v\", help=\"Enable verbose logging for debugging.\"\n    ),\n    debug: bool = typer.Option(\n        False, \"--debug\", \"-d\", help=\"Enable debug mode for debugging.\"\n    ),\n    prompt_file: str = typer.Option(\n        \"prompt\",\n        \"--prompt_file\",\n        help=\"Relative path to a text file containing a prompt.\",\n    ),\n    entrypoint_prompt_file: str = typer.Option(\n        \"\",\n        \"--entrypoint_prompt\",\n        help=\"Relative path to a text file containing a file that specifies requirements for you entrypoint.\",\n    ),\n    image_directory: str = typer.Option(\n        \"\",\n        \"--image_directory\",\n        help=\"Relative path to a folder containing images.\",\n    ),\n    use_cache: bool = typer.Option(\n        False,\n        \"--use_cache\",\n        help=\"Speeds up computations and saves tokens when running the same prompt multiple times by caching the LLM response.\",\n    ),\n    skip_file_selection: bool = typer.Option(\n        False,\n        \"--skip-file-selection\",\n        \"-s\",\n        help=\"Skip interactive file selection in improve mode and use the generated TOML file directly.\",\n    ),\n    no_execution: bool = typer.Option(\n        False,\n        \"--no_execution\",\n        help=\"Run setup but to not call LLM or write any code. For testing purposes.\",\n    ),\n    sysinfo: bool = typer.Option(\n        False,\n        \"--sysinfo\",\n        help=\"Output system information for debugging\",\n    ),\n    diff_timeout: int = typer.Option(\n        3,\n        \"--diff_timeout\",\n        help=\"Diff regexp timeout. Default: 3. Increase if regexp search timeouts.\",\n    ),\n):\n    \"\"\"\n    The main entry point for the CLI tool that generates or improves a project.\n\n    This function sets up the CLI tool, loads environment variables, initializes\n    the AI, and processes the user's request to generate or improve a project\n    based on the provided arguments.\n\n    Parameters\n    ----------\n    project_path : str\n        The file path to the project directory.\n    model : str\n        The model ID string for the AI.\n    temperature : float\n        The temperature setting for the AI's responses.\n    improve_mode : bool\n        Flag indicating whether to improve an existing project.\n    lite_mode : bool\n        Flag indicating whether to run in lite mode.\n    clarify_mode : bool\n        Flag indicating whether to discuss specifications with AI before implementation.\n    self_heal_mode : bool\n        Flag indicating whether to enable self-healing mode.\n    azure_endpoint : str\n        The endpoint for Azure OpenAI services.\n    use_custom_preprompts : bool\n        Flag indicating whether to use custom preprompts.\n    prompt_file : str\n        Relative path to a text file containing a prompt.\n    entrypoint_prompt_file: str\n        Relative path to a text file containing a file that specifies requirements for you entrypoint.\n    image_directory: str\n        Relative path to a folder containing images.\n    use_cache: bool\n        Speeds up computations and saves tokens when running the same prompt multiple times by caching the LLM response.\n    verbose : bool\n        Flag indicating whether to enable verbose logging.\n    skip_file_selection: bool\n        Skip interactive file selection in improve mode and use the generated TOML file directly\n    no_execution: bool\n        Run setup but to not call LLM or write any code. For testing purposes.\n    sysinfo: bool\n        Flag indicating whether to output system information for debugging.\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    if debug:\n        import pdb\n\n        sys.excepthook = lambda *_: pdb.pm()\n\n    if sysinfo:\n        sys_info = get_system_info()\n        for key, value in sys_info.items():\n            print(f\"{key}: {value}\")\n        raise typer.Exit()\n\n    # Validate arguments\n    if improve_mode and (clarify_mode or lite_mode):\n        typer.echo(\"Error: Clarify and lite mode are not compatible with improve mode.\")\n        raise typer.Exit(code=1)\n\n    # Set up logging\n    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)\n    if use_cache:\n        set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n    if improve_mode:\n        assert not (\n            clarify_mode or lite_mode\n        ), \"Clarify and lite mode are not active for improve mode\"\n\n    load_env_if_needed()\n\n    if llm_via_clipboard:\n        ai = ClipboardAI()\n    else:\n        ai = AI(\n            model_name=model,\n            temperature=temperature,\n            azure_endpoint=azure_endpoint,\n        )\n\n    path = Path(project_path)\n    print(\"Running gpt-engineer in\", path.absolute(), \"\\n\")\n\n    prompt = load_prompt(\n        DiskMemory(path),\n        improve_mode,\n        prompt_file,\n        image_directory,\n        entrypoint_prompt_file,\n    )\n\n    # todo: if ai.vision is false and not llm_via_clipboard - ask if they would like to use gpt-4-vision-preview instead? If so recreate AI\n    if not ai.vision:\n        prompt.image_urls = None\n\n    # configure generation function\n    if clarify_mode:\n        code_gen_fn = clarified_gen\n    elif lite_mode:\n        code_gen_fn = lite_gen\n    else:\n        code_gen_fn = gen_code\n\n    # configure execution function\n    if self_heal_mode:\n        execution_fn = self_heal\n    else:\n        execution_fn = execute_entrypoint\n\n    preprompts_holder = PrepromptsHolder(\n        get_preprompts_path(use_custom_preprompts, Path(project_path))\n    )\n\n    memory = DiskMemory(memory_path(project_path))\n    memory.archive_logs()\n\n    execution_env = DiskExecutionEnv()\n    agent = CliAgent.with_default_config(\n        memory,\n        execution_env,\n        ai=ai,\n        code_gen_fn=code_gen_fn,\n        improve_fn=improve_fn,\n        process_code_fn=execution_fn,\n        preprompts_holder=preprompts_holder,\n    )\n\n    files = FileStore(project_path)\n    if not no_execution:\n        if improve_mode:\n            files_dict_before, is_linting = FileSelector(project_path).ask_for_files(\n                skip_file_selection=skip_file_selection\n            )\n\n            # lint the code\n            if is_linting:\n                files_dict_before = files.linting(files_dict_before)\n\n            files_dict = handle_improve_mode(\n                prompt, agent, memory, files_dict_before, diff_timeout=diff_timeout\n            )\n            if not files_dict or files_dict_before == files_dict:\n                print(\n                    f\"No changes applied. Could you please upload the debug_log_file.txt in {memory.path}/logs folder in a github issue?\"\n                )\n\n            else:\n                print(\"\\nChanges to be made:\")\n                compare(files_dict_before, files_dict)\n\n                print()\n                print(colored(\"Do you want to apply these changes?\", \"light_green\"))\n                if not prompt_yesno():\n                    files_dict = files_dict_before\n\n        else:\n            files_dict = agent.init(prompt)\n            # collect user feedback if user consents\n            config = (code_gen_fn.__name__, execution_fn.__name__)\n            collect_and_send_human_review(prompt, model, temperature, config, memory)\n\n        stage_uncommitted_to_git(path, files_dict, improve_mode)\n\n        files.push(files_dict)\n\n    if ai.token_usage_log.is_openai_model():\n        print(\"Total api cost: $ \", ai.token_usage_log.usage_cost())\n    elif os.getenv(\"LOCAL_MODEL\"):\n        print(\"Total api cost: $ 0.0 since we are using local LLM.\")\n    else:\n        print(\"Total tokens used: \", ai.token_usage_log.total_tokens())",
    "source": "github_repo:AntonOsika/gpt-engineer",
    "file": "gpt_engineer/applications/cli/main.py",
    "license": "MIT",
    "language": "python"
}