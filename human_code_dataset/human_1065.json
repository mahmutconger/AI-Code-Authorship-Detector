{
    "code": "class Text2SemanticDecoder(nn.Module):\n    def __init__(self, config, norm_first=False, top_k=3):\n        super(Text2SemanticDecoder, self).__init__()\n        self.model_dim = config[\"model\"][\"hidden_dim\"]\n        self.embedding_dim = config[\"model\"][\"embedding_dim\"]\n        self.num_head = config[\"model\"][\"head\"]\n        self.num_layers = config[\"model\"][\"n_layer\"]\n        self.norm_first = norm_first\n        self.vocab_size = config[\"model\"][\"vocab_size\"]\n        self.phoneme_vocab_size = config[\"model\"][\"phoneme_vocab_size\"]\n        self.p_dropout = config[\"model\"][\"dropout\"]\n        self.EOS = config[\"model\"][\"EOS\"]\n        self.norm_first = norm_first\n        assert self.EOS == self.vocab_size - 1\n        # should be same as num of kmeans bin\n        # assert self.EOS == 1024\n        self.bert_proj = nn.Linear(1024, self.embedding_dim)\n        self.ar_text_embedding = TokenEmbedding(\n            self.embedding_dim,\n            self.phoneme_vocab_size,\n            self.p_dropout,\n        )\n        self.ar_text_position = SinePositionalEmbedding(\n            self.embedding_dim,\n            dropout=0.1,\n            scale=False,\n            alpha=True,\n        )\n        self.ar_audio_embedding = TokenEmbedding(\n            self.embedding_dim,\n            self.vocab_size,\n            self.p_dropout,\n        )\n        self.ar_audio_position = SinePositionalEmbedding(\n            self.embedding_dim,\n            dropout=0.1,\n            scale=False,\n            alpha=True,\n        )\n\n        self.h = TransformerEncoder(\n            TransformerEncoderLayer(\n                d_model=self.model_dim,\n                nhead=self.num_head,\n                dim_feedforward=self.model_dim * 4,\n                dropout=0.1,\n                batch_first=True,\n                norm_first=norm_first,\n            ),\n            num_layers=self.num_layers,\n            norm=LayerNorm(self.model_dim) if norm_first else None,\n        )\n\n        self.ar_predict_layer = nn.Linear(self.model_dim, self.vocab_size, bias=False)\n        self.loss_fct = nn.CrossEntropyLoss(reduction=\"sum\")\n\n        self.ar_accuracy_metric = MulticlassAccuracy(\n            self.vocab_size,\n            top_k=top_k,\n            average=\"micro\",\n            multidim_average=\"global\",\n            ignore_index=self.EOS,\n        )\n\n        blocks = []\n\n        for i in range(self.num_layers):\n            layer = self.h.layers[i]\n            t2smlp = T2SMLP(\n                layer.linear1.weight,\n                layer.linear1.bias,\n                layer.linear2.weight,\n                layer.linear2.bias,\n            )\n\n            block = T2SBlock(\n                self.num_head,\n                self.model_dim,\n                t2smlp,\n                layer.self_attn.in_proj_weight,\n                layer.self_attn.in_proj_bias,\n                layer.self_attn.out_proj.weight,\n                layer.self_attn.out_proj.bias,\n                layer.norm1.weight,\n                layer.norm1.bias,\n                layer.norm1.eps,\n                layer.norm2.weight,\n                layer.norm2.bias,\n                layer.norm2.eps,\n            )\n\n            blocks.append(block)\n\n        self.t2s_transformer = T2STransformer(self.num_layers, blocks)\n\n    def make_input_data(self, x, x_lens, y, y_lens, bert_feature):\n        x = self.ar_text_embedding(x)\n        x = x + self.bert_proj(bert_feature.transpose(1, 2))\n        x = self.ar_text_position(x)\n        x_mask = make_pad_mask_left(x_lens)\n\n        y_mask = make_pad_mask(y_lens)\n        y_mask_int = y_mask.type(torch.int64)\n        codes = y.type(torch.int64) * (1 - y_mask_int)\n\n        # Training\n        # AR Decoder\n        y, targets = self.pad_y_eos(codes, y_mask_int, eos_id=self.EOS)\n        x_len = x_lens.max()\n        y_len = y_lens.max()\n        y_emb = self.ar_audio_embedding(y)\n        y_pos = self.ar_audio_position(y_emb)\n\n        xy_padding_mask = torch.concat([x_mask, y_mask], dim=1)\n\n        ar_xy_padding_mask = xy_padding_mask\n\n        x_attn_mask = F.pad(\n            torch.zeros((x_len, x_len), dtype=torch.bool, device=x.device),\n            (0, y_len),\n            value=True,\n        )\n        # x_attn_mask[:, x_len]=False\n        y_attn_mask = F.pad(\n            torch.triu(\n                torch.ones(y_len, y_len, dtype=torch.bool, device=x.device),\n                diagonal=1,\n            ),\n            (x_len, 0),\n            value=False,\n        )\n\n        xy_attn_mask = torch.concat([x_attn_mask, y_attn_mask], dim=0)\n        bsz, src_len = x.shape[0], x_len + y_len\n        _xy_padding_mask = (\n            ar_xy_padding_mask.view(bsz, 1, 1, src_len)\n            .expand(-1, self.num_head, -1, -1)\n            .reshape(bsz * self.num_head, 1, src_len)\n        )\n        xy_attn_mask = xy_attn_mask.logical_or(_xy_padding_mask)\n        new_attn_mask = torch.zeros_like(xy_attn_mask, dtype=x.dtype)\n        new_attn_mask.masked_fill_(xy_attn_mask, float(\"-inf\"))\n        xy_attn_mask = new_attn_mask\n        # x 和完整的 y 一次性输入模型\n        xy_pos = torch.concat([x, y_pos], dim=1)\n\n        return xy_pos, xy_attn_mask, targets\n\n    def forward(self, x, x_lens, y, y_lens, bert_feature):\n        \"\"\"\n        x: phoneme_ids\n        y: semantic_ids\n        \"\"\"\n\n        reject_y, reject_y_lens = make_reject_y(y, y_lens)\n\n        xy_pos, xy_attn_mask, targets = self.make_input_data(x, x_lens, y, y_lens, bert_feature)\n\n        xy_dec, _ = self.h(\n            (xy_pos, None),\n            mask=xy_attn_mask,\n        )\n        x_len = x_lens.max()\n        logits = self.ar_predict_layer(xy_dec[:, x_len-1:])\n\n        ###### DPO #############\n        reject_xy_pos, reject_xy_attn_mask, reject_targets = self.make_input_data(\n            x, x_lens, reject_y, reject_y_lens, bert_feature\n        )\n\n        reject_xy_dec, _ = self.h(\n            (reject_xy_pos, None),\n            mask=reject_xy_attn_mask,\n        )\n        x_len = x_lens.max()\n        reject_logits = self.ar_predict_layer(reject_xy_dec[:, x_len-1:])\n\n        # loss\n        # from feiteng: 每次 duration 越多, 梯度更新也应该更多, 所以用 sum\n\n        loss_1 = F.cross_entropy(logits.permute(0, 2, 1), targets, reduction=\"sum\")\n        acc = self.ar_accuracy_metric(logits.permute(0, 2, 1).detach(), targets).item()\n\n        A_logits, R_logits = get_batch_logps(logits, reject_logits, targets, reject_targets)\n        loss_2, _, _ = dpo_loss(A_logits, R_logits, 0, 0, 0.2, reference_free=True)\n\n        loss = loss_1 + loss_2\n\n        return loss, acc\n\n    def forward_old(self, x, x_lens, y, y_lens, bert_feature):\n        \"\"\"\n        x: phoneme_ids\n        y: semantic_ids\n        \"\"\"\n        x = self.ar_text_embedding(x)\n        x = x + self.bert_proj(bert_feature.transpose(1, 2))\n        x = self.ar_text_position(x)\n        x_mask = make_pad_mask_left(x_lens)\n\n        y_mask = make_pad_mask(y_lens)\n        y_mask_int = y_mask.type(torch.int64)\n        codes = y.type(torch.int64) * (1 - y_mask_int)\n\n        # Training\n        # AR Decoder\n        y, targets = self.pad_y_eos(codes, y_mask_int, eos_id=self.EOS)\n        x_len = x_lens.max()\n        y_len = y_lens.max()\n        y_emb = self.ar_audio_embedding(y)\n        y_pos = self.ar_audio_position(y_emb)\n\n        xy_padding_mask = torch.concat([x_mask, y_mask], dim=1)\n        ar_xy_padding_mask = xy_padding_mask\n\n        x_attn_mask = F.pad(\n            torch.zeros((x_len, x_len), dtype=torch.bool, device=x.device),\n            (0, y_len),\n            value=True,\n        )\n        y_attn_mask = F.pad(\n            torch.triu(\n                torch.ones(y_len, y_len, dtype=torch.bool, device=x.device),\n                diagonal=1,\n            ),\n            (x_len, 0),\n            value=False,\n        )\n        xy_attn_mask = torch.concat([x_attn_mask, y_attn_mask], dim=0)\n        bsz, src_len = x.shape[0], x_len + y_len\n        _xy_padding_mask = (\n            ar_xy_padding_mask.view(bsz, 1, 1, src_len)\n            .expand(-1, self.num_head, -1, -1)\n            .reshape(bsz * self.num_head, 1, src_len)\n        )\n        xy_attn_mask = xy_attn_mask.logical_or(_xy_padding_mask)\n        new_attn_mask = torch.zeros_like(xy_attn_mask, dtype=x.dtype)\n        new_attn_mask.masked_fill_(xy_attn_mask, float(\"-inf\"))\n        xy_attn_mask = new_attn_mask\n        # x 和完整的 y 一次性输入模型\n        xy_pos = torch.concat([x, y_pos], dim=1)\n        xy_dec, _ = self.h(\n            (xy_pos, None),\n            mask=xy_attn_mask,\n        )\n        logits = self.ar_predict_layer(xy_dec[:, x_len-1:]).permute(0, 2, 1)\n        # loss\n        # from feiteng: 每次 duration 越多, 梯度更新也应该更多, 所以用 sum\n        loss = F.cross_entropy(logits, targets, reduction=\"sum\")\n        acc = self.ar_accuracy_metric(logits.detach(), targets).item()\n        return loss, acc\n\n    # 需要看下这个函数和 forward 的区别以及没有 semantic 的时候 prompts 输入什么\n    def infer(\n        self,\n        x,\n        x_lens,\n        prompts,\n        bert_feature,\n        top_k: int = -100,\n        early_stop_num: int = -1,\n        temperature: float = 1.0,\n    ):\n        x = self.ar_text_embedding(x)\n        x = x + self.bert_proj(bert_feature.transpose(1, 2))\n        x = self.ar_text_position(x)\n\n        # AR Decoder\n        y = prompts\n        prefix_len = y.shape[1]\n        x_len = x.shape[1]\n        x_attn_mask = torch.zeros((x_len, x_len), dtype=torch.bool)\n        stop = False\n        for _ in tqdm(range(1500)):\n            y_emb = self.ar_audio_embedding(y)\n            y_pos = self.ar_audio_position(y_emb)\n            # x 和逐渐增长的 y 一起输入给模型\n            xy_pos = torch.concat([x, y_pos], dim=1)\n            y_len = y.shape[1]\n            x_attn_mask_pad = F.pad(\n                x_attn_mask,\n                (0, y_len),\n                value=True,\n            )\n            y_attn_mask = F.pad(\n                torch.triu(torch.ones(y_len, y_len, dtype=torch.bool), diagonal=1),\n                (x_len, 0),\n                value=False,\n            )\n            xy_attn_mask = torch.concat([x_attn_mask_pad, y_attn_mask], dim=0).to(y.device)\n\n            xy_dec, _ = self.h(\n                (xy_pos, None),\n                mask=xy_attn_mask,\n            )\n            logits = self.ar_predict_layer(xy_dec[:, -1])\n            samples = topk_sampling(logits, top_k=top_k, top_p=1.0, temperature=temperature)\n\n            if early_stop_num != -1 and (y.shape[1] - prefix_len) > early_stop_num:\n                print(\"use early stop num:\", early_stop_num)\n                stop = True\n\n            if torch.argmax(logits, dim=-1)[0] == self.EOS or samples[0, 0] == self.EOS:\n                # print(torch.argmax(logits, dim=-1)[0] == self.EOS, samples[0, 0] == self.EOS)\n                stop = True\n            if stop:\n                if prompts.shape[1] == y.shape[1]:\n                    y = torch.concat([y, torch.zeros_like(samples)], dim=1)\n                    print(\"bad zero prediction\")\n                print(f\"T2S Decoding EOS [{prefix_len} -> {y.shape[1]}]\")\n                break\n            # 本次生成的 semantic_ids 和之前的 y 构成新的 y\n            # print(samples.shape)#[1,1]#第一个1是bs\n            # import os\n            # os._exit(2333)\n            y = torch.concat([y, samples], dim=1)\n        return y\n\n    def pad_y_eos(self, y, y_mask_int, eos_id):\n        targets = F.pad(y, (0, 1), value=0) + eos_id * F.pad(y_mask_int, (0, 1), value=1)\n        # 错位\n        return targets[:, :-1], targets\n\n    def infer_panel_batch_infer(\n        self,\n        x: List[torch.LongTensor],  #####全部文本token\n        x_lens: torch.LongTensor,\n        prompts: torch.LongTensor,  ####参考音频token\n        bert_feature: List[torch.LongTensor],\n        top_k: int = -100,\n        top_p: int = 100,\n        early_stop_num: int = -1,\n        temperature: float = 1.0,\n        repetition_penalty: float = 1.35,\n        **kwargs,\n    ):\n        if prompts is None:\n            print(\"Warning: Prompt free is not supported batch_infer! switch to naive_infer\")\n            return self.infer_panel_naive_batched(\n                x,\n                x_lens,\n                prompts,\n                bert_feature,\n                top_k=top_k,\n                top_p=top_p,\n                early_stop_num=early_stop_num,\n                temperature=temperature,\n                **kwargs,\n            )\n\n        max_len = kwargs.get(\"max_len\", x_lens.max())\n        x_list = []\n        for x_item, bert_item in zip(x, bert_feature):\n            # max_len = max(max_len, x_item.shape[0], bert_item.shape[1])\n            x_item = self.ar_text_embedding(x_item.unsqueeze(0))\n            x_item = x_item + self.bert_proj(bert_item.transpose(0, 1).unsqueeze(0))\n            x_item = self.ar_text_position(x_item).squeeze(0)\n            # x_item = F.pad(x_item,(0,0,0,max_len-x_item.shape[0]),value=0) if x_item.shape[0]<max_len else x_item  ### padding right\n            x_item = (\n                F.pad(x_item, (0, 0, max_len - x_item.shape[0], 0), value=0) if x_item.shape[0] < max_len else x_item\n            )  ### padding left\n            x_list.append(x_item)\n        x: torch.Tensor = torch.stack(x_list, dim=0)\n\n        # AR Decoder\n        y = prompts\n\n        x_len = x.shape[1]\n        stop = False\n\n        k_cache = None\n        v_cache = None\n        ###################  first step ##########################\n        assert y is not None, \"Error: Prompt free is not supported batch_infer!\"\n        ref_free = False\n\n        y_emb = self.ar_audio_embedding(y)\n        y_len = y_emb.shape[1]\n        prefix_len = y.shape[1]\n        y_lens = torch.LongTensor([y_emb.shape[1]] * y_emb.shape[0]).to(x.device)\n        y_pos = self.ar_audio_position(y_emb)\n        xy_pos = torch.concat([x, y_pos], dim=1)\n\n        ##### create mask #####\n        bsz = x.shape[0]\n        src_len = x_len + y_len\n        y_paddind_mask = make_pad_mask_left(y_lens, y_len)\n        x_paddind_mask = make_pad_mask_left(x_lens, max_len)\n\n        # (bsz, x_len + y_len)\n        padding_mask = torch.concat([x_paddind_mask, y_paddind_mask], dim=1)\n\n        x_mask = F.pad(\n            torch.zeros(x_len, x_len, dtype=torch.bool, device=x.device),\n            (0, y_len),\n            value=True,\n        )\n\n        y_mask = F.pad(  ###yy的右上1扩展到左边xy的0,(y,x+y)\n            torch.triu(torch.ones(y_len, y_len, dtype=torch.bool, device=x.device), diagonal=1),\n            (x_len, 0),\n            value=False,\n        )\n\n        causal_mask = torch.concat([x_mask, y_mask], dim=0).view(1, src_len, src_len).repeat(bsz, 1, 1).to(x.device)\n        # padding_mask = padding_mask.unsqueeze(1) * padding_mask.unsqueeze(2) ### [b, x+y, x+y]\n        ### 上面是错误的，会导致padding的token被\"看见\"\n\n        # 正确的padding_mask应该是：\n        # |   pad_len   |  x_len  |  y_len  |\n        # [[PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],  前3行按理说也应该被mask掉，但是为了防止计算attention时不出现nan，还是保留了，不影响结果\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6],\n        # [PAD, PAD, PAD, 1, 2, 3, 4, 5, 6]]\n\n        padding_mask = padding_mask.view(bsz, 1, src_len).repeat(1, src_len, 1)\n\n        attn_mask: torch.Tensor = causal_mask.logical_or(padding_mask)\n        attn_mask = attn_mask.unsqueeze(1).expand(-1, self.num_head, -1, -1).bool()\n\n        # 正确的attn_mask应该是这样的：\n        # |   pad_len   |  x_len  |  y_len  |\n        # [[PAD, PAD, PAD, 1, 2, 3, EOS, EOS, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3, EOS, EOS, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3, EOS, EOS, EOS],  前3行按理说也应该被mask掉，但是为了防止计算attention时不出现nan，还是保留了，不影响结果\n        # [PAD, PAD, PAD, 1, 2, 3, EOS, EOS, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3, EOS, EOS, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3, EOS, EOS, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3,   4, EOS, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3,   4,   5, EOS],\n        # [PAD, PAD, PAD, 1, 2, 3,   4,   5,   6]]\n\n        ###### decode #####\n        y_list = [None] * y.shape[0]\n        batch_idx_map = list(range(y.shape[0]))\n        idx_list = [None] * y.shape[0]\n        for idx in tqdm(range(1500)):\n            if idx == 0:\n                xy_dec, k_cache, v_cache = self.t2s_transformer.process_prompt(xy_pos, attn_mask, None)\n            else:\n                xy_dec, k_cache, v_cache = self.t2s_transformer.decode_next_token(xy_pos, k_cache, v_cache, attn_mask)\n            logits = self.ar_predict_layer(xy_dec[:, -1])\n\n            if idx == 0:\n                attn_mask = F.pad(attn_mask[:, :, -1].unsqueeze(-2), (0, 1), value=False)\n                logits = logits[:, :-1]\n            else:\n                attn_mask = F.pad(attn_mask, (0, 1), value=False)\n\n            samples = sample(\n                logits, y, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty, temperature=temperature\n            )[0]\n\n            y = torch.concat([y, samples], dim=1)\n\n            ####### 移除batch中已经生成完毕的序列,进一步优化计算量\n            tokens = torch.argmax(logits, dim=-1)\n            reserved_idx_of_batch_for_y = None\n            if (self.EOS in samples[:, 0]) or (self.EOS in tokens):  ###如果生成到EOS，则停止\n                l1 = samples[:, 0] == self.EOS\n                l2 = tokens == self.EOS\n                l = l1.logical_or(l2)\n                removed_idx_of_batch_for_y = torch.where(l == True)[0].tolist()\n                reserved_idx_of_batch_for_y = torch.where(l == False)[0]\n                # batch_indexs = torch.tensor(batch_idx_map, device=y.device)[removed_idx_of_batch_for_y]\n                for i in removed_idx_of_batch_for_y:\n                    batch_index = batch_idx_map[i]\n                    idx_list[batch_index] = idx\n                    y_list[batch_index] = y[i, :-1]\n\n                batch_idx_map = [batch_idx_map[i] for i in reserved_idx_of_batch_for_y.tolist()]\n\n            # 只保留batch中未生成完毕的序列\n            if reserved_idx_of_batch_for_y is not None:\n                # index = torch.LongTensor(batch_idx_map).to(y.device)\n                y = torch.index_select(y, dim=0, index=reserved_idx_of_batch_for_y)\n                attn_mask = torch.index_select(attn_mask, dim=0, index=reserved_idx_of_batch_for_y)\n                if k_cache is not None:\n                    for i in range(len(k_cache)):\n                        k_cache[i] = torch.index_select(k_cache[i], dim=0, index=reserved_idx_of_batch_for_y)\n                        v_cache[i] = torch.index_select(v_cache[i], dim=0, index=reserved_idx_of_batch_for_y)\n\n            if (early_stop_num != -1 and (y.shape[1] - prefix_len) > early_stop_num) or idx == 1499:\n                print(\"use early stop num:\", early_stop_num)\n                stop = True\n                for i, batch_index in enumerate(batch_idx_map):\n                    batch_index = batch_idx_map[i]\n                    idx_list[batch_index] = idx\n                    y_list[batch_index] = y[i, :-1]\n\n            if None not in idx_list:\n                stop = True\n\n            if stop:\n                if y.shape[1] == 0:\n                    y = torch.concat([y, torch.zeros_like(samples)], dim=1)\n                    print(\"bad zero prediction\")\n                print(f\"T2S Decoding EOS [{prefix_len} -> {y.shape[1]}]\")\n                break\n\n            ####################### update next step ###################################\n            y_emb = self.ar_audio_embedding(y[:, -1:])\n            xy_pos = y_emb * self.ar_audio_position.x_scale + self.ar_audio_position.alpha * self.ar_audio_position.pe[\n                :, y_len + idx\n            ].to(dtype=y_emb.dtype, device=y_emb.device)\n\n        if None in idx_list:\n            for i in range(x.shape[0]):\n                if idx_list[i] is None:\n                    idx_list[i] = 1500 - 1  ###如果没有生成到EOS，就用最大长度代替\n\n        if ref_free:\n            return y_list, [0] * x.shape[0]\n        # print(idx_list)\n        return y_list, idx_list\n\n    def infer_panel_naive_batched(\n        self,\n        x: List[torch.LongTensor],  #####全部文本token\n        x_lens: torch.LongTensor,\n        prompts: torch.LongTensor,  ####参考音频token\n        bert_feature: List[torch.LongTensor],\n        top_k: int = -100,\n        top_p: int = 100,\n        early_stop_num: int = -1,\n        temperature: float = 1.0,\n        repetition_penalty: float = 1.35,\n        **kwargs,\n    ):\n        y_list = []\n        idx_list = []\n        for i in range(len(x)):\n            y, idx = self.infer_panel_naive(\n                x[i].unsqueeze(0),\n                x_lens[i],\n                prompts[i].unsqueeze(0) if prompts is not None else None,\n                bert_feature[i].unsqueeze(0),\n                top_k,\n                top_p,\n                early_stop_num,\n                temperature,\n                repetition_penalty,\n                **kwargs,\n            )\n            y_list.append(y[0])\n            idx_list.append(idx)\n\n        return y_list, idx_list\n\n    def infer_panel_naive(\n        self,\n        x: torch.LongTensor,  #####全部文本token\n        x_lens: torch.LongTensor,\n        prompts: torch.LongTensor,  ####参考音频token\n        bert_feature: torch.LongTensor,\n        top_k: int = -100,\n        top_p: int = 100,\n        early_stop_num: int = -1,\n        temperature: float = 1.0,\n        repetition_penalty: float = 1.35,\n        **kwargs,\n    ):\n        x = self.ar_text_embedding(x)\n        x = x + self.bert_proj(bert_feature.transpose(1, 2))\n        x = self.ar_text_position(x)\n\n        # AR Decoder\n        y = prompts\n\n        x_len = x.shape[1]\n        x_attn_mask = torch.zeros((x_len, x_len), dtype=torch.bool)\n        stop = False\n        # print(1111111,self.num_layers)\n\n        k_cache = None\n        v_cache = None\n        ###################  first step ##########################\n        if y is not None:\n            y_emb = self.ar_audio_embedding(y)\n            y_len = y_emb.shape[1]\n            prefix_len = y.shape[1]\n            y_pos = self.ar_audio_position(y_emb)\n            xy_pos = torch.concat([x, y_pos], dim=1)\n            ref_free = False\n        else:\n            y_emb = None\n            y_len = 0\n            prefix_len = 0\n            y_pos = None\n            xy_pos = x\n            y = torch.zeros(x.shape[0], 0, dtype=torch.int, device=x.device)\n            ref_free = True\n\n        bsz = x.shape[0]\n        src_len = x_len + y_len\n        x_attn_mask_pad = F.pad(\n            x_attn_mask,\n            (0, y_len),  ###xx的纯0扩展到xx纯0+xy纯1，(x,x+y)\n            value=True,\n        )\n        y_attn_mask = F.pad(  ###yy的右上1扩展到左边xy的0,(y,x+y)\n            torch.triu(torch.ones(y_len, y_len, dtype=torch.bool), diagonal=1),\n            (x_len, 0),\n            value=False,\n        )\n        xy_attn_mask = (\n            torch.concat([x_attn_mask_pad, y_attn_mask], dim=0)\n            .unsqueeze(0)\n            .expand(bsz * self.num_head, -1, -1)\n            .view(bsz, self.num_head, src_len, src_len)\n            .to(device=x.device, dtype=torch.bool)\n        )\n\n        for idx in tqdm(range(1500)):\n            if xy_attn_mask is not None:\n                xy_dec, k_cache, v_cache = self.t2s_transformer.process_prompt(xy_pos, xy_attn_mask, None)\n            else:\n                xy_dec, k_cache, v_cache = self.t2s_transformer.decode_next_token(xy_pos, k_cache, v_cache)\n\n            logits = self.ar_predict_layer(xy_dec[:, -1])\n\n            if idx == 0:\n                xy_attn_mask = None\n            if idx < 11:  ###至少预测出10个token不然不给停止（0.4s）\n                logits = logits[:, :-1]\n\n            samples = sample(\n                logits, y, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty, temperature=temperature\n            )[0]\n\n            y = torch.concat([y, samples], dim=1)\n\n            if early_stop_num != -1 and (y.shape[1] - prefix_len) > early_stop_num:\n                print(\"use early stop num:\", early_stop_num)\n                stop = True\n\n            if torch.argmax(logits, dim=-1)[0] == self.EOS or samples[0, 0] == self.EOS:\n                stop = True\n            if stop:\n                if y.shape[1] == 0:\n                    y = torch.concat([y, torch.zeros_like(samples)], dim=1)\n                    print(\"bad zero prediction\")\n                print(f\"T2S Decoding EOS [{prefix_len} -> {y.shape[1]}]\")\n                break\n\n            ####################### update next step ###################################\n            y_emb = self.ar_audio_embedding(y[:, -1:])\n            xy_pos = y_emb * self.ar_audio_position.x_scale + self.ar_audio_position.alpha * self.ar_audio_position.pe[\n                :, y_len + idx\n            ].to(dtype=y_emb.dtype, device=y_emb.device)\n\n        if ref_free:\n            return y[:, :-1], 0\n        return y[:, :-1], idx\n\n    def infer_panel(\n        self,\n        x: torch.LongTensor,  #####全部文本token\n        x_lens: torch.LongTensor,\n        prompts: torch.LongTensor,  ####参考音频token\n        bert_feature: torch.LongTensor,\n        top_k: int = -100,\n        top_p: int = 100,\n        early_stop_num: int = -1,\n        temperature: float = 1.0,\n        repetition_penalty: float = 1.35,\n        **kwargs,\n    ):\n        return self.infer_panel_naive(\n            x, x_lens, prompts, bert_feature, top_k, top_p, early_stop_num, temperature, repetition_penalty, **kwargs\n        )",
    "source": "github_repo:RVC-Boss/GPT-SoVITS",
    "file": "GPT_SoVITS/AR/models/t2s_model.py",
    "license": "MIT",
    "language": "python"
}