{
    "code": "class ToneColorConverter(OpenVoiceBaseClass):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        if kwargs.get('enable_watermark', True):\n            import wavmark\n            self.watermark_model = wavmark.load_model().to(self.device)\n        else:\n            self.watermark_model = None\n        self.version = getattr(self.hps, '_version_', \"v1\")\n\n\n\n    def extract_se(self, ref_wav_list, se_save_path=None):\n        if isinstance(ref_wav_list, str):\n            ref_wav_list = [ref_wav_list]\n        \n        device = self.device\n        hps = self.hps\n        gs = []\n        \n        for fname in ref_wav_list:\n            audio_ref, sr = librosa.load(fname, sr=hps.data.sampling_rate)\n            y = torch.FloatTensor(audio_ref)\n            y = y.to(device)\n            y = y.unsqueeze(0)\n            y = spectrogram_torch(y, hps.data.filter_length,\n                                        hps.data.sampling_rate, hps.data.hop_length, hps.data.win_length,\n                                        center=False).to(device)\n            with torch.no_grad():\n                g = self.model.ref_enc(y.transpose(1, 2)).unsqueeze(-1)\n                gs.append(g.detach())\n        gs = torch.stack(gs).mean(0)\n\n        if se_save_path is not None:\n            os.makedirs(os.path.dirname(se_save_path), exist_ok=True)\n            torch.save(gs.cpu(), se_save_path)\n\n        return gs\n\n    def convert(self, audio_src_path, src_se, tgt_se, output_path=None, tau=0.3, message=\"default\"):\n        hps = self.hps\n        # load audio\n        audio, sample_rate = librosa.load(audio_src_path, sr=hps.data.sampling_rate)\n        audio = torch.tensor(audio).float()\n        \n        with torch.no_grad():\n            y = torch.FloatTensor(audio).to(self.device)\n            y = y.unsqueeze(0)\n            spec = spectrogram_torch(y, hps.data.filter_length,\n                                    hps.data.sampling_rate, hps.data.hop_length, hps.data.win_length,\n                                    center=False).to(self.device)\n            spec_lengths = torch.LongTensor([spec.size(-1)]).to(self.device)\n            audio = self.model.voice_conversion(spec, spec_lengths, sid_src=src_se, sid_tgt=tgt_se, tau=tau)[0][\n                        0, 0].data.cpu().float().numpy()\n            audio = self.add_watermark(audio, message)\n            if output_path is None:\n                return audio\n            else:\n                soundfile.write(output_path, audio, hps.data.sampling_rate)\n    \n    def add_watermark(self, audio, message):\n        if self.watermark_model is None:\n            return audio\n        device = self.device\n        bits = utils.string_to_bits(message).reshape(-1)\n        n_repeat = len(bits) // 32\n\n        K = 16000\n        coeff = 2\n        for n in range(n_repeat):\n            trunck = audio[(coeff * n) * K: (coeff * n + 1) * K]\n            if len(trunck) != K:\n                print('Audio too short, fail to add watermark')\n                break\n            message_npy = bits[n * 32: (n + 1) * 32]\n            \n            with torch.no_grad():\n                signal = torch.FloatTensor(trunck).to(device)[None]\n                message_tensor = torch.FloatTensor(message_npy).to(device)[None]\n                signal_wmd_tensor = self.watermark_model.encode(signal, message_tensor)\n                signal_wmd_npy = signal_wmd_tensor.detach().cpu().squeeze()\n            audio[(coeff * n) * K: (coeff * n + 1) * K] = signal_wmd_npy\n        return audio\n\n    def detect_watermark(self, audio, n_repeat):\n        bits = []\n        K = 16000\n        coeff = 2\n        for n in range(n_repeat):\n            trunck = audio[(coeff * n) * K: (coeff * n + 1) * K]\n            if len(trunck) != K:\n                print('Audio too short, fail to detect watermark')\n                return 'Fail'\n            with torch.no_grad():\n                signal = torch.FloatTensor(trunck).to(self.device).unsqueeze(0)\n                message_decoded_npy = (self.watermark_model.decode(signal) >= 0.5).int().detach().cpu().numpy().squeeze()\n            bits.append(message_decoded_npy)\n        bits = np.stack(bits).reshape(-1, 8)\n        message = utils.bits_to_string(bits)\n        return message",
    "source": "github_repo:myshell-ai/OpenVoice",
    "file": "openvoice/api.py",
    "license": "MIT",
    "language": "python"
}