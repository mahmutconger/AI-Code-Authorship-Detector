{
    "code": "class Block(nn.Module):\n    \"\"\"\n    Transformer block combining attention and feed-forward layers.\n\n    Attributes:\n        attn (nn.Module): Attention layer (MLA).\n        ffn (nn.Module): Feed-forward network (MLP or MoE).\n        attn_norm (nn.Module): Layer normalization for attention.\n        ffn_norm (nn.Module): Layer normalization for feed-forward network.\n    \"\"\"\n    def __init__(self, layer_id: int, args: ModelArgs):\n        \"\"\"\n        Initializes the Transformer block.\n\n        Args:\n            layer_id (int): Layer index in the transformer.\n            args (ModelArgs): Model arguments containing block parameters.\n        \"\"\"\n        super().__init__()\n        self.attn = MLA(args)\n        self.ffn = MLP(args.dim, args.inter_dim) if layer_id < args.n_dense_layers else MoE(args)\n        self.attn_norm = RMSNorm(args.dim)\n        self.ffn_norm = RMSNorm(args.dim)\n\n    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the Transformer block.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n            start_pos (int): Starting position in the sequence.\n            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n\n        Returns:\n            torch.Tensor: Output tensor after block computation.\n        \"\"\"\n        x = x + self.attn(self.attn_norm(x), start_pos, freqs_cis, mask)\n        x = x + self.ffn(self.ffn_norm(x))\n        return x",
    "source": "github_repo:deepseek-ai/DeepSeek-V3",
    "file": "inference/model.py",
    "license": "MIT",
    "language": "python"
}