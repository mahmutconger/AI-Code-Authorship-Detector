{
    "code": "def cut(self, sentence, cut_all=False, HMM=True, use_paddle=False):\n        \"\"\"\n        The main function that segments an entire sentence that contains\n        Chinese characters into separated words.\n\n        Parameter:\n            - sentence: The str(unicode) to be segmented.\n            - cut_all: Model type. True for full pattern, False for accurate pattern.\n            - HMM: Whether to use the Hidden Markov Model.\n        \"\"\"\n        is_paddle_installed = check_paddle_install['is_paddle_installed']\n        sentence = strdecode(sentence)\n        if use_paddle and is_paddle_installed:\n            # if sentence is null, it will raise core exception in paddle.\n            if sentence is None or len(sentence) == 0:\n                return\n            import jieba.lac_small.predict as predict\n            results = predict.get_sent(sentence)\n            for sent in results:\n                if sent is None:\n                    continue\n                yield sent\n            return\n        re_han = re_han_default\n        re_skip = re_skip_default\n        if cut_all:\n            cut_block = self.__cut_all\n        elif HMM:\n            cut_block = self.__cut_DAG\n        else:\n            cut_block = self.__cut_DAG_NO_HMM\n        blocks = re_han.split(sentence)\n        for blk in blocks:\n            if not blk:\n                continue\n            if re_han.match(blk):\n                for word in cut_block(blk):\n                    yield word\n            else:\n                tmp = re_skip.split(blk)\n                for x in tmp:\n                    if re_skip.match(x):\n                        yield x\n                    elif not cut_all:\n                        for xx in x:\n                            yield xx\n                    else:\n                        yield x",
    "source": "github_repo:fxsjy/jieba",
    "file": "jieba/__init__.py",
    "license": "MIT",
    "language": "python"
}