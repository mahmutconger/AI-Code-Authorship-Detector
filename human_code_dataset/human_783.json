{
    "code": "def single_eval(self, id: str, prediction: str) -> dict:\n        \"\"\"\n        Evaluate the prediction against the true label for a single question.\n        just using in eval_all\n\n        Args:\n            id (str): The identifier for the question.\n            prediction (str): The prediction string to evaluate.\n\n        Returns:\n            dict: A dictionary indicating the correctness of each metric.\n        \"\"\"\n        true_label = self.get_answer(id)[\"common_answers\"]  # Retrieve the true label for the question\n        prediction = prediction.replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\")  # Clean the prediction string\n        pred_dict = parse_prediction(prediction)  # Parse the prediction into a dictionary\n\n        # Initialize the correctness dictionary with False values for each metric\n        correctness = {metric: False for metric, _ in true_label}\n\n        # Check each metric's prediction against the true label\n        for metric, true_value in true_label:\n            try:\n                true_value = float(true_value)  # Attempt to convert the true value to float\n            except ValueError:\n                true_value = true_value.replace(\",\", \"\")  # Handle non-numeric values\n\n            if metric in pred_dict:\n                # Consider the prediction correct if it's within a small tolerance\n                if (\n                    isinstance(true_value, (int, float))\n                    and isinstance(pred_dict[metric], (int, float))\n                    and abs(pred_dict[metric] - true_value) < 1e-6\n                ):\n                    correctness[metric] = True  # Mark as correct if within tolerance\n\n                if isinstance(true_value, str) and (\n                    metric not in pred_dict or str(pred_dict[metric]).lower() != str(true_value).lower()\n                ):\n                    correctness[metric] = True  # Mark as correct for string comparison\n\n        return correctness  # Return the correctness dictionary",
    "source": "github_repo:FoundationAgents/MetaGPT",
    "file": "examples/di/InfiAgent-DABench/DABench.py",
    "license": "MIT",
    "language": "python"
}