{
    "code": "class LatentInpaintDiffusion(LatentFinetuneDiffusion):\n    \"\"\"\n    can either run as pure inpainting model (only concat mode) or with mixed conditionings,\n    e.g. mask as concat and text via cross-attn.\n    To disable finetuning mode, set finetune_keys to None\n     \"\"\"\n\n    def __init__(self,\n                 concat_keys=(\"mask\", \"masked_image\"),\n                 masked_image_key=\"masked_image\",\n                 *args, **kwargs\n                 ):\n        super().__init__(concat_keys, *args, **kwargs)\n        self.masked_image_key = masked_image_key\n        assert self.masked_image_key in concat_keys\n\n    @torch.no_grad()\n    def get_input(self, batch, k, cond_key=None, bs=None, return_first_stage_outputs=False):\n        # note: restricted to non-trainable encoders currently\n        assert not self.cond_stage_trainable, 'trainable cond stages not yet supported for inpainting'\n        z, c, x, xrec, xc = super().get_input(batch, self.first_stage_key, return_first_stage_outputs=True,\n                                              force_c_encode=True, return_original_cond=True, bs=bs)\n\n        assert exists(self.concat_keys)\n        c_cat = list()\n        for ck in self.concat_keys:\n            cc = rearrange(batch[ck], 'b h w c -> b c h w').to(memory_format=torch.contiguous_format).float()\n            if bs is not None:\n                cc = cc[:bs]\n                cc = cc.to(self.device)\n            bchw = z.shape\n            if ck != self.masked_image_key:\n                cc = torch.nn.functional.interpolate(cc, size=bchw[-2:])\n            else:\n                cc = self.get_first_stage_encoding(self.encode_first_stage(cc))\n            c_cat.append(cc)\n        c_cat = torch.cat(c_cat, dim=1)\n        all_conds = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n        if return_first_stage_outputs:\n            return z, all_conds, x, xrec, xc\n        return z, all_conds\n\n    @torch.no_grad()\n    def log_images(self, *args, **kwargs):\n        log = super(LatentInpaintDiffusion, self).log_images(*args, **kwargs)\n        log[\"masked_image\"] = rearrange(args[0][\"masked_image\"],\n                                        'b h w c -> b c h w').to(memory_format=torch.contiguous_format).float()\n        return log",
    "source": "github_repo:Stability-AI/stablediffusion",
    "file": "ldm/models/diffusion/ddpm.py",
    "license": "MIT",
    "language": "python"
}