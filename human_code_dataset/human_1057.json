{
    "code": "class Text2SemanticLightningModule(LightningModule):\n    def __init__(self, config, output_dir, is_train=True):\n        super().__init__()\n        self.config = config\n        self.top_k = 3\n        self.model = Text2SemanticDecoder(config=config, top_k=self.top_k)\n        pretrained_s1 = config.get(\"pretrained_s1\")\n        if pretrained_s1 and is_train:\n            # print(self.load_state_dict(torch.load(pretrained_s1,map_location=\"cpu\")[\"state_dict\"]))\n            print(\n                self.load_state_dict(\n                    torch.load(\n                        pretrained_s1,\n                        map_location=\"cpu\",\n                    )[\"weight\"],\n                ),\n            )\n        if is_train:\n            self.automatic_optimization = False\n            self.save_hyperparameters()\n            self.eval_dir = output_dir / \"eval\"\n            self.eval_dir.mkdir(parents=True, exist_ok=True)\n\n    def training_step(self, batch: Dict, batch_idx: int):\n        opt = self.optimizers()\n        scheduler = self.lr_schedulers()\n        loss, acc = self.model.forward(\n            batch[\"phoneme_ids\"],\n            batch[\"phoneme_ids_len\"],\n            batch[\"semantic_ids\"],\n            batch[\"semantic_ids_len\"],\n            batch[\"bert_feature\"],\n        )\n        self.manual_backward(loss)\n        if batch_idx > 0 and batch_idx % 4 == 0:\n            opt.step()\n            opt.zero_grad()\n            scheduler.step()\n\n        self.log(\n            \"total_loss\",\n            loss,\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"lr\",\n            scheduler.get_last_lr()[0],\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n        self.log(\n            f\"top_{self.top_k}_acc\",\n            acc,\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n\n    def validation_step(self, batch: Dict, batch_idx: int):\n        return\n\n    def configure_optimizers(self):\n        model_parameters = self.model.parameters()\n        parameters_names = []\n        parameters_names.append([name_param_pair[0] for name_param_pair in self.model.named_parameters()])\n        lm_opt = ScaledAdam(\n            model_parameters,\n            lr=0.01,\n            betas=(0.9, 0.95),\n            clipping_scale=2.0,\n            parameters_names=parameters_names,\n            show_dominant_parameters=False,\n            clipping_update_period=1000,\n        )\n\n        return {\n            \"optimizer\": lm_opt,\n            \"lr_scheduler\": {\n                \"scheduler\": WarmupCosineLRSchedule(\n                    lm_opt,\n                    init_lr=self.config[\"optimizer\"][\"lr_init\"],\n                    peak_lr=self.config[\"optimizer\"][\"lr\"],\n                    end_lr=self.config[\"optimizer\"][\"lr_end\"],\n                    warmup_steps=self.config[\"optimizer\"][\"warmup_steps\"],\n                    total_steps=self.config[\"optimizer\"][\"decay_steps\"],\n                )\n            },\n        }",
    "source": "github_repo:RVC-Boss/GPT-SoVITS",
    "file": "GPT_SoVITS/AR/models/t2s_lightning_module_onnx.py",
    "license": "MIT",
    "language": "python"
}