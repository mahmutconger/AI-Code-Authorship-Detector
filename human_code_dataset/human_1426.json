{
    "code": "class ImageEmbeddingConditionedLatentDiffusion(LatentDiffusion):\n    def __init__(self, embedder_config, embedding_key=\"jpg\", embedding_dropout=0.5,\n                 freeze_embedder=True, noise_aug_config=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.embed_key = embedding_key\n        self.embedding_dropout = embedding_dropout\n        self._init_embedder(embedder_config, freeze_embedder)\n        self._init_noise_aug(noise_aug_config)\n\n    def _init_embedder(self, config, freeze=True):\n        embedder = instantiate_from_config(config)\n        if freeze:\n            self.embedder = embedder.eval()\n            self.embedder.train = disabled_train\n            for param in self.embedder.parameters():\n                param.requires_grad = False\n\n    def _init_noise_aug(self, config):\n        if config is not None:\n            # use the KARLO schedule for noise augmentation on CLIP image embeddings\n            noise_augmentor = instantiate_from_config(config)\n            assert isinstance(noise_augmentor, nn.Module)\n            noise_augmentor = noise_augmentor.eval()\n            noise_augmentor.train = disabled_train\n            self.noise_augmentor = noise_augmentor\n        else:\n            self.noise_augmentor = None\n\n    def get_input(self, batch, k, cond_key=None, bs=None, **kwargs):\n        outputs = LatentDiffusion.get_input(self, batch, k, bs=bs, **kwargs)\n        z, c = outputs[0], outputs[1]\n        img = batch[self.embed_key][:bs]\n        img = rearrange(img, 'b h w c -> b c h w')\n        c_adm = self.embedder(img)\n        if self.noise_augmentor is not None:\n            c_adm, noise_level_emb = self.noise_augmentor(c_adm)\n            # assume this gives embeddings of noise levels\n            c_adm = torch.cat((c_adm, noise_level_emb), 1)\n        if self.training:\n            c_adm = torch.bernoulli((1. - self.embedding_dropout) * torch.ones(c_adm.shape[0],\n                                                                               device=c_adm.device)[:, None]) * c_adm\n        all_conds = {\"c_crossattn\": [c], \"c_adm\": c_adm}\n        noutputs = [z, all_conds]\n        noutputs.extend(outputs[2:])\n        return noutputs\n\n    @torch.no_grad()\n    def log_images(self, batch, N=8, n_row=4, **kwargs):\n        log = dict()\n        z, c, x, xrec, xc = self.get_input(batch, self.first_stage_key, bs=N, return_first_stage_outputs=True,\n                                           return_original_cond=True)\n        log[\"inputs\"] = x\n        log[\"reconstruction\"] = xrec\n        assert self.model.conditioning_key is not None\n        assert self.cond_stage_key in [\"caption\", \"txt\"]\n        xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[self.cond_stage_key], size=x.shape[2] // 25)\n        log[\"conditioning\"] = xc\n        uc = self.get_unconditional_conditioning(N, kwargs.get('unconditional_guidance_label', ''))\n        unconditional_guidance_scale = kwargs.get('unconditional_guidance_scale', 5.)\n\n        uc_ = {\"c_crossattn\": [uc], \"c_adm\": c[\"c_adm\"]}\n        ema_scope = self.ema_scope if kwargs.get('use_ema_scope', True) else nullcontext\n        with ema_scope(f\"Sampling\"):\n            samples_cfg, _ = self.sample_log(cond=c, batch_size=N, ddim=True,\n                                             ddim_steps=kwargs.get('ddim_steps', 50), eta=kwargs.get('ddim_eta', 0.),\n                                             unconditional_guidance_scale=unconditional_guidance_scale,\n                                             unconditional_conditioning=uc_, )\n            x_samples_cfg = self.decode_first_stage(samples_cfg)\n            log[f\"samplescfg_scale_{unconditional_guidance_scale:.2f}\"] = x_samples_cfg\n        return log",
    "source": "github_repo:Stability-AI/stablediffusion",
    "file": "ldm/models/diffusion/ddpm.py",
    "license": "MIT",
    "language": "python"
}