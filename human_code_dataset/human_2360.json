{
    "code": "class HyperparamOptManager:\n    \"\"\"Manages hyperparameter optimisation using random search for a single GPU.\n\n    Attributes:\n      param_ranges: Discrete hyperparameter range for random search.\n      results: Dataframe of validation results.\n      fixed_params: Fixed model parameters per experiment.\n      saved_params: Dataframe of parameters trained.\n      best_score: Minimum validation loss observed thus far.\n      optimal_name: Key to best configuration.\n      hyperparam_folder: Where to save optimisation outputs.\n    \"\"\"\n\n    def __init__(self, param_ranges, fixed_params, model_folder, override_w_fixed_params=True):\n        \"\"\"Instantiates model.\n\n        Args:\n          param_ranges: Discrete hyperparameter range for random search.\n          fixed_params: Fixed model parameters per experiment.\n          model_folder: Folder to store optimisation artifacts.\n          override_w_fixed_params: Whether to override serialsed fixed model\n            parameters with new supplied values.\n        \"\"\"\n\n        self.param_ranges = param_ranges\n\n        self._max_tries = 1000\n        self.results = pd.DataFrame()\n        self.fixed_params = fixed_params\n        self.saved_params = pd.DataFrame()\n\n        self.best_score = np.Inf\n        self.optimal_name = \"\"\n\n        # Setup\n        # Create folder for saving if its not there\n        self.hyperparam_folder = model_folder\n        utils.create_folder_if_not_exist(self.hyperparam_folder)\n\n        self._override_w_fixed_params = override_w_fixed_params\n\n    def load_results(self):\n        \"\"\"Loads results from previous hyperparameter optimisation.\n\n        Returns:\n          A boolean indicating if previous results can be loaded.\n        \"\"\"\n        print(\"Loading results from\", self.hyperparam_folder)\n\n        results_file = os.path.join(self.hyperparam_folder, \"results.csv\")\n        params_file = os.path.join(self.hyperparam_folder, \"params.csv\")\n\n        if os.path.exists(results_file) and os.path.exists(params_file):\n            self.results = pd.read_csv(results_file, index_col=0)\n            self.saved_params = pd.read_csv(params_file, index_col=0)\n\n            if not self.results.empty:\n                self.results.at[\"loss\"] = self.results.loc[\"loss\"].apply(float)\n                self.best_score = self.results.loc[\"loss\"].min()\n\n                is_optimal = self.results.loc[\"loss\"] == self.best_score\n                self.optimal_name = self.results.T[is_optimal].index[0]\n\n                return True\n\n        return False\n\n    def _get_params_from_name(self, name):\n        \"\"\"Returns previously saved parameters given a key.\"\"\"\n        params = self.saved_params\n\n        selected_params = dict(params[name])\n\n        if self._override_w_fixed_params:\n            for k in self.fixed_params:\n                selected_params[k] = self.fixed_params[k]\n\n        return selected_params\n\n    def get_best_params(self):\n        \"\"\"Returns the optimal hyperparameters thus far.\"\"\"\n\n        optimal_name = self.optimal_name\n\n        return self._get_params_from_name(optimal_name)\n\n    def clear(self):\n        \"\"\"Clears all previous results and saved parameters.\"\"\"\n        shutil.rmtree(self.hyperparam_folder)\n        os.makedirs(self.hyperparam_folder)\n        self.results = pd.DataFrame()\n        self.saved_params = pd.DataFrame()\n\n    def _check_params(self, params):\n        \"\"\"Checks that parameter map is properly defined.\"\"\"\n\n        valid_fields = list(self.param_ranges.keys()) + list(self.fixed_params.keys())\n        invalid_fields = [k for k in params if k not in valid_fields]\n        missing_fields = [k for k in valid_fields if k not in params]\n\n        if invalid_fields:\n            raise ValueError(\"Invalid Fields Found {} - Valid ones are {}\".format(invalid_fields, valid_fields))\n        if missing_fields:\n            raise ValueError(\"Missing Fields Found {} - Valid ones are {}\".format(missing_fields, valid_fields))\n\n    def _get_name(self, params):\n        \"\"\"Returns a unique key for the supplied set of params.\"\"\"\n\n        self._check_params(params)\n\n        fields = list(params.keys())\n        fields.sort()\n\n        return \"_\".join([str(params[k]) for k in fields])\n\n    def get_next_parameters(self, ranges_to_skip=None):\n        \"\"\"Returns the next set of parameters to optimise.\n\n        Args:\n          ranges_to_skip: Explicitly defines a set of keys to skip.\n        \"\"\"\n        if ranges_to_skip is None:\n            ranges_to_skip = set(self.results.index)\n\n        if not isinstance(self.param_ranges, dict):\n            raise ValueError(\"Only works for random search!\")\n\n        param_range_keys = list(self.param_ranges.keys())\n        param_range_keys.sort()\n\n        def _get_next():\n            \"\"\"Returns next hyperparameter set per try.\"\"\"\n\n            parameters = {k: np.random.choice(self.param_ranges[k]) for k in param_range_keys}\n\n            # Adds fixed params\n            for k in self.fixed_params:\n                parameters[k] = self.fixed_params[k]\n\n            return parameters\n\n        for _ in range(self._max_tries):\n            parameters = _get_next()\n            name = self._get_name(parameters)\n\n            if name not in ranges_to_skip:\n                return parameters\n\n        raise ValueError(\"Exceeded max number of hyperparameter searches!!\")\n\n    def update_score(self, parameters, loss, model, info=\"\"):\n        \"\"\"Updates the results from last optimisation run.\n\n        Args:\n          parameters: Hyperparameters used in optimisation.\n          loss: Validation loss obtained.\n          model: Model to serialised if required.\n          info: Any ancillary information to tag on to results.\n\n        Returns:\n          Boolean flag indicating if the model is the best seen so far.\n        \"\"\"\n\n        if np.isnan(loss):\n            loss = np.Inf\n\n        if not os.path.isdir(self.hyperparam_folder):\n            os.makedirs(self.hyperparam_folder)\n\n        name = self._get_name(parameters)\n\n        is_optimal = self.results.empty or loss < self.best_score\n\n        # save the first model\n        if is_optimal:\n            # Try saving first, before updating info\n            if model is not None:\n                print(\"Optimal model found, updating\")\n                model.save(self.hyperparam_folder)\n            self.best_score = loss\n            self.optimal_name = name\n\n        self.results[name] = pd.Series({\"loss\": loss, \"info\": info})\n        self.saved_params[name] = pd.Series(parameters)\n\n        self.results.to_csv(os.path.join(self.hyperparam_folder, \"results.csv\"))\n        self.saved_params.to_csv(os.path.join(self.hyperparam_folder, \"params.csv\"))\n\n        return is_optimal",
    "source": "github_repo:microsoft/qlib",
    "file": "examples/benchmarks/TFT/libs/hyperparam_opt.py",
    "license": "MIT",
    "language": "python"
}