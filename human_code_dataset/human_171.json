{
    "code": "class Chat__ModuleName__(BaseChatModel):\n    # TODO: Replace all TODOs in docstring. See example docstring:\n    # https://github.com/langchain-ai/langchain/blob/7ff05357bac6eaedf5058a2af88f23a1817d40fe/libs/partners/openai/langchain_openai/chat_models/base.py#L1120\n    \"\"\"__ModuleName__ chat model integration.\n\n    The default implementation echoes the first `parrot_buffer_length` characters of\n    the input.\n\n    # TODO: Replace with relevant packages, env vars.\n    Setup:\n        Install `__package_name__` and set environment variable\n        `__MODULE_NAME___API_KEY`.\n\n        ```bash\n        pip install -U __package_name__\n        export __MODULE_NAME___API_KEY=\"your-api-key\"\n        ```\n\n    # TODO: Populate with relevant params.\n    Key init args — completion params:\n        model:\n            Name of __ModuleName__ model to use.\n        temperature:\n            Sampling temperature.\n        max_tokens:\n            Max number of tokens to generate.\n\n    # TODO: Populate with relevant params.\n    Key init args — client params:\n        timeout:\n            Timeout for requests.\n        max_retries:\n            Max number of retries.\n        api_key:\n            __ModuleName__ API key. If not passed in will be read from env var\n            __MODULE_NAME___API_KEY.\n\n    See full list of supported init args and their descriptions in the params section.\n\n    # TODO: Replace with relevant init params.\n    Instantiate:\n        ```python\n        from __module_name__ import Chat__ModuleName__\n\n        model = Chat__ModuleName__(\n            model=\"...\",\n            temperature=0,\n            max_tokens=None,\n            timeout=None,\n            max_retries=2,\n            # api_key=\"...\",\n            # other params...\n        )\n        ```\n\n    Invoke:\n        ```python\n        messages = [\n            (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n            (\"human\", \"I love programming.\"),\n        ]\n        model.invoke(messages)\n        ```\n\n        ```python\n        # TODO: Example output.\n        ```\n\n    # TODO: Delete if token-level streaming isn't supported.\n    Stream:\n        ```python\n        for chunk in model.stream(messages):\n            print(chunk.text, end=\"\")\n        ```\n\n        ```python\n        # TODO: Example output.\n        ```\n\n        ```python\n        stream = model.stream(messages)\n        full = next(stream)\n        for chunk in stream:\n            full += chunk\n        full\n        ```\n\n        ```python\n        # TODO: Example output.\n        ```\n\n    # TODO: Delete if native async isn't supported.\n    Async:\n        ```python\n        await model.ainvoke(messages)\n\n        # stream:\n        # async for chunk in (await model.astream(messages))\n\n        # batch:\n        # await model.abatch([messages])\n        ```\n\n        ```python\n        # TODO: Example output.\n        ```\n    # TODO: Delete if .bind_tools() isn't supported.\n    Tool calling:\n        ```python\n        from pydantic import BaseModel, Field\n\n        class GetWeather(BaseModel):\n            '''Get the current weather in a given location'''\n\n            location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n        class GetPopulation(BaseModel):\n            '''Get the current population in a given location'''\n\n            location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n\n        model_with_tools = model.bind_tools([GetWeather, GetPopulation])\n        ai_msg = model_with_tools.invoke(\"Which city is hotter today and which is bigger: LA or NY?\")\n        ai_msg.tool_calls\n        ```\n\n        ```python\n        # TODO: Example output.\n        ```\n\n        See `Chat__ModuleName__.bind_tools()` method for more.\n\n    # TODO: Delete if .with_structured_output() isn't supported.\n    Structured output:\n        ```python\n        from typing import Optional\n\n        from pydantic import BaseModel, Field\n\n        class Joke(BaseModel):\n            '''Joke to tell user.'''\n\n            setup: str = Field(description=\"The setup of the joke\")\n            punchline: str = Field(description=\"The punchline to the joke\")\n            rating: int | None = Field(description=\"How funny the joke is, from 1 to 10\")\n\n        structured_model = model.with_structured_output(Joke)\n        structured_model.invoke(\"Tell me a joke about cats\")\n        ```\n\n        ```python\n        # TODO: Example output.\n        ```\n\n        See `Chat__ModuleName__.with_structured_output()` for more.\n\n    # TODO: Delete if JSON mode response format isn't supported.\n    JSON mode:\n        ```python\n        # TODO: Replace with appropriate bind arg.\n        json_model = model.bind(response_format={\"type\": \"json_object\"})\n        ai_msg = json_model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")\n        ai_msg.content\n        ```\n\n        ```python\n        # TODO: Example output.\n        ```\n\n    # TODO: Delete if image inputs aren't supported.\n    Image input:\n        ```python\n        import base64\n        import httpx\n        from langchain_core.messages import HumanMessage\n\n        image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n        image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n        # TODO: Replace with appropriate message content format.\n        message = HumanMessage(\n            content=[\n                {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n                },\n            ],\n        )\n        ai_msg = model.invoke([message])\n        ai_msg.content\n        ```\n\n        ```python\n        # TODO: Example output.\n        ```\n\n    # TODO: Delete if audio inputs aren't supported.\n    Audio input:\n        ```python\n        # TODO: Example input\n        ```\n\n        ```python\n        # TODO: Example output\n        ```\n\n    # TODO: Delete if video inputs aren't supported.\n    Video input:\n        ```python\n        # TODO: Example input\n        ```\n\n        ```python\n        # TODO: Example output\n        ```\n\n    # TODO: Delete if token usage metadata isn't supported.\n    Token usage:\n        ```python\n        ai_msg = model.invoke(messages)\n        ai_msg.usage_metadata\n        ```\n\n        ```python\n        {'input_tokens': 28, 'output_tokens': 5, 'total_tokens': 33}\n        ```\n\n    # TODO: Delete if logprobs aren't supported.\n    Logprobs:\n        ```python\n        # TODO: Replace with appropriate bind arg.\n        logprobs_model = model.bind(logprobs=True)\n        ai_msg = logprobs_model.invoke(messages)\n        ai_msg.response_metadata[\"logprobs\"]\n        ```\n\n        ```python\n        # TODO: Example output.\n        ```\n    Response metadata\n        ```python\n        ai_msg = model.invoke(messages)\n        ai_msg.response_metadata\n        ```\n\n        ```python\n        # TODO: Example output.\n\n        ```\n    \"\"\"  # noqa: E501\n\n    model_name: str = Field(alias=\"model\")\n    \"\"\"The name of the model\"\"\"\n    parrot_buffer_length: int\n    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n    temperature: float | None = None\n    max_tokens: int | None = None\n    timeout: int | None = None\n    stop: list[str] | None = None\n    max_retries: int = 2\n\n    @property\n    def _llm_type(self) -> str:\n        \"\"\"Return type of chat model.\"\"\"\n        return \"chat-__package_name_short__\"\n\n    @property\n    def _identifying_params(self) -> Dict[str, Any]:\n        \"\"\"Return a dictionary of identifying parameters.\n\n        This information is used by the LangChain callback system, which\n        is used for tracing purposes make it possible to monitor LLMs.\n        \"\"\"\n        return {\n            # The model name allows users to specify custom token counting\n            # rules in LLM monitoring applications (e.g., in LangSmith users\n            # can provide per token pricing for their model and monitor\n            # costs for the given LLM.)\n            \"model_name\": self.model_name,\n        }\n\n    def _generate(\n        self,\n        messages: List[BaseMessage],\n        stop: list[str] | None = None,\n        run_manager: CallbackManagerForLLMRun | None = None,\n        **kwargs: Any,\n    ) -> ChatResult:\n        \"\"\"Override the _generate method to implement the chat model logic.\n\n        This can be a call to an API, a call to a local model, or any other\n        implementation that generates a response to the input prompt.\n\n        Args:\n            messages: the prompt composed of a list of messages.\n            stop: a list of strings on which the model should stop generating.\n                If generation stops due to a stop token, the stop token itself\n                SHOULD BE INCLUDED as part of the output. This is not enforced\n                across models right now, but it's a good practice to follow since\n                it makes it much easier to parse the output of the model\n                downstream and understand why generation stopped.\n            run_manager: A run manager with callbacks for the LLM.\n        \"\"\"\n        # Replace this with actual logic to generate a response from a list\n        # of messages.\n        last_message = messages[-1]\n        tokens = last_message.content[: self.parrot_buffer_length]\n        ct_input_tokens = sum(len(message.content) for message in messages)\n        ct_output_tokens = len(tokens)\n        message = AIMessage(\n            content=tokens,\n            additional_kwargs={},  # Used to add additional payload to the message\n            response_metadata={  # Use for response metadata\n                \"time_in_seconds\": 3,\n                \"model_name\": self.model_name,\n            },\n            usage_metadata={\n                \"input_tokens\": ct_input_tokens,\n                \"output_tokens\": ct_output_tokens,\n                \"total_tokens\": ct_input_tokens + ct_output_tokens,\n            },\n        )\n        ##\n\n        generation = ChatGeneration(message=message)\n        return ChatResult(generations=[generation])\n\n    def _stream(\n        self,\n        messages: List[BaseMessage],\n        stop: list[str] | None = None,\n        run_manager: CallbackManagerForLLMRun | None = None,\n        **kwargs: Any,\n    ) -> Iterator[ChatGenerationChunk]:\n        \"\"\"Stream the output of the model.\n\n        This method should be implemented if the model can generate output\n        in a streaming fashion. If the model does not support streaming,\n        do not implement it. In that case streaming requests will be automatically\n        handled by the _generate method.\n\n        Args:\n            messages: the prompt composed of a list of messages.\n            stop: a list of strings on which the model should stop generating.\n                If generation stops due to a stop token, the stop token itself\n                SHOULD BE INCLUDED as part of the output. This is not enforced\n                across models right now, but it's a good practice to follow since\n                it makes it much easier to parse the output of the model\n                downstream and understand why generation stopped.\n            run_manager: A run manager with callbacks for the LLM.\n        \"\"\"\n        last_message = messages[-1]\n        tokens = str(last_message.content[: self.parrot_buffer_length])\n        ct_input_tokens = sum(len(message.content) for message in messages)\n\n        for token in tokens:\n            usage_metadata = UsageMetadata(\n                {\n                    \"input_tokens\": ct_input_tokens,\n                    \"output_tokens\": 1,\n                    \"total_tokens\": ct_input_tokens + 1,\n                }\n            )\n            ct_input_tokens = 0\n            chunk = ChatGenerationChunk(\n                message=AIMessageChunk(content=token, usage_metadata=usage_metadata)\n            )\n\n            if run_manager:\n                # This is optional in newer versions of LangChain\n                # The on_llm_new_token will be called automatically\n                run_manager.on_llm_new_token(token, chunk=chunk)\n\n            yield chunk\n\n        # Let's add some other information (e.g., response metadata)\n        chunk = ChatGenerationChunk(\n            message=AIMessageChunk(\n                content=\"\",\n                response_metadata={\"time_in_sec\": 3, \"model_name\": self.model_name},\n            )\n        )\n        if run_manager:\n            # This is optional in newer versions of LangChain\n            # The on_llm_new_token will be called automatically\n            run_manager.on_llm_new_token(token, chunk=chunk)\n        yield chunk",
    "source": "github_repo:langchain-ai/langchain",
    "file": "libs/cli/langchain_cli/integration_template/integration_template/chat_models.py",
    "license": "MIT",
    "language": "python"
}