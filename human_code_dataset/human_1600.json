{
    "code": "def add(self, *args: ContentItem, **kwargs: Unpack[AddDocumentParams]) -> None:\n        \"\"\"Add content to the knowledge base.\n\n        This method handles various input types and converts them to documents\n        for the vector database. It supports the data_type parameter for\n        compatibility with existing tools.\n\n        Args:\n            *args: Content items to add (strings, paths, or document dicts)\n            **kwargs: Additional parameters including data_type, metadata, etc.\n        \"\"\"\n        import os\n\n        from crewai_tools.rag.base_loader import LoaderResult\n        from crewai_tools.rag.data_types import DataType, DataTypes\n        from crewai_tools.rag.source_content import SourceContent\n\n        documents: list[BaseRecord] = []\n        data_type: DataType | None = kwargs.get(\"data_type\")\n        base_metadata: dict[str, Any] = kwargs.get(\"metadata\", {})\n\n        for arg in args:\n            source_ref: str\n            if isinstance(arg, dict):\n                source_ref = str(arg.get(\"source\", arg.get(\"content\", \"\")))\n            else:\n                source_ref = str(arg)\n\n            if not data_type:\n                data_type = DataTypes.from_content(source_ref)\n\n            if data_type == DataType.DIRECTORY:\n                if not os.path.isdir(source_ref):\n                    raise ValueError(f\"Directory does not exist: {source_ref}\")\n\n                # Define binary and non-text file extensions to skip\n                binary_extensions = {\n                    \".pyc\",\n                    \".pyo\",\n                    \".png\",\n                    \".jpg\",\n                    \".jpeg\",\n                    \".gif\",\n                    \".bmp\",\n                    \".ico\",\n                    \".svg\",\n                    \".webp\",\n                    \".pdf\",\n                    \".zip\",\n                    \".tar\",\n                    \".gz\",\n                    \".bz2\",\n                    \".7z\",\n                    \".rar\",\n                    \".exe\",\n                    \".dll\",\n                    \".so\",\n                    \".dylib\",\n                    \".bin\",\n                    \".dat\",\n                    \".db\",\n                    \".sqlite\",\n                    \".class\",\n                    \".jar\",\n                    \".war\",\n                    \".ear\",\n                }\n\n                for root, dirs, files in os.walk(source_ref):\n                    dirs[:] = [d for d in dirs if not d.startswith(\".\")]\n\n                    for filename in files:\n                        if filename.startswith(\".\"):\n                            continue\n\n                        # Skip binary files based on extension\n                        file_ext = os.path.splitext(filename)[1].lower()\n                        if file_ext in binary_extensions:\n                            continue\n\n                        # Skip __pycache__ directories\n                        if \"__pycache__\" in root:\n                            continue\n\n                        file_path: str = os.path.join(root, filename)\n                        try:\n                            file_data_type: DataType = DataTypes.from_content(file_path)\n                            file_loader = file_data_type.get_loader()\n                            file_chunker = file_data_type.get_chunker()\n\n                            file_source = SourceContent(file_path)\n                            file_result: LoaderResult = file_loader.load(file_source)\n\n                            file_chunks = file_chunker.chunk(file_result.content)\n\n                            for chunk_idx, file_chunk in enumerate(file_chunks):\n                                file_metadata: dict[str, Any] = base_metadata.copy()\n                                file_metadata.update(file_result.metadata)\n                                file_metadata[\"data_type\"] = str(file_data_type)\n                                file_metadata[\"file_path\"] = file_path\n                                file_metadata[\"chunk_index\"] = chunk_idx\n                                file_metadata[\"total_chunks\"] = len(file_chunks)\n\n                                if isinstance(arg, dict):\n                                    file_metadata.update(arg.get(\"metadata\", {}))\n\n                                chunk_hash = hashlib.sha256(\n                                    f\"{file_result.doc_id}_{chunk_idx}_{file_chunk}\".encode()\n                                ).hexdigest()\n                                chunk_id = str(uuid.UUID(chunk_hash[:32]))\n\n                                documents.append(\n                                    {\n                                        \"doc_id\": chunk_id,\n                                        \"content\": file_chunk,\n                                        \"metadata\": sanitize_metadata_for_chromadb(\n                                            file_metadata\n                                        ),\n                                    }\n                                )\n                        except Exception:  # noqa: S112\n                            # Silently skip files that can't be processed\n                            continue\n            else:\n                metadata: dict[str, Any] = base_metadata.copy()\n                source_content = SourceContent(source_ref)\n\n                if data_type in [\n                    DataType.PDF_FILE,\n                    DataType.TEXT_FILE,\n                    DataType.DOCX,\n                    DataType.CSV,\n                    DataType.JSON,\n                    DataType.XML,\n                    DataType.MDX,\n                ]:\n                    if not source_content.is_url() and not source_content.path_exists():\n                        raise FileNotFoundError(f\"File does not exist: {source_ref}\")\n\n                loader = data_type.get_loader()\n                chunker = data_type.get_chunker()\n\n                loader_result: LoaderResult = loader.load(source_content)\n\n                chunks = chunker.chunk(loader_result.content)\n\n                for i, chunk in enumerate(chunks):\n                    chunk_metadata: dict[str, Any] = metadata.copy()\n                    chunk_metadata.update(loader_result.metadata)\n                    chunk_metadata[\"data_type\"] = str(data_type)\n                    chunk_metadata[\"chunk_index\"] = i\n                    chunk_metadata[\"total_chunks\"] = len(chunks)\n                    chunk_metadata[\"source\"] = source_ref\n\n                    if isinstance(arg, dict):\n                        chunk_metadata.update(arg.get(\"metadata\", {}))\n\n                    chunk_hash = hashlib.sha256(\n                        f\"{loader_result.doc_id}_{i}_{chunk}\".encode()\n                    ).hexdigest()\n                    chunk_id = str(uuid.UUID(chunk_hash[:32]))\n\n                    documents.append(\n                        {\n                            \"doc_id\": chunk_id,\n                            \"content\": chunk,\n                            \"metadata\": sanitize_metadata_for_chromadb(chunk_metadata),\n                        }\n                    )\n\n        if documents:\n            if self._client is None:\n                raise ValueError(\"Client is not initialized\")\n            self._client.add_documents(\n                collection_name=self.collection_name, documents=documents\n            )",
    "source": "github_repo:crewAIInc/crewAI",
    "file": "lib/crewai-tools/src/crewai_tools/adapters/crewai_rag_adapter.py",
    "license": "MIT",
    "language": "python"
}