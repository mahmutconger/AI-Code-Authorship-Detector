{
    "code": "def evaluate_example(idx, model, tokenizer, data, device, task_meta):\n    \"\"\"Evaluate a single example, return True if correct, False otherwise\"\"\"\n    item = data[idx]\n    task_type = task_meta['task_type']\n    num_fewshot = task_meta['num_fewshot']\n    continuation_delimiter = task_meta['continuation_delimiter']\n\n    # Sample few-shot examples (excluding current item)\n    fewshot_examples = []\n    if num_fewshot > 0:\n        rng = random.Random(1234 + idx)\n        available_indices = [i for i in range(len(data)) if i != idx]\n        fewshot_indices = rng.sample(available_indices, num_fewshot)\n        fewshot_examples = [data[i] for i in fewshot_indices]\n\n    # Render prompts and batch sequences based on task type\n    if task_type == 'multiple_choice':\n        prompts = render_prompts_mc(item, continuation_delimiter, fewshot_examples)\n        tokens, start_idxs, end_idxs = batch_sequences_mc(tokenizer, prompts)\n    elif task_type == 'schema':\n        prompts = render_prompts_schema(item, continuation_delimiter, fewshot_examples)\n        tokens, start_idxs, end_idxs = batch_sequences_schema(tokenizer, prompts)\n    elif task_type == 'language_modeling':\n        prompts = render_prompts_lm(item, continuation_delimiter, fewshot_examples)\n        tokens, start_idxs, end_idxs = batch_sequences_lm(tokenizer, prompts)\n    else:\n        raise ValueError(f\"Unsupported task type: {task_type}\")\n\n    # Some models can't forward sequences beyond a certain length (e.g. GPT-2)\n    # In these cases, we have to truncate sequences to max length and adjust the indices\n    if hasattr(model, 'max_seq_len') and model.max_seq_len is not None:\n        max_tokens = model.max_seq_len\n        new_tokens, new_start_idxs, new_end_idxs = [], [], []\n        for t, s, e in zip(tokens, start_idxs, end_idxs):\n            if len(t) > max_tokens:\n                num_to_crop = len(t) - max_tokens\n                new_tokens.append(t[-max_tokens:]) # take the last max_tokens tokens\n                new_start_idxs.append(s - num_to_crop) # shift the indices down\n                new_end_idxs.append(e - num_to_crop)\n                assert s - num_to_crop >= 0, \"this should never happen right?\"\n                assert e - num_to_crop >= 0, \"this should never happen right?\"\n            else:\n                new_tokens.append(t) # keep unchanged\n                new_start_idxs.append(s)\n                new_end_idxs.append(e)\n        tokens, start_idxs, end_idxs = new_tokens, new_start_idxs, new_end_idxs\n\n    # Stack up all the sequences into a batch\n    pad_token_id = tokenizer.get_bos_token_id() # use BOS as pad token is ok\n    input_ids = stack_sequences(tokens, pad_token_id)\n    input_ids = input_ids.to(device)\n\n    # Forward the model, get the autoregressive loss and argmax prediction at each token\n    losses, predictions = forward_model(model, input_ids)\n\n    # See if the losses/predictions come out correctly\n    if task_type == 'language_modeling':\n        # language modeling task is currently always batch size 1\n        si = start_idxs[0]\n        ei = end_idxs[0]\n        # predictions[i] predict input_ids[i+1] autoregressively\n        predicted_tokens = predictions[0, si-1:ei-1]\n        actual_tokens = input_ids[0, si:ei]\n        is_correct = torch.all(predicted_tokens == actual_tokens).item()\n    elif task_type in ['multiple_choice', 'schema']:\n        # For MC/schema: find the option with lowest average loss\n        mean_losses = [losses[i, si-1:ei-1].mean().item()\n                        for i, (si, ei) in enumerate(zip(start_idxs, end_idxs))]\n        pred_idx = mean_losses.index(min(mean_losses))\n        is_correct = pred_idx == item['gold']\n    else:\n        raise ValueError(f\"Unsupported task type: {task_type}\")\n\n    return is_correct",
    "source": "github_repo:karpathy/nanochat",
    "file": "nanochat/core_eval.py",
    "license": "MIT",
    "language": "python"
}