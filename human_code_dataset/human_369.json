{
    "code": "def detect_language(\n    model: \"Whisper\", mel: Tensor, tokenizer: Tokenizer = None\n) -> Tuple[Tensor, List[dict]]:\n    \"\"\"\n    Detect the spoken language in the audio, and return them as list of strings, along with the ids\n    of the most probable language tokens and the probability distribution over all language tokens.\n    This is performed outside the main decode loop in order to not interfere with kv-caching.\n\n    Returns\n    -------\n    language_tokens : Tensor, shape = (n_audio,)\n        ids of the most probable language tokens, which appears after the startoftranscript token.\n    language_probs : List[Dict[str, float]], length = n_audio\n        list of dictionaries containing the probability distribution over all languages.\n    \"\"\"\n    if tokenizer is None:\n        tokenizer = get_tokenizer(\n            model.is_multilingual, num_languages=model.num_languages\n        )\n    if (\n        tokenizer.language is None\n        or tokenizer.language_token not in tokenizer.sot_sequence\n    ):\n        raise ValueError(\n            \"This model doesn't have language tokens so it can't perform lang id\"\n        )\n\n    single = mel.ndim == 2\n    if single:\n        mel = mel.unsqueeze(0)\n\n    # skip encoder forward pass if already-encoded audio features were given\n    if mel.shape[-2:] != (model.dims.n_audio_ctx, model.dims.n_audio_state):\n        mel = model.encoder(mel)\n\n    # forward pass using a single token, startoftranscript\n    n_audio = mel.shape[0]\n    x = torch.tensor([[tokenizer.sot]] * n_audio).to(mel.device)  # [n_audio, 1]\n    logits = model.logits(x, mel)[:, 0]\n\n    # collect detected languages; suppress all non-language tokens\n    mask = torch.ones(logits.shape[-1], dtype=torch.bool)\n    mask[list(tokenizer.all_language_tokens)] = False\n    logits[:, mask] = -np.inf\n    language_tokens = logits.argmax(dim=-1)\n    language_token_probs = logits.softmax(dim=-1).cpu()\n    language_probs = [\n        {\n            c: language_token_probs[i, j].item()\n            for j, c in zip(tokenizer.all_language_tokens, tokenizer.all_language_codes)\n        }\n        for i in range(n_audio)\n    ]\n\n    if single:\n        language_tokens = language_tokens[0]\n        language_probs = language_probs[0]\n\n    return language_tokens, language_probs",
    "source": "github_repo:openai/whisper",
    "file": "whisper/decoding.py",
    "license": "MIT",
    "language": "python"
}