{
    "code": "from config import ANTHROPIC_API_KEY, GEMINI_API_KEY, OPENAI_API_KEY\nfrom llm import Llm, ANTHROPIC_MODELS, GEMINI_MODELS\nfrom models import (\n    stream_claude_response,\n    stream_gemini_response,\n    stream_openai_response,\n)\nfrom prompts import assemble_prompt\nfrom prompts.types import Stack\nfrom openai.types.chat import ChatCompletionMessageParam\n\n\nasync def generate_code_for_image(image_url: str, stack: Stack, model: Llm) -> str:\n    prompt_messages = assemble_prompt(image_url, stack)\n    return await generate_code_core(prompt_messages, model)\n\n\nasync def generate_code_core(\n    prompt_messages: list[ChatCompletionMessageParam], model: Llm\n) -> str:\n\n    async def process_chunk(_: str):\n        pass\n\n    if model in ANTHROPIC_MODELS:\n        if not ANTHROPIC_API_KEY:\n            raise Exception(\"Anthropic API key not found\")\n\n        completion = await stream_claude_response(\n            prompt_messages,\n            api_key=ANTHROPIC_API_KEY,\n            callback=lambda x: process_chunk(x),\n            model_name=model.value,\n        )\n    elif model in GEMINI_MODELS:\n        if not GEMINI_API_KEY:\n            raise Exception(\"Gemini API key not found\")\n\n        completion = await stream_gemini_response(\n            prompt_messages,\n            api_key=GEMINI_API_KEY,\n            callback=lambda x: process_chunk(x),\n            model_name=model.value,\n        )\n    else:\n        if not OPENAI_API_KEY:\n            raise Exception(\"OpenAI API key not found\")\n\n        completion = await stream_openai_response(\n            prompt_messages,\n            api_key=OPENAI_API_KEY,\n            base_url=None,\n            callback=lambda x: process_chunk(x),\n            model_name=model.value,\n        )\n\n    return completion[\"code\"]",
    "source": "github_repo:abi/screenshot-to-code",
    "file": "backend/evals/core.py",
    "license": "MIT",
    "language": "python"
}