{
    "code": "def main(\n    path_to_agent: Annotated[\n        str,\n        typer.Argument(\n            help=\"python file that contains a function called 'default_config_agent'\"\n        ),\n    ],\n    bench_config: Annotated[\n        str, typer.Argument(help=\"optional task name in benchmark\")\n    ] = os.path.join(os.path.dirname(__file__), \"default_bench_config.toml\"),\n    yaml_output: Annotated[\n        Optional[str],\n        typer.Option(help=\"print results for each task\", show_default=False),\n    ] = None,\n    verbose: Annotated[\n        Optional[bool],\n        typer.Option(help=\"print results for each task\", show_default=False),\n    ] = False,\n    use_cache: Annotated[\n        Optional[bool],\n        typer.Option(\n            help=\"Speeds up computations and saves tokens when running the same prompt multiple times by caching the LLM response.\",\n            show_default=False,\n        ),\n    ] = True,\n):\n    \"\"\"\n    The main function that runs the specified benchmarks with the given agent and outputs the results to the console.\n\n    Parameters\n    ----------\n    path_to_agent : str\n        The file path to the Python module that contains a function called 'default_config_agent'.\n    bench_config : str, default=default_bench_config.toml\n        Configuration file for choosing which benchmark problems to run. See default config for more details.\n    yaml_output: Optional[str], default=None\n        Pass a path to a yaml file to have results written to file.\n    verbose : Optional[bool], default=False\n        A flag to indicate whether to print results for each task.\n    use_cache : Optional[bool], default=True\n        Speeds up computations and saves tokens when running the same prompt multiple times by caching the LLM response.\n    Returns\n    -------\n    None\n    \"\"\"\n    if use_cache:\n        set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n    load_env_if_needed()\n    config = BenchConfig.from_toml(bench_config)\n    print(\"using config file: \" + bench_config)\n    benchmarks = list()\n    benchmark_results = dict()\n    for specific_config_name in vars(config):\n        specific_config = getattr(config, specific_config_name)\n        if hasattr(specific_config, \"active\"):\n            if specific_config.active:\n                benchmarks.append(specific_config_name)\n\n    for benchmark_name in benchmarks:\n        benchmark = get_benchmark(benchmark_name, config)\n        if len(benchmark.tasks) == 0:\n            print(\n                benchmark_name\n                + \" was skipped, since no tasks are specified. Increase the number of tasks in the config file at: \"\n                + bench_config\n            )\n            continue\n        agent = get_agent(path_to_agent)\n\n        results = run(agent, benchmark, verbose=verbose)\n        print(\n            f\"\\n--- Results for agent {path_to_agent}, benchmark: {benchmark_name} ---\"\n        )\n        print_results(results)\n        print()\n        benchmark_results[benchmark_name] = {\n            \"detailed\": [result.to_dict() for result in results]\n        }\n    if yaml_output is not None:\n        export_yaml_results(yaml_output, benchmark_results, config.to_dict())",
    "source": "github_repo:AntonOsika/gpt-engineer",
    "file": "gpt_engineer/benchmark/__main__.py",
    "license": "MIT",
    "language": "python"
}