{
    "code": "def main(\n    ckpt_path: str,\n    config: str,\n    input_file: str = \"\",\n    interactive: bool = True,\n    max_new_tokens: int = 100,\n    temperature: float = 1.0,\n) -> None:\n    \"\"\"\n    Main function to load the model and perform interactive or batch text generation.\n\n    Args:\n        ckpt_path (str): Path to the model checkpoint directory.\n        config (str): Path to the model configuration file.\n        input_file (str, optional): Path to a file containing input prompts. Defaults to \"\".\n        interactive (bool, optional): Whether to run in interactive mode. Defaults to True.\n        max_new_tokens (int, optional): Maximum number of new tokens to generate. Defaults to 100.\n        temperature (float, optional): Temperature for sampling. Defaults to 1.0.\n    \"\"\"\n    world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n    rank = int(os.getenv(\"RANK\", \"0\"))\n    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n    if world_size > 1:\n        dist.init_process_group(\"nccl\")\n    global print\n    if rank != 0:\n        print = lambda *_, **__: None\n    torch.cuda.set_device(local_rank)\n    torch.set_default_dtype(torch.bfloat16)\n    torch.set_num_threads(8)\n    torch.manual_seed(965)\n    with open(config) as f:\n        args = ModelArgs(**json.load(f))\n    print(args)\n    with torch.device(\"cuda\"):\n        model = Transformer(args)\n    tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n    tokenizer.decode(generate(model, [tokenizer.encode(\"DeepSeek\")], 2, -1, 1.)[0])\n    load_model(model, os.path.join(ckpt_path, f\"model{rank}-mp{world_size}.safetensors\"))\n\n    if interactive:\n        messages = []\n        while True:\n            if world_size == 1:\n                prompt = input(\">>> \")\n            elif rank == 0:\n                prompt = input(\">>> \")\n                objects = [prompt]\n                dist.broadcast_object_list(objects, 0)\n            else:\n                objects = [None]\n                dist.broadcast_object_list(objects, 0)\n                prompt = objects[0]\n            if prompt == \"/exit\":\n                break\n            elif prompt == \"/clear\":\n                messages.clear()\n                continue\n            messages.append({\"role\": \"user\", \"content\": prompt})\n            prompt_tokens = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n            completion_tokens = generate(model, [prompt_tokens], max_new_tokens, tokenizer.eos_token_id, temperature)\n            completion = tokenizer.decode(completion_tokens[0], skip_special_tokens=True)\n            print(completion)\n            messages.append({\"role\": \"assistant\", \"content\": completion})\n    else:\n        with open(input_file) as f:\n            prompts = [line.strip() for line in f.readlines()]\n        assert len(prompts) <= args.max_batch_size, f\"Number of prompts exceeds maximum batch size ({args.max_batch_size})\"\n        prompt_tokens = [tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True) for prompt in prompts]\n        completion_tokens = generate(model, prompt_tokens, max_new_tokens, tokenizer.eos_token_id, temperature)\n        completions = tokenizer.batch_decode(completion_tokens, skip_special_tokens=True)\n        for prompt, completion in zip(prompts, completions):\n            print(\"Prompt:\", prompt)\n            print(\"Completion:\", completion)\n            print()\n\n    if world_size > 1:\n        dist.destroy_process_group()",
    "source": "github_repo:deepseek-ai/DeepSeek-V3",
    "file": "inference/generate.py",
    "license": "MIT",
    "language": "python"
}