{
    "code": "class Text2SemanticDataModule(LightningDataModule):\n    def __init__(\n        self,\n        config,\n        train_semantic_path,\n        train_phoneme_path,\n        dev_semantic_path=None,\n        dev_phoneme_path=None,\n    ):\n        super().__init__()\n        self.config = config\n        self.train_semantic_path = train_semantic_path\n        self.train_phoneme_path = train_phoneme_path\n        self.dev_semantic_path = dev_semantic_path\n        self.dev_phoneme_path = dev_phoneme_path\n        self.num_workers = self.config[\"data\"][\"num_workers\"]\n\n    def prepare_data(self):\n        pass\n\n    def setup(self, stage=None, output_logs=False):\n        self._train_dataset = Text2SemanticDataset(\n            phoneme_path=self.train_phoneme_path,\n            semantic_path=self.train_semantic_path,\n            max_sec=self.config[\"data\"][\"max_sec\"],\n            pad_val=self.config[\"data\"][\"pad_val\"],\n        )\n        self._dev_dataset = self._train_dataset\n        # self._dev_dataset = Text2SemanticDataset(\n        #     phoneme_path=self.dev_phoneme_path,\n        #     semantic_path=self.dev_semantic_path,\n        #     max_sample=self.config['data']['max_eval_sample'],\n        #     max_sec=self.config['data']['max_sec'],\n        #     pad_val=self.config['data']['pad_val'])\n\n    def train_dataloader(self):\n        batch_size = (\n            self.config[\"train\"][\"batch_size\"] // 2\n            if self.config[\"train\"].get(\"if_dpo\", False) is True\n            else self.config[\"train\"][\"batch_size\"]\n        )\n        batch_size = max(min(batch_size, len(self._train_dataset) // 4), 1)  # 防止不保存\n        sampler = DistributedBucketSampler(self._train_dataset, batch_size=batch_size)\n        return DataLoader(\n            self._train_dataset,\n            batch_size=batch_size,\n            sampler=sampler,\n            collate_fn=self._train_dataset.collate,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            prefetch_factor=16,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self._dev_dataset,\n            batch_size=1,\n            shuffle=False,\n            collate_fn=self._train_dataset.collate,\n            num_workers=max(self.num_workers, 12),\n            persistent_workers=True,\n            prefetch_factor=16,\n        )\n\n    # 这个会使用到嘛？\n    def test_dataloader(self):\n        return DataLoader(\n            self._dev_dataset,\n            batch_size=1,\n            shuffle=False,\n            collate_fn=self._train_dataset.collate,\n        )",
    "source": "github_repo:RVC-Boss/GPT-SoVITS",
    "file": "GPT_SoVITS/AR/data/data_module.py",
    "license": "MIT",
    "language": "python"
}