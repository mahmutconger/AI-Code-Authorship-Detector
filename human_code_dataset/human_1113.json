{
    "code": "class ChatCompletions:\n    def __init__(self, client):\n        self.client = client\n\n    def _convert_openai_tools_to_bedrock_format(self, tools):\n        # Convert OpenAI function calling format to Bedrock tool format\n        bedrock_tools = []\n        for tool in tools:\n            if tool.get(\"type\") == \"function\":\n                function = tool.get(\"function\", {})\n                bedrock_tool = {\n                    \"toolSpec\": {\n                        \"name\": function.get(\"name\", \"\"),\n                        \"description\": function.get(\"description\", \"\"),\n                        \"inputSchema\": {\n                            \"json\": {\n                                \"type\": \"object\",\n                                \"properties\": function.get(\"parameters\", {}).get(\n                                    \"properties\", {}\n                                ),\n                                \"required\": function.get(\"parameters\", {}).get(\n                                    \"required\", []\n                                ),\n                            }\n                        },\n                    }\n                }\n                bedrock_tools.append(bedrock_tool)\n        return bedrock_tools\n\n    def _convert_openai_messages_to_bedrock_format(self, messages):\n        # Convert OpenAI message format to Bedrock message format\n        bedrock_messages = []\n        system_prompt = []\n        for message in messages:\n            if message.get(\"role\") == \"system\":\n                system_prompt = [{\"text\": message.get(\"content\")}]\n            elif message.get(\"role\") == \"user\":\n                bedrock_message = {\n                    \"role\": message.get(\"role\", \"user\"),\n                    \"content\": [{\"text\": message.get(\"content\")}],\n                }\n                bedrock_messages.append(bedrock_message)\n            elif message.get(\"role\") == \"assistant\":\n                bedrock_message = {\n                    \"role\": \"assistant\",\n                    \"content\": [{\"text\": message.get(\"content\")}],\n                }\n                openai_tool_calls = message.get(\"tool_calls\", [])\n                if openai_tool_calls:\n                    bedrock_tool_use = {\n                        \"toolUseId\": openai_tool_calls[0][\"id\"],\n                        \"name\": openai_tool_calls[0][\"function\"][\"name\"],\n                        \"input\": json.loads(\n                            openai_tool_calls[0][\"function\"][\"arguments\"]\n                        ),\n                    }\n                    bedrock_message[\"content\"].append({\"toolUse\": bedrock_tool_use})\n                    global CURRENT_TOOLUSE_ID\n                    CURRENT_TOOLUSE_ID = openai_tool_calls[0][\"id\"]\n                bedrock_messages.append(bedrock_message)\n            elif message.get(\"role\") == \"tool\":\n                bedrock_message = {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"toolResult\": {\n                                \"toolUseId\": CURRENT_TOOLUSE_ID,\n                                \"content\": [{\"text\": message.get(\"content\")}],\n                            }\n                        }\n                    ],\n                }\n                bedrock_messages.append(bedrock_message)\n            else:\n                raise ValueError(f\"Invalid role: {message.get('role')}\")\n        return system_prompt, bedrock_messages\n\n    def _convert_bedrock_response_to_openai_format(self, bedrock_response):\n        # Convert Bedrock response format to OpenAI format\n        content = \"\"\n        if bedrock_response.get(\"output\", {}).get(\"message\", {}).get(\"content\"):\n            content_array = bedrock_response[\"output\"][\"message\"][\"content\"]\n            content = \"\".join(item.get(\"text\", \"\") for item in content_array)\n        if content == \"\":\n            content = \".\"\n\n        # Handle tool calls in response\n        openai_tool_calls = []\n        if bedrock_response.get(\"output\", {}).get(\"message\", {}).get(\"content\"):\n            for content_item in bedrock_response[\"output\"][\"message\"][\"content\"]:\n                if content_item.get(\"toolUse\"):\n                    bedrock_tool_use = content_item[\"toolUse\"]\n                    global CURRENT_TOOLUSE_ID\n                    CURRENT_TOOLUSE_ID = bedrock_tool_use[\"toolUseId\"]\n                    openai_tool_call = {\n                        \"id\": CURRENT_TOOLUSE_ID,\n                        \"type\": \"function\",\n                        \"function\": {\n                            \"name\": bedrock_tool_use[\"name\"],\n                            \"arguments\": json.dumps(bedrock_tool_use[\"input\"]),\n                        },\n                    }\n                    openai_tool_calls.append(openai_tool_call)\n\n        # Construct final OpenAI format response\n        openai_format = {\n            \"id\": f\"chatcmpl-{uuid.uuid4()}\",\n            \"created\": int(time.time()),\n            \"object\": \"chat.completion\",\n            \"system_fingerprint\": None,\n            \"choices\": [\n                {\n                    \"finish_reason\": bedrock_response.get(\"stopReason\", \"end_turn\"),\n                    \"index\": 0,\n                    \"message\": {\n                        \"content\": content,\n                        \"role\": bedrock_response.get(\"output\", {})\n                        .get(\"message\", {})\n                        .get(\"role\", \"assistant\"),\n                        \"tool_calls\": openai_tool_calls\n                        if openai_tool_calls != []\n                        else None,\n                        \"function_call\": None,\n                    },\n                }\n            ],\n            \"usage\": {\n                \"completion_tokens\": bedrock_response.get(\"usage\", {}).get(\n                    \"outputTokens\", 0\n                ),\n                \"prompt_tokens\": bedrock_response.get(\"usage\", {}).get(\n                    \"inputTokens\", 0\n                ),\n                \"total_tokens\": bedrock_response.get(\"usage\", {}).get(\"totalTokens\", 0),\n            },\n        }\n        return OpenAIResponse(openai_format)\n\n    async def _invoke_bedrock(\n        self,\n        model: str,\n        messages: List[Dict[str, str]],\n        max_tokens: int,\n        temperature: float,\n        tools: Optional[List[dict]] = None,\n        tool_choice: Literal[\"none\", \"auto\", \"required\"] = \"auto\",\n        **kwargs,\n    ) -> OpenAIResponse:\n        # Non-streaming invocation of Bedrock model\n        (\n            system_prompt,\n            bedrock_messages,\n        ) = self._convert_openai_messages_to_bedrock_format(messages)\n        response = self.client.converse(\n            modelId=model,\n            system=system_prompt,\n            messages=bedrock_messages,\n            inferenceConfig={\"temperature\": temperature, \"maxTokens\": max_tokens},\n            toolConfig={\"tools\": tools} if tools else None,\n        )\n        openai_response = self._convert_bedrock_response_to_openai_format(response)\n        return openai_response\n\n    async def _invoke_bedrock_stream(\n        self,\n        model: str,\n        messages: List[Dict[str, str]],\n        max_tokens: int,\n        temperature: float,\n        tools: Optional[List[dict]] = None,\n        tool_choice: Literal[\"none\", \"auto\", \"required\"] = \"auto\",\n        **kwargs,\n    ) -> OpenAIResponse:\n        # Streaming invocation of Bedrock model\n        (\n            system_prompt,\n            bedrock_messages,\n        ) = self._convert_openai_messages_to_bedrock_format(messages)\n        response = self.client.converse_stream(\n            modelId=model,\n            system=system_prompt,\n            messages=bedrock_messages,\n            inferenceConfig={\"temperature\": temperature, \"maxTokens\": max_tokens},\n            toolConfig={\"tools\": tools} if tools else None,\n        )\n\n        # Initialize response structure\n        bedrock_response = {\n            \"output\": {\"message\": {\"role\": \"\", \"content\": []}},\n            \"stopReason\": \"\",\n            \"usage\": {},\n            \"metrics\": {},\n        }\n        bedrock_response_text = \"\"\n        bedrock_response_tool_input = \"\"\n\n        # Process streaming response\n        stream = response.get(\"stream\")\n        if stream:\n            for event in stream:\n                if event.get(\"messageStart\", {}).get(\"role\"):\n                    bedrock_response[\"output\"][\"message\"][\"role\"] = event[\n                        \"messageStart\"\n                    ][\"role\"]\n                if event.get(\"contentBlockDelta\", {}).get(\"delta\", {}).get(\"text\"):\n                    bedrock_response_text += event[\"contentBlockDelta\"][\"delta\"][\"text\"]\n                    print(\n                        event[\"contentBlockDelta\"][\"delta\"][\"text\"], end=\"\", flush=True\n                    )\n                if event.get(\"contentBlockStop\", {}).get(\"contentBlockIndex\") == 0:\n                    bedrock_response[\"output\"][\"message\"][\"content\"].append(\n                        {\"text\": bedrock_response_text}\n                    )\n                if event.get(\"contentBlockStart\", {}).get(\"start\", {}).get(\"toolUse\"):\n                    bedrock_tool_use = event[\"contentBlockStart\"][\"start\"][\"toolUse\"]\n                    tool_use = {\n                        \"toolUseId\": bedrock_tool_use[\"toolUseId\"],\n                        \"name\": bedrock_tool_use[\"name\"],\n                    }\n                    bedrock_response[\"output\"][\"message\"][\"content\"].append(\n                        {\"toolUse\": tool_use}\n                    )\n                    global CURRENT_TOOLUSE_ID\n                    CURRENT_TOOLUSE_ID = bedrock_tool_use[\"toolUseId\"]\n                if event.get(\"contentBlockDelta\", {}).get(\"delta\", {}).get(\"toolUse\"):\n                    bedrock_response_tool_input += event[\"contentBlockDelta\"][\"delta\"][\n                        \"toolUse\"\n                    ][\"input\"]\n                    print(\n                        event[\"contentBlockDelta\"][\"delta\"][\"toolUse\"][\"input\"],\n                        end=\"\",\n                        flush=True,\n                    )\n                if event.get(\"contentBlockStop\", {}).get(\"contentBlockIndex\") == 1:\n                    bedrock_response[\"output\"][\"message\"][\"content\"][1][\"toolUse\"][\n                        \"input\"\n                    ] = json.loads(bedrock_response_tool_input)\n        print()\n        openai_response = self._convert_bedrock_response_to_openai_format(\n            bedrock_response\n        )\n        return openai_response\n\n    def create(\n        self,\n        model: str,\n        messages: List[Dict[str, str]],\n        max_tokens: int,\n        temperature: float,\n        stream: Optional[bool] = True,\n        tools: Optional[List[dict]] = None,\n        tool_choice: Literal[\"none\", \"auto\", \"required\"] = \"auto\",\n        **kwargs,\n    ) -> OpenAIResponse:\n        # Main entry point for chat completion\n        bedrock_tools = []\n        if tools is not None:\n            bedrock_tools = self._convert_openai_tools_to_bedrock_format(tools)\n        if stream:\n            return self._invoke_bedrock_stream(\n                model,\n                messages,\n                max_tokens,\n                temperature,\n                bedrock_tools,\n                tool_choice,\n                **kwargs,\n            )\n        else:\n            return self._invoke_bedrock(\n                model,\n                messages,\n                max_tokens,\n                temperature,\n                bedrock_tools,\n                tool_choice,\n                **kwargs,\n            )",
    "source": "github_repo:FoundationAgents/OpenManus",
    "file": "app/bedrock.py",
    "license": "MIT",
    "language": "python"
}