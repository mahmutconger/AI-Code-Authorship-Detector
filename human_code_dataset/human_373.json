{
    "code": "class PyTorchInference(Inference):\n    def __init__(self, model: \"Whisper\", initial_token_length: int):\n        self.model: \"Whisper\" = model\n        self.initial_token_length = initial_token_length\n        self.kv_cache = {}\n        self.hooks = []\n\n        key_modules = [block.attn.key for block in self.model.decoder.blocks]\n        value_modules = [block.attn.value for block in self.model.decoder.blocks]\n        self.kv_modules = key_modules + value_modules\n\n    def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:\n        if not self.kv_cache:\n            self.kv_cache, self.hooks = self.model.install_kv_cache_hooks()\n\n        if tokens.shape[-1] > self.initial_token_length:\n            # only need to use the last token except in the first forward pass\n            tokens = tokens[:, -1:]\n\n        return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)\n\n    def cleanup_caching(self):\n        for hook in self.hooks:\n            hook.remove()\n\n        self.kv_cache = {}\n        self.hooks = []\n\n    def rearrange_kv_cache(self, source_indices):\n        if source_indices != list(range(len(source_indices))):\n            for module in self.kv_modules:\n                # update the key/value cache to contain the selected sequences\n                self.kv_cache[module] = self.kv_cache[module][source_indices].detach()",
    "source": "github_repo:openai/whisper",
    "file": "whisper/decoding.py",
    "license": "MIT",
    "language": "python"
}