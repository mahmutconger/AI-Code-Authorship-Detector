{
    "code": "def tokenizing_distributed_data_loader_with_state(B, T, split, tokenizer_threads=4, tokenizer_batch_size=128, device=\"cuda\", resume_state_dict=None):\n    \"\"\"\n    Stream pretraining text from parquet files, tokenize, yield training batches.\n\n    This implementation became a bit more complex because we wish to support approximate resume training.\n    Instead of turning this into a Class, we opt to return the state_dict with every batch,\n    and then the caller can pass in a state_dict to resume training from a desired point.\n    Note that this resumption is atm only *approximate* for simplicity.\n    We won't repeat the same documents but we might skip a few.\n    The state_dict that is returned can be later passed into this function via `resume_state_dict` to approximately resume.\n\n    Perfect state resumption is possible but would be a lot more bloated, probably not worth it atm.\n    \"\"\"\n    assert split in [\"train\", \"val\"], \"split must be 'train' or 'val'\"\n\n    # infinite iterator over document batches (list of text strings)\n    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n    def document_batches():\n        parquet_paths = list_parquet_files()\n        parquet_paths = parquet_paths[:-1] if split == \"train\" else parquet_paths[-1:]\n        resume_pq_idx = resume_state_dict[\"pq_idx\"] if resume_state_dict is not None else 0\n        resume_rg_idx = resume_state_dict[\"rg_idx\"] if resume_state_dict is not None else None\n        pq_idx = resume_pq_idx # we kick off parquet files at the resume index (or by default just 0)\n        while True: # iterate infinitely (multi-epoch)\n            while pq_idx < len(parquet_paths): # iterate over all parquet files\n                filepath = parquet_paths[pq_idx]\n                pf = pq.ParquetFile(filepath)\n                # Start from resume point if resuming on same file, otherwise from DDP rank\n                # I know this state resumption is a little bit tricky and a little bit hacky... sigh.\n                if resume_rg_idx is not None:\n                    base_idx = resume_rg_idx // ddp_world_size # in units of ddp_world_size\n                    base_idx += 1 # advance by 1 so that we definitely don't repeat data after resuming\n                    rg_idx = base_idx * ddp_world_size + ddp_rank\n                    resume_rg_idx = None # set to None as we only want to do this a single time\n                else:\n                    rg_idx = ddp_rank\n                while rg_idx < pf.num_row_groups:\n                    rg = pf.read_row_group(rg_idx)\n                    batch = rg.column('text').to_pylist() # each batch is a parquet group, e.g. 1024 rows\n                    # the tokenizer encode might want to go in even smaller batches, e.g. 128 rows\n                    for i in range(0, len(batch), tokenizer_batch_size):\n                        yield batch[i:i+tokenizer_batch_size], (pq_idx, rg_idx)\n                    rg_idx += ddp_world_size # advance to the next row group (in DDP)\n                pq_idx += 1 # advance to the next parquet file\n    batches = document_batches()\n\n    # Now emit batches of tokens.\n    needed_tokens = B * T + 1 # +1 is because we also need the target at the last token\n    # get the tokenizer and the bos token\n    tokenizer = get_tokenizer()\n    bos_token = tokenizer.get_bos_token_id()\n    # scratch buffer holds the tokens for one iteration\n    token_buffer = deque() # we stream tokens on the right and pop from the left\n    while True:\n        # Accumulate enough tokens for one iteration before yielding.\n        while len(token_buffer) < needed_tokens:\n            doc_batch, (pq_idx, rg_idx) = next(batches)\n            token_lists = tokenizer.encode(doc_batch, prepend=bos_token, num_threads=tokenizer_threads)\n            for tokens in token_lists:\n                token_buffer.extend(tokens)\n        # Move tokens from the deque into the scratch buffer\n        tokens = [token_buffer.popleft() for _ in range(needed_tokens)]\n        # CUDA supports memory pinning for asynchronous transfers between CPU and GPU\n        use_cuda_optimizations = device == \"cuda\"\n        scratch = torch.tensor(tokens, dtype=torch.long, pin_memory=use_cuda_optimizations) # in PyTorch, long=int64\n        # Create the inputs/targets as 1D tensors\n        inputs_cpu = scratch[:-1]\n        targets_cpu = scratch[1:]\n        # Reshape to 2D and move to GPU async\n        inputs = inputs_cpu.view(B, T).to(device=device, non_blocking=use_cuda_optimizations)\n        targets = targets_cpu.view(B, T).to(device=device, non_blocking=use_cuda_optimizations)\n        state_dict = {\"pq_idx\": pq_idx, \"rg_idx\": rg_idx} # we need this in case we wish to approximately resume training\n        yield inputs, targets, state_dict",
    "source": "github_repo:karpathy/nanochat",
    "file": "nanochat/dataloader.py",
    "license": "MIT",
    "language": "python"
}