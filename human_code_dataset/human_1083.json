{
    "code": "def infer_panel_naive(\n        self,\n        x: torch.LongTensor,  #####全部文本token\n        x_lens: torch.LongTensor,\n        prompts: torch.LongTensor,  ####参考音频token\n        bert_feature: torch.LongTensor,\n        top_k: int = -100,\n        top_p: int = 100,\n        early_stop_num: int = -1,\n        temperature: float = 1.0,\n        repetition_penalty: float = 1.35,\n        **kwargs,\n    ):\n        x = self.ar_text_embedding(x)\n        x = x + self.bert_proj(bert_feature.transpose(1, 2))\n        x = self.ar_text_position(x)\n\n        # AR Decoder\n        y = prompts\n\n        x_len = x.shape[1]\n        x_attn_mask = torch.zeros((x_len, x_len), dtype=torch.bool)\n        stop = False\n        # print(1111111,self.num_layers)\n\n        k_cache = None\n        v_cache = None\n        ###################  first step ##########################\n        if y is not None:\n            y_emb = self.ar_audio_embedding(y)\n            y_len = y_emb.shape[1]\n            prefix_len = y.shape[1]\n            y_pos = self.ar_audio_position(y_emb)\n            xy_pos = torch.concat([x, y_pos], dim=1)\n            ref_free = False\n        else:\n            y_emb = None\n            y_len = 0\n            prefix_len = 0\n            y_pos = None\n            xy_pos = x\n            y = torch.zeros(x.shape[0], 0, dtype=torch.int, device=x.device)\n            ref_free = True\n\n        bsz = x.shape[0]\n        src_len = x_len + y_len\n        x_attn_mask_pad = F.pad(\n            x_attn_mask,\n            (0, y_len),  ###xx的纯0扩展到xx纯0+xy纯1，(x,x+y)\n            value=True,\n        )\n        y_attn_mask = F.pad(  ###yy的右上1扩展到左边xy的0,(y,x+y)\n            torch.triu(torch.ones(y_len, y_len, dtype=torch.bool), diagonal=1),\n            (x_len, 0),\n            value=False,\n        )\n        xy_attn_mask = (\n            torch.concat([x_attn_mask_pad, y_attn_mask], dim=0)\n            .unsqueeze(0)\n            .expand(bsz * self.num_head, -1, -1)\n            .view(bsz, self.num_head, src_len, src_len)\n            .to(device=x.device, dtype=torch.bool)\n        )\n\n        for idx in tqdm(range(1500)):\n            if xy_attn_mask is not None:\n                xy_dec, k_cache, v_cache = self.t2s_transformer.process_prompt(xy_pos, xy_attn_mask, None)\n            else:\n                xy_dec, k_cache, v_cache = self.t2s_transformer.decode_next_token(xy_pos, k_cache, v_cache)\n\n            logits = self.ar_predict_layer(xy_dec[:, -1])\n\n            if idx == 0:\n                xy_attn_mask = None\n            if idx < 11:  ###至少预测出10个token不然不给停止（0.4s）\n                logits = logits[:, :-1]\n\n            samples = sample(\n                logits, y, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty, temperature=temperature\n            )[0]\n\n            y = torch.concat([y, samples], dim=1)\n\n            if early_stop_num != -1 and (y.shape[1] - prefix_len) > early_stop_num:\n                print(\"use early stop num:\", early_stop_num)\n                stop = True\n\n            if torch.argmax(logits, dim=-1)[0] == self.EOS or samples[0, 0] == self.EOS:\n                stop = True\n            if stop:\n                if y.shape[1] == 0:\n                    y = torch.concat([y, torch.zeros_like(samples)], dim=1)\n                    print(\"bad zero prediction\")\n                print(f\"T2S Decoding EOS [{prefix_len} -> {y.shape[1]}]\")\n                break\n\n            ####################### update next step ###################################\n            y_emb = self.ar_audio_embedding(y[:, -1:])\n            xy_pos = y_emb * self.ar_audio_position.x_scale + self.ar_audio_position.alpha * self.ar_audio_position.pe[\n                :, y_len + idx\n            ].to(dtype=y_emb.dtype, device=y_emb.device)\n\n        if ref_free:\n            return y[:, :-1], 0\n        return y[:, :-1], idx",
    "source": "github_repo:RVC-Boss/GPT-SoVITS",
    "file": "GPT_SoVITS/AR/models/t2s_model.py",
    "license": "MIT",
    "language": "python"
}