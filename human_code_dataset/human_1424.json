{
    "code": "class LatentDepth2ImageDiffusion(LatentFinetuneDiffusion):\n    \"\"\"\n    condition on monocular depth estimation\n    \"\"\"\n\n    def __init__(self, depth_stage_config, concat_keys=(\"midas_in\",), *args, **kwargs):\n        super().__init__(concat_keys=concat_keys, *args, **kwargs)\n        self.depth_model = instantiate_from_config(depth_stage_config)\n        self.depth_stage_key = concat_keys[0]\n\n    @torch.no_grad()\n    def get_input(self, batch, k, cond_key=None, bs=None, return_first_stage_outputs=False):\n        # note: restricted to non-trainable encoders currently\n        assert not self.cond_stage_trainable, 'trainable cond stages not yet supported for depth2img'\n        z, c, x, xrec, xc = super().get_input(batch, self.first_stage_key, return_first_stage_outputs=True,\n                                              force_c_encode=True, return_original_cond=True, bs=bs)\n\n        assert exists(self.concat_keys)\n        assert len(self.concat_keys) == 1\n        c_cat = list()\n        for ck in self.concat_keys:\n            cc = batch[ck]\n            if bs is not None:\n                cc = cc[:bs]\n                cc = cc.to(self.device)\n            cc = self.depth_model(cc)\n            cc = torch.nn.functional.interpolate(\n                cc,\n                size=z.shape[2:],\n                mode=\"bicubic\",\n                align_corners=False,\n            )\n\n            depth_min, depth_max = torch.amin(cc, dim=[1, 2, 3], keepdim=True), torch.amax(cc, dim=[1, 2, 3],\n                                                                                           keepdim=True)\n            cc = 2. * (cc - depth_min) / (depth_max - depth_min + 0.001) - 1.\n            c_cat.append(cc)\n        c_cat = torch.cat(c_cat, dim=1)\n        all_conds = {\"c_concat\": [c_cat], \"c_crossattn\": [c]}\n        if return_first_stage_outputs:\n            return z, all_conds, x, xrec, xc\n        return z, all_conds\n\n    @torch.no_grad()\n    def log_images(self, *args, **kwargs):\n        log = super().log_images(*args, **kwargs)\n        depth = self.depth_model(args[0][self.depth_stage_key])\n        depth_min, depth_max = torch.amin(depth, dim=[1, 2, 3], keepdim=True), \\\n                               torch.amax(depth, dim=[1, 2, 3], keepdim=True)\n        log[\"depth\"] = 2. * (depth - depth_min) / (depth_max - depth_min) - 1.\n        return log",
    "source": "github_repo:Stability-AI/stablediffusion",
    "file": "ldm/models/diffusion/ddpm.py",
    "license": "MIT",
    "language": "python"
}