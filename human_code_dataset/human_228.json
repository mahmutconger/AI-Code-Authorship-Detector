{
    "code": "class Transformer(nn.Module):\n    \"\"\"\n    Transformer model with positional embeddings, multiple layers, and output projection.\n\n    Attributes:\n        max_seq_len (int): Maximum sequence length for the transformer.\n        embed (nn.Module): Embedding layer for input tokens.\n        layers (torch.nn.ModuleList): List of transformer blocks.\n        norm (nn.Module): Layer normalization applied after all blocks.\n        head (nn.Module): Output projection layer mapping to vocabulary size.\n        freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n    \"\"\"\n    def __init__(self, args: ModelArgs):\n        \"\"\"\n        Initializes the Transformer model.\n\n        Args:\n            args (ModelArgs): Model arguments containing transformer parameters.\n        \"\"\"\n        global world_size, rank\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if dist.is_initialized() else 0\n        Linear.dtype = torch.float8_e4m3fn if args.dtype == \"fp8\" else torch.bfloat16\n        Linear.scale_fmt = args.scale_fmt\n        super().__init__()\n        self.max_seq_len = args.max_seq_len\n        self.embed = ParallelEmbedding(args.vocab_size, args.dim)\n        self.layers = torch.nn.ModuleList()\n        for layer_id in range(args.n_layers):\n            self.layers.append(Block(layer_id, args))\n        self.norm = RMSNorm(args.dim)\n        self.head = ColumnParallelLinear(args.dim, args.vocab_size, dtype=torch.get_default_dtype())\n        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(args), persistent=False)\n\n    @torch.inference_mode()\n    def forward(self, tokens: torch.Tensor, start_pos: int = 0):\n        \"\"\"\n        Forward pass for the Transformer model.\n\n        Args:\n            tokens (torch.Tensor): Input tensor of token IDs with shape (batch_size, seq_len).\n            start_pos (int, optional): Starting position in the sequence for rotary embeddings. Defaults to 0.\n\n        Returns:\n            torch.Tensor: Logits tensor of shape (batch_size, vocab_size).\n        \"\"\"\n        seqlen = tokens.size(1)\n        h = self.embed(tokens)\n        freqs_cis = self.freqs_cis[start_pos:start_pos+seqlen]\n        mask = None\n        if seqlen > 1:\n            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device).triu_(1)\n        for layer in self.layers:\n            h = layer(h, start_pos, freqs_cis, mask)\n        h = self.norm(h)[:, -1]\n        logits = self.head(h)\n        if world_size > 1:\n            all_logits = [torch.empty_like(logits) for _ in range(world_size)]\n            dist.all_gather(all_logits, logits)\n            logits = torch.cat(all_logits, dim=-1)\n        return logits",
    "source": "github_repo:deepseek-ai/DeepSeek-V3",
    "file": "inference/model.py",
    "license": "MIT",
    "language": "python"
}