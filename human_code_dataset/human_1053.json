{
    "code": "class Text2SemanticLightningModule(LightningModule):\n    def __init__(self, config, output_dir, is_train=True):\n        super().__init__()\n        self.config = config\n        self.top_k = 3\n        self.model = Text2SemanticDecoder(config=config, top_k=self.top_k)\n        pretrained_s1 = config.get(\"pretrained_s1\")\n        if pretrained_s1 and is_train:\n            # print(self.load_state_dict(torch.load(pretrained_s1,map_location=\"cpu\")[\"state_dict\"]))\n            print(\n                self.load_state_dict(\n                    torch.load(\n                        pretrained_s1,\n                        map_location=\"cpu\",\n                        weights_only=False,\n                    )[\"weight\"],\n                )\n            )\n        if is_train:\n            self.automatic_optimization = False\n            self.save_hyperparameters()\n            self.eval_dir = output_dir / \"eval\"\n            self.eval_dir.mkdir(parents=True, exist_ok=True)\n\n    def training_step(self, batch: Dict, batch_idx: int):\n        opt = self.optimizers()\n        scheduler = self.lr_schedulers()\n        forward = self.model.forward if self.config[\"train\"].get(\"if_dpo\", False) == True else self.model.forward_old\n        loss, acc = forward(\n            batch[\"phoneme_ids\"],\n            batch[\"phoneme_ids_len\"],\n            batch[\"semantic_ids\"],\n            batch[\"semantic_ids_len\"],\n            batch[\"bert_feature\"],\n        )\n        self.manual_backward(loss)\n        if batch_idx > 0 and batch_idx % 4 == 0:\n            opt.step()\n            opt.zero_grad()\n            scheduler.step()\n\n        self.log(\n            \"total_loss\",\n            loss,\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"lr\",\n            scheduler.get_last_lr()[0],\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n        self.log(\n            f\"top_{self.top_k}_acc\",\n            acc,\n            on_step=True,\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n\n    def validation_step(self, batch: Dict, batch_idx: int):\n        return\n\n    # # get loss\n    # loss, acc = self.model.forward(\n    #     batch['phoneme_ids'], batch['phoneme_ids_len'],\n    #     batch['semantic_ids'], batch['semantic_ids_len'],\n    #     batch['bert_feature']\n    # )\n    #\n    # self.log(\n    #     \"val_total_loss\",\n    #     loss,\n    #     on_step=True,\n    #     on_epoch=True,\n    #     prog_bar=True,\n    #     sync_dist=True)\n    # self.log(\n    #     f\"val_top_{self.top_k}_acc\",\n    #     acc,\n    #     on_step=True,\n    #     on_epoch=True,\n    #     prog_bar=True,\n    #     sync_dist=True)\n    #\n    # # get infer output\n    # semantic_len = batch['semantic_ids'].size(1)\n    # prompt_len = min(int(semantic_len * 0.5), 150)\n    # prompt = batch['semantic_ids'][:, :prompt_len]\n    # pred_semantic = self.model.infer(batch['phoneme_ids'],\n    #                                  batch['phoneme_ids_len'], prompt,\n    #                                  batch['bert_feature']\n    #                                  )\n    # save_name = f'semantic_toks_{batch_idx}.pt'\n    # save_path = os.path.join(self.eval_dir, save_name)\n    # torch.save(pred_semantic.detach().cpu(), save_path)\n\n    def configure_optimizers(self):\n        model_parameters = self.model.parameters()\n        parameters_names = []\n        parameters_names.append([name_param_pair[0] for name_param_pair in self.model.named_parameters()])\n        lm_opt = ScaledAdam(\n            model_parameters,\n            lr=0.01,\n            betas=(0.9, 0.95),\n            clipping_scale=2.0,\n            parameters_names=parameters_names,\n            show_dominant_parameters=False,\n            clipping_update_period=1000,\n        )\n\n        return {\n            \"optimizer\": lm_opt,\n            \"lr_scheduler\": {\n                \"scheduler\": WarmupCosineLRSchedule(\n                    lm_opt,\n                    init_lr=self.config[\"optimizer\"][\"lr_init\"],\n                    peak_lr=self.config[\"optimizer\"][\"lr\"],\n                    end_lr=self.config[\"optimizer\"][\"lr_end\"],\n                    warmup_steps=self.config[\"optimizer\"][\"warmup_steps\"],\n                    total_steps=self.config[\"optimizer\"][\"decay_steps\"],\n                )\n            },\n        }",
    "source": "github_repo:RVC-Boss/GPT-SoVITS",
    "file": "GPT_SoVITS/AR/models/t2s_lightning_module.py",
    "license": "MIT",
    "language": "python"
}