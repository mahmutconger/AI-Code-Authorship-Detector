{
    "code": "def num_tokens_from_messages(messages, model):\n    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n\n    if model in [\"wenxin\", \"xunfei\"] or model.startswith(const.GEMINI):\n        return num_tokens_by_character(messages)\n\n    import tiktoken\n\n    if model in [\"gpt-3.5-turbo-0301\", \"gpt-35-turbo\", \"gpt-3.5-turbo-1106\", \"moonshot\", const.LINKAI_35]:\n        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo\")\n    elif model in [\"gpt-4-0314\", \"gpt-4-0613\", \"gpt-4-32k\", \"gpt-4-32k-0613\", \"gpt-3.5-turbo-0613\",\n                   \"gpt-3.5-turbo-16k\", \"gpt-3.5-turbo-16k-0613\", \"gpt-35-turbo-16k\", \"gpt-4-turbo-preview\",\n                   \"gpt-4-1106-preview\", const.GPT4_TURBO_PREVIEW, const.GPT4_VISION_PREVIEW, const.GPT4_TURBO_01_25,\n                   const.GPT_4o, const.GPT_4O_0806, const.GPT_4o_MINI, const.LINKAI_4o, const.LINKAI_4_TURBO, const.GPT_5, const.GPT_5_MINI, const.GPT_5_NANO]:\n        return num_tokens_from_messages(messages, model=\"gpt-4\")\n    elif model.startswith(\"claude-3\"):\n        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo\")\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        logger.debug(\"Warning: model not found. Using cl100k_base encoding.\")\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n    if model == \"gpt-3.5-turbo\":\n        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n        tokens_per_name = -1  # if there's a name, the role is omitted\n    elif model == \"gpt-4\":\n        tokens_per_message = 3\n        tokens_per_name = 1\n    else:\n        logger.debug(f\"num_tokens_from_messages() is not implemented for model {model}. Returning num tokens assuming gpt-3.5-turbo.\")\n        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo\")\n    num_tokens = 0\n    for message in messages:\n        num_tokens += tokens_per_message\n        for key, value in message.items():\n            num_tokens += len(encoding.encode(value))\n            if key == \"name\":\n                num_tokens += tokens_per_name\n    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n    return num_tokens",
    "source": "github_repo:zhayujie/chatgpt-on-wechat",
    "file": "bot/chatgpt/chat_gpt_session.py",
    "license": "MIT",
    "language": "python"
}