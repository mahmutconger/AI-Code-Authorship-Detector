{
    "code": "def configure_optimizers(self):\n        lr = self.learning_rate\n        ae_params_list = list(self.encoder.parameters()) + list(self.decoder.parameters()) + list(\n            self.quant_conv.parameters()) + list(self.post_quant_conv.parameters())\n        if self.learn_logvar:\n            print(f\"{self.__class__.__name__}: Learning logvar\")\n            ae_params_list.append(self.loss.logvar)\n        opt_ae = torch.optim.Adam(ae_params_list,\n                                  lr=lr, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr, betas=(0.5, 0.9))\n        return [opt_ae, opt_disc], []",
    "source": "github_repo:Stability-AI/stablediffusion",
    "file": "ldm/models/autoencoder.py",
    "license": "MIT",
    "language": "python"
}