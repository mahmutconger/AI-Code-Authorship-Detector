{
    "code": "def linear(x: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, scale_fmt: Optional[str] = None) -> torch.Tensor:\n    \"\"\"\n    Applies a linear transformation to the incoming data: y = xA^T + b.\n    This function supports specialized implementations based on quantization\n    and tensor formats.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        weight (torch.Tensor): The weight tensor. It may be quantized and \n            requires dequantization for certain cases.\n        bias (Optional[torch.Tensor]): The bias tensor to be added. Default is None.\n\n    Returns:\n        torch.Tensor: The result of the linear transformation, which may involve \n        quantization-aware computations depending on the input parameters.\n\n    Notes:\n        - If `weight` is quantized (e.g., `element_size() == 1`), a dequantized version \n          is used for computation.\n        - If `gemm_impl == \"bf16\"`, dequantization and a `bf16` GEMM operation are applied.\n        - For other cases, the function applies quantization to `x` and uses `fp8_gemm` for computation.\n    \"\"\"\n    if weight.element_size() > 1:\n        return F.linear(x, weight, bias)\n    elif gemm_impl == \"bf16\":\n        weight = weight_dequant(weight, weight.scale)\n        return F.linear(x, weight, bias)\n    else:\n        x, scale = act_quant(x, block_size, scale_fmt)\n        y = fp8_gemm(x, scale, weight, weight.scale)\n        if bias is not None:\n            y += bias\n        return y",
    "source": "github_repo:deepseek-ai/DeepSeek-V3",
    "file": "inference/model.py",
    "license": "MIT",
    "language": "python"
}