{
    "code": "def audio_callback(self, indata: np.ndarray, outdata: np.ndarray, frames, times, status):\n        start_time = time.perf_counter()\n        indata = librosa.to_mono(indata.T)\n        if self.gui_config.threhold > -60:\n            indata = np.append(self.rms_buffer, indata)\n            rms = librosa.feature.rms(y=indata, frame_length=4 * self.zc, hop_length=self.zc)[:, 2:]\n            self.rms_buffer[:] = indata[-4 * self.zc :]\n            indata = indata[2 * self.zc - self.zc // 2 :]\n            db_threhold = (\n                librosa.amplitude_to_db(rms, ref=1.0)[0] < self.gui_config.threhold\n            )\n            for i in range(db_threhold.shape[0]):\n                if db_threhold[i]:\n                    indata[i * self.zc : (i + 1) * self.zc] = 0\n            indata = indata[self.zc // 2 :]\n        self.input_wav[: -self.block_frame] = self.input_wav[self.block_frame :].clone()\n        self.input_wav[-indata.shape[0] :] = torch.from_numpy(indata).to(self.config.device)\n        self.input_wav_res[: -self.block_frame_16k] = self.input_wav_res[self.block_frame_16k :].clone()\n        # input noise reduction and resampling\n        if self.gui_config.I_noise_reduce:\n            self.input_wav_denoise[: -self.block_frame] = self.input_wav_denoise[self.block_frame :].clone()\n            input_wav = self.input_wav[-self.sola_buffer_frame - self.block_frame :]\n            input_wav = self.tg(input_wav.unsqueeze(0), self.input_wav.unsqueeze(0)).squeeze(0)\n            input_wav[: self.sola_buffer_frame] *= self.fade_in_window\n            input_wav[: self.sola_buffer_frame] += self.nr_buffer * self.fade_out_window\n            self.input_wav_denoise[-self.block_frame :] = input_wav[: self.block_frame]\n            self.nr_buffer[:] = input_wav[self.block_frame :]\n            self.input_wav_res[-self.block_frame_16k - 160 :] = self.resampler(\n                self.input_wav_denoise[-self.block_frame - 2 * self.zc :]\n            )[160:]\n        else:\n            self.input_wav_res[-160 * (indata.shape[0] // self.zc + 1) :] = (\n                self.resampler(self.input_wav[-indata.shape[0] - 2 * self.zc :])[160:]\n            )\n        # infer\n        if self.function == \"vc\":\n            infer_wav = self.rvc.infer(\n                self.input_wav_res,\n                self.block_frame_16k,\n                self.skip_head,\n                self.return_length,\n                self.gui_config.f0method,\n            )\n            if self.resampler2 is not None:\n                infer_wav = self.resampler2(infer_wav)\n        elif self.gui_config.I_noise_reduce:\n            infer_wav = self.input_wav_denoise[self.extra_frame :].clone()\n        else:\n            infer_wav = self.input_wav[self.extra_frame :].clone()\n        # output noise reduction\n        if self.gui_config.O_noise_reduce and self.function == \"vc\":\n            self.output_buffer[: -self.block_frame] = self.output_buffer[self.block_frame :].clone()\n            self.output_buffer[-self.block_frame :] = infer_wav[-self.block_frame :]\n            infer_wav = self.tg(infer_wav.unsqueeze(0), self.output_buffer.unsqueeze(0)).squeeze(0)\n        # volume envelop mixing\n        if self.gui_config.rms_mix_rate < 1 and self.function == \"vc\":\n            if self.gui_config.I_noise_reduce:\n                input_wav = self.input_wav_denoise[self.extra_frame :]\n            else:\n                input_wav = self.input_wav[self.extra_frame :]\n            rms1 = librosa.feature.rms(\n                y=input_wav[: infer_wav.shape[0]].cpu().numpy(),\n                frame_length=4 * self.zc,\n                hop_length=self.zc,\n            )\n            rms1 = torch.from_numpy(rms1).to(self.config.device)\n            rms1 = F.interpolate(\n                rms1.unsqueeze(0),\n                size=infer_wav.shape[0] + 1,\n                mode=\"linear\",\n                align_corners=True,\n            )[0, 0, :-1]\n            rms2 = librosa.feature.rms(\n                y=infer_wav[:].cpu().numpy(),\n                frame_length=4 * self.zc,\n                hop_length=self.zc,\n            )\n            rms2 = torch.from_numpy(rms2).to(self.config.device)\n            rms2 = F.interpolate(\n                rms2.unsqueeze(0),\n                size=infer_wav.shape[0] + 1,\n                mode=\"linear\",\n                align_corners=True,\n            )[0, 0, :-1]\n            rms2 = torch.max(rms2, torch.zeros_like(rms2) + 1e-3)\n            infer_wav *= torch.pow(\n                rms1 / rms2, torch.tensor(1 - self.gui_config.rms_mix_rate)\n            )\n        # SOLA algorithm from https://github.com/yxlllc/DDSP-SVC\n        conv_input = infer_wav[None, None, : self.sola_buffer_frame + self.sola_search_frame]\n        cor_nom = F.conv1d(conv_input, self.sola_buffer[None, None, :])\n        cor_den = torch.sqrt(\n            F.conv1d(\n                conv_input**2,\n                torch.ones(1, 1, self.sola_buffer_frame, device=self.config.device),\n            )\n            + 1e-8\n        )\n        if sys.platform == \"darwin\":\n            _, sola_offset = torch.max(cor_nom[0, 0] / cor_den[0, 0])\n            sola_offset = sola_offset.item()\n        else:\n            sola_offset = torch.argmax(cor_nom[0, 0] / cor_den[0, 0])\n        logger.info(f\"sola_offset = {sola_offset}\")\n        infer_wav = infer_wav[sola_offset:]\n        if \"privateuseone\" in str(self.config.device) or not self.gui_config.use_pv:\n            infer_wav[: self.sola_buffer_frame] *= self.fade_in_window\n            infer_wav[: self.sola_buffer_frame] += self.sola_buffer * self.fade_out_window\n        else:\n            infer_wav[: self.sola_buffer_frame] = phase_vocoder(\n                self.sola_buffer,\n                infer_wav[: self.sola_buffer_frame],\n                self.fade_out_window,\n                self.fade_in_window,\n            )\n        self.sola_buffer[:] = infer_wav[\n            self.block_frame : self.block_frame + self.sola_buffer_frame\n        ]\n        if sys.platform == \"darwin\":\n            outdata[:] = infer_wav[: self.block_frame].cpu().numpy()[:, np.newaxis]\n        else:\n            outdata[:] = infer_wav[: self.block_frame].repeat(2, 1).t().cpu().numpy()\n        total_time = time.perf_counter() - start_time\n        logger.info(f\"Infer time: {total_time:.2f}\")",
    "source": "github_repo:RVC-Project/Retrieval-based-Voice-Conversion-WebUI",
    "file": "api_240604.py",
    "license": "MIT",
    "language": "python"
}