{
    "code": "def compute_init(device_type=\"cuda\"): # cuda|cpu|mps\n    \"\"\"Basic initialization that we keep doing over and over, so make common.\"\"\"\n\n    assert device_type in [\"cuda\", \"mps\", \"cpu\"], \"Invalid device type atm\"\n    if device_type == \"cuda\":\n        assert torch.cuda.is_available(), \"Your PyTorch installation is not configured for CUDA but device_type is 'cuda'\"\n    if device_type == \"mps\":\n        assert torch.backends.mps.is_available(), \"Your PyTorch installation is not configured for MPS but device_type is 'mps'\"\n\n    # Reproducibility\n    # Note that we set the global seeds here, but most of the code uses explicit rng objects.\n    # The only place where global rng might be used is nn.Module initialization of the model weights.\n    torch.manual_seed(42)\n    if device_type == \"cuda\":\n        torch.cuda.manual_seed(42)\n    # skipping full reproducibility for now, possibly investigate slowdown later\n    # torch.use_deterministic_algorithms(True)\n\n    # Precision\n    if device_type == \"cuda\":\n        torch.set_float32_matmul_precision(\"high\") # uses tf32 instead of fp32 for matmuls\n\n    # Distributed setup: Distributed Data Parallel (DDP), optional, and requires CUDA\n    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n    if ddp and device_type == \"cuda\":\n        device = torch.device(\"cuda\", ddp_local_rank)\n        torch.cuda.set_device(device)  # make \"cuda\" default to this device\n        dist.init_process_group(backend=\"nccl\", device_id=device)\n        dist.barrier()\n    else:\n        device = torch.device(device_type) # mps|cpu\n\n    if ddp_rank == 0:\n        logger.info(f\"Distributed world size: {ddp_world_size}\")\n\n    return ddp, ddp_rank, ddp_local_rank, ddp_world_size, device",
    "source": "github_repo:karpathy/nanochat",
    "file": "nanochat/common.py",
    "license": "MIT",
    "language": "python"
}