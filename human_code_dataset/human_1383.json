{
    "code": "class WorkflowAnalytics:\n    def __init__(self, db_path: str = \"workflows.db\"):\n        self.db_path = db_path\n    \n    def get_db_connection(self):\n        conn = sqlite3.connect(self.db_path)\n        conn.row_factory = sqlite3.Row\n        return conn\n    \n    def get_workflow_analytics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive workflow analytics.\"\"\"\n        conn = self.get_db_connection()\n        \n        # Basic statistics\n        cursor = conn.execute(\"SELECT COUNT(*) as total FROM workflows\")\n        total_workflows = cursor.fetchone()['total']\n        \n        cursor = conn.execute(\"SELECT COUNT(*) as active FROM workflows WHERE active = 1\")\n        active_workflows = cursor.fetchone()['active']\n        \n        # Trigger type distribution\n        cursor = conn.execute(\"\"\"\n            SELECT trigger_type, COUNT(*) as count \n            FROM workflows \n            GROUP BY trigger_type \n            ORDER BY count DESC\n        \"\"\")\n        trigger_distribution = {row['trigger_type']: row['count'] for row in cursor.fetchall()}\n        \n        # Complexity distribution\n        cursor = conn.execute(\"\"\"\n            SELECT complexity, COUNT(*) as count \n            FROM workflows \n            GROUP BY complexity \n            ORDER BY count DESC\n        \"\"\")\n        complexity_distribution = {row['complexity']: row['count'] for row in cursor.fetchall()}\n        \n        # Node count statistics\n        cursor = conn.execute(\"\"\"\n            SELECT \n                AVG(node_count) as avg_nodes,\n                MIN(node_count) as min_nodes,\n                MAX(node_count) as max_nodes,\n                COUNT(*) as total\n            FROM workflows\n        \"\"\")\n        node_stats = dict(cursor.fetchone())\n        \n        # Integration analysis\n        cursor = conn.execute(\"SELECT integrations FROM workflows WHERE integrations IS NOT NULL\")\n        all_integrations = []\n        for row in cursor.fetchall():\n            integrations = json.loads(row['integrations'] or '[]')\n            all_integrations.extend(integrations)\n        \n        integration_counts = Counter(all_integrations)\n        top_integrations = dict(integration_counts.most_common(10))\n        \n        # Workflow patterns\n        patterns = self.analyze_workflow_patterns(conn)\n        \n        # Recommendations\n        recommendations = self.generate_recommendations(\n            total_workflows, active_workflows, trigger_distribution, \n            complexity_distribution, top_integrations\n        )\n        \n        conn.close()\n        \n        return {\n            \"overview\": {\n                \"total_workflows\": total_workflows,\n                \"active_workflows\": active_workflows,\n                \"activation_rate\": round((active_workflows / total_workflows) * 100, 2) if total_workflows > 0 else 0,\n                \"unique_integrations\": len(integration_counts),\n                \"avg_nodes_per_workflow\": round(node_stats['avg_nodes'], 2),\n                \"most_complex_workflow\": node_stats['max_nodes']\n            },\n            \"distributions\": {\n                \"trigger_types\": trigger_distribution,\n                \"complexity_levels\": complexity_distribution,\n                \"top_integrations\": top_integrations\n            },\n            \"patterns\": patterns,\n            \"recommendations\": recommendations,\n            \"generated_at\": datetime.now().isoformat()\n        }\n    \n    def analyze_workflow_patterns(self, conn) -> Dict[str, Any]:\n        \"\"\"Analyze common workflow patterns and relationships.\"\"\"\n        # Integration co-occurrence analysis\n        cursor = conn.execute(\"\"\"\n            SELECT name, integrations, trigger_type, complexity, node_count\n            FROM workflows \n            WHERE integrations IS NOT NULL\n        \"\"\")\n        \n        integration_pairs = defaultdict(int)\n        service_categories = defaultdict(int)\n        \n        for row in cursor.fetchall():\n            integrations = json.loads(row['integrations'] or '[]')\n            \n            # Count service categories\n            for integration in integrations:\n                category = self.categorize_service(integration)\n                service_categories[category] += 1\n            \n            # Find integration pairs\n            for i in range(len(integrations)):\n                for j in range(i + 1, len(integrations)):\n                    pair = tuple(sorted([integrations[i], integrations[j]]))\n                    integration_pairs[pair] += 1\n        \n        # Most common integration pairs\n        top_pairs = dict(Counter(integration_pairs).most_common(5))\n        \n        # Workflow complexity patterns\n        cursor = conn.execute(\"\"\"\n            SELECT \n                trigger_type,\n                complexity,\n                AVG(node_count) as avg_nodes,\n                COUNT(*) as count\n            FROM workflows \n            GROUP BY trigger_type, complexity\n            ORDER BY count DESC\n        \"\"\")\n        \n        complexity_patterns = []\n        for row in cursor.fetchall():\n            complexity_patterns.append({\n                \"trigger_type\": row['trigger_type'],\n                \"complexity\": row['complexity'],\n                \"avg_nodes\": round(row['avg_nodes'], 2),\n                \"frequency\": row['count']\n            })\n        \n        return {\n            \"integration_pairs\": top_pairs,\n            \"service_categories\": dict(service_categories),\n            \"complexity_patterns\": complexity_patterns[:10]\n        }\n    \n    def categorize_service(self, service: str) -> str:\n        \"\"\"Categorize a service into a broader category.\"\"\"\n        service_lower = service.lower()\n        \n        if any(word in service_lower for word in ['slack', 'telegram', 'discord', 'whatsapp']):\n            return \"Communication\"\n        elif any(word in service_lower for word in ['openai', 'ai', 'chat', 'gpt']):\n            return \"AI/ML\"\n        elif any(word in service_lower for word in ['google', 'microsoft', 'office']):\n            return \"Productivity\"\n        elif any(word in service_lower for word in ['shopify', 'woocommerce', 'stripe']):\n            return \"E-commerce\"\n        elif any(word in service_lower for word in ['airtable', 'notion', 'database']):\n            return \"Data Management\"\n        elif any(word in service_lower for word in ['twitter', 'facebook', 'instagram']):\n            return \"Social Media\"\n        else:\n            return \"Other\"\n    \n    def generate_recommendations(self, total: int, active: int, triggers: Dict, \n                               complexity: Dict, integrations: Dict) -> List[str]:\n        \"\"\"Generate actionable recommendations based on analytics.\"\"\"\n        recommendations = []\n        \n        # Activation rate recommendations\n        activation_rate = (active / total) * 100 if total > 0 else 0\n        if activation_rate < 20:\n            recommendations.append(\n                f\"Low activation rate ({activation_rate:.1f}%). Consider reviewing inactive workflows \"\n                \"and updating them for current use cases.\"\n            )\n        elif activation_rate > 80:\n            recommendations.append(\n                f\"High activation rate ({activation_rate:.1f}%)! Your workflows are well-maintained. \"\n                \"Consider documenting successful patterns for team sharing.\"\n            )\n        \n        # Trigger type recommendations\n        webhook_count = triggers.get('Webhook', 0)\n        scheduled_count = triggers.get('Scheduled', 0)\n        \n        if webhook_count > scheduled_count * 2:\n            recommendations.append(\n                \"You have many webhook-triggered workflows. Consider adding scheduled workflows \"\n                \"for data synchronization and maintenance tasks.\"\n            )\n        elif scheduled_count > webhook_count * 2:\n            recommendations.append(\n                \"You have many scheduled workflows. Consider adding webhook-triggered workflows \"\n                \"for real-time integrations and event-driven automation.\"\n            )\n        \n        # Integration recommendations\n        if 'OpenAI' in integrations and integrations['OpenAI'] > 5:\n            recommendations.append(\n                \"You're using OpenAI extensively. Consider creating AI workflow templates \"\n                \"for common use cases like content generation and data analysis.\"\n            )\n        \n        if 'Slack' in integrations and 'Telegram' in integrations:\n            recommendations.append(\n                \"You're using multiple communication platforms. Consider creating unified \"\n                \"notification workflows that can send to multiple channels.\"\n            )\n        \n        # Complexity recommendations\n        high_complexity = complexity.get('high', 0)\n        if high_complexity > total * 0.3:\n            recommendations.append(\n                \"You have many high-complexity workflows. Consider breaking them down into \"\n                \"smaller, reusable components for better maintainability.\"\n            )\n        \n        return recommendations\n    \n    def get_trend_analysis(self, days: int = 30) -> Dict[str, Any]:\n        \"\"\"Analyze trends over time (simulated for demo).\"\"\"\n        # In a real implementation, this would analyze historical data\n        return {\n            \"workflow_growth\": {\n                \"daily_average\": 2.3,\n                \"growth_rate\": 15.2,\n                \"trend\": \"increasing\"\n            },\n            \"popular_integrations\": {\n                \"trending_up\": [\"OpenAI\", \"Slack\", \"Google Sheets\"],\n                \"trending_down\": [\"Twitter\", \"Facebook\"],\n                \"stable\": [\"Telegram\", \"Airtable\"]\n            },\n            \"complexity_trends\": {\n                \"average_nodes\": 12.5,\n                \"complexity_increase\": 8.3,\n                \"automation_maturity\": \"intermediate\"\n            }\n        }\n    \n    def get_usage_insights(self) -> Dict[str, Any]:\n        \"\"\"Get usage insights and patterns.\"\"\"\n        conn = self.get_db_connection()\n        \n        # Active vs inactive analysis\n        cursor = conn.execute(\"\"\"\n            SELECT \n                trigger_type,\n                complexity,\n                COUNT(*) as total,\n                SUM(active) as active_count\n            FROM workflows \n            GROUP BY trigger_type, complexity\n        \"\"\")\n        \n        usage_patterns = []\n        for row in cursor.fetchall():\n            activation_rate = (row['active_count'] / row['total']) * 100 if row['total'] > 0 else 0\n            usage_patterns.append({\n                \"trigger_type\": row['trigger_type'],\n                \"complexity\": row['complexity'],\n                \"total_workflows\": row['total'],\n                \"active_workflows\": row['active_count'],\n                \"activation_rate\": round(activation_rate, 2)\n            })\n        \n        # Most effective patterns\n        effective_patterns = sorted(usage_patterns, key=lambda x: x['activation_rate'], reverse=True)[:5]\n        \n        conn.close()\n        \n        return {\n            \"usage_patterns\": usage_patterns,\n            \"most_effective_patterns\": effective_patterns,\n            \"insights\": [\n                \"Webhook-triggered workflows have higher activation rates\",\n                \"Medium complexity workflows are most commonly used\",\n                \"AI-powered workflows show increasing adoption\",\n                \"Communication integrations are most popular\"\n            ]\n        }",
    "source": "github_repo:Zie619/n8n-workflows",
    "file": "src/analytics_engine.py",
    "license": "MIT",
    "language": "python"
}