{
    "code": "class ModelArgs:\n    \"\"\"\n    Data class for defining model arguments and hyperparameters.\n\n    Attributes:\n        max_batch_size (int): Maximum batch size.\n        max_seq_len (int): Maximum sequence length.\n        dtype (Literal[\"bf16\", \"fp8\"]): Data type for computations.\n        scale_fmt (Optional[str]): Format for quantization scale.\n        vocab_size (int): Vocabulary size.\n        dim (int): Model dimension.\n        inter_dim (int): Intermediate dimension for MLP layers.\n        moe_inter_dim (int): Intermediate dimension for MoE layers.\n        n_layers (int): Number of transformer layers.\n        n_dense_layers (int): Number of dense layers in the model.\n        n_heads (int): Number of attention heads.\n        n_routed_experts (int): Number of routed experts for MoE layers.\n        n_shared_experts (int): Number of shared experts for MoE layers.\n        n_activated_experts (int): Number of activated experts in MoE layers.\n        n_expert_groups (int): Number of expert groups.\n        n_limited_groups (int): Number of limited groups for MoE routing.\n        score_func (Literal[\"softmax\", \"sigmoid\"]): Scoring function for MoE routing.\n        route_scale (float): Scaling factor for routing scores.\n        q_lora_rank (int): LoRA rank for query projections.\n        kv_lora_rank (int): LoRA rank for key-value projections.\n        qk_nope_head_dim (int): Dimension for query-key projections without positional embeddings.\n        qk_rope_head_dim (int): Dimension for query-key projections with rotary embeddings.\n        v_head_dim (int): Dimension for value projections.\n        original_seq_len (int): Original sequence length.\n        rope_theta (float): Base for rotary positional encoding.\n        rope_factor (float): Scaling factor for extended sequence lengths.\n        beta_fast (int): Fast beta correction factor.\n        beta_slow (int): Slow beta correction factor.\n        mscale (float): Scaling factor for extended attention.\n    \"\"\"\n    max_batch_size: int = 8\n    max_seq_len: int = 4096 * 4\n    dtype: Literal[\"bf16\", \"fp8\"] = \"bf16\"\n    scale_fmt: Optional[str] = None\n    vocab_size: int = 102400\n    dim: int = 2048\n    inter_dim: int = 10944\n    moe_inter_dim: int = 1408\n    n_layers: int = 27\n    n_dense_layers: int = 1\n    n_heads: int = 16\n    # moe\n    n_routed_experts: int = 64\n    n_shared_experts: int = 2\n    n_activated_experts: int = 6\n    n_expert_groups: int = 1\n    n_limited_groups: int = 1\n    score_func: Literal[\"softmax\", \"sigmoid\"] = \"softmax\"\n    route_scale: float = 1.\n    # mla\n    q_lora_rank: int = 0\n    kv_lora_rank: int = 512\n    qk_nope_head_dim: int = 128\n    qk_rope_head_dim: int = 64\n    v_head_dim: int = 128\n    # yarn\n    original_seq_len: int = 4096\n    rope_theta: float = 10000.0\n    rope_factor: float = 40\n    beta_fast: int = 32\n    beta_slow: int = 1\n    mscale: float = 1.",
    "source": "github_repo:deepseek-ai/DeepSeek-V3",
    "file": "inference/model.py",
    "license": "MIT",
    "language": "python"
}