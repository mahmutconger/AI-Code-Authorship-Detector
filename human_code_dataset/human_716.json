{
    "code": "class Configs(SimpleTrainValidConfigs):\n    \"\"\"\n    Configurations with a\n     [simple training loop](../../helpers/trainer.html)\n    \"\"\"\n\n    # Number of epochs\n    epochs: int = 100\n    # Number of batches per epoch\n    n_batches: int = 500\n    # Batch size\n    batch_size: int = 128\n\n    # Model\n    model: ParityPonderGRU\n\n    # $L_{Rec}$\n    loss_rec: ReconstructionLoss\n    # $L_{Reg}$\n    loss_reg: RegularizationLoss\n\n    # The number of elements in the input vector.\n    # *We keep it low for demonstration; otherwise, training takes a lot of time.\n    # Although the parity task seems simple, figuring out the pattern by looking at samples\n    # is quite hard.*\n    n_elems: int = 8\n    # Number of units in the hidden layer (state)\n    n_hidden: int = 64\n    # Maximum number of steps $N$\n    max_steps: int = 20\n\n    # $\\lambda_p$ for the geometric distribution $p_G(\\lambda_p)$\n    lambda_p: float = 0.2\n    # Regularization loss $L_{Reg}$ coefficient $\\beta$\n    beta: float = 0.01\n\n    # Gradient clipping by norm\n    grad_norm_clip: float = 1.0\n\n    # Training and validation loaders\n    train_loader: DataLoader\n    valid_loader: DataLoader\n\n    # Accuracy calculator\n    accuracy = AccuracyDirect()\n\n    def init(self):\n        # Print indicators to screen\n        tracker.set_scalar('loss.*', True)\n        tracker.set_scalar('loss_reg.*', True)\n        tracker.set_scalar('accuracy.*', True)\n        tracker.set_scalar('steps.*', True)\n\n        # We need to set the metrics to calculate them for the epoch for training and validation\n        self.state_modules = [self.accuracy]\n\n        # Initialize the model\n        self.model = ParityPonderGRU(self.n_elems, self.n_hidden, self.max_steps).to(self.device)\n        # $L_{Rec}$\n        self.loss_rec = ReconstructionLoss(nn.BCEWithLogitsLoss(reduction='none')).to(self.device)\n        # $L_{Reg}$\n        self.loss_reg = RegularizationLoss(self.lambda_p, self.max_steps).to(self.device)\n\n        # Training and validation loaders\n        self.train_loader = DataLoader(ParityDataset(self.batch_size * self.n_batches, self.n_elems),\n                                       batch_size=self.batch_size)\n        self.valid_loader = DataLoader(ParityDataset(self.batch_size * 32, self.n_elems),\n                                       batch_size=self.batch_size)\n\n    def step(self, batch: Any, batch_idx: BatchIndex):\n        \"\"\"\n        This method gets called by the trainer for each batch\n        \"\"\"\n        # Set the model mode\n        self.model.train(self.mode.is_train)\n\n        # Get the input and labels and move them to the model's device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Increment step in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Run the model\n        p, y_hat, p_sampled, y_hat_sampled = self.model(data)\n\n        # Calculate the reconstruction loss\n        loss_rec = self.loss_rec(p, y_hat, target.to(torch.float))\n        tracker.add(\"loss.\", loss_rec)\n\n        # Calculate the regularization loss\n        loss_reg = self.loss_reg(p)\n        tracker.add(\"loss_reg.\", loss_reg)\n\n        # $L = L_{Rec} + \\beta L_{Reg}$\n        loss = loss_rec + self.beta * loss_reg\n\n        # Calculate the expected number of steps taken\n        steps = torch.arange(1, p.shape[0] + 1, device=p.device)\n        expected_steps = (p * steps[:, None]).sum(dim=0)\n        tracker.add(\"steps.\", expected_steps)\n\n        # Call accuracy metric\n        self.accuracy(y_hat_sampled > 0, target)\n\n        if self.mode.is_train:\n            # Compute gradients\n            loss.backward()\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n            # Optimizer\n            self.optimizer.step()\n            # Clear gradients\n            self.optimizer.zero_grad()\n            #\n            tracker.save()",
    "source": "github_repo:labmlai/annotated_deep_learning_paper_implementations",
    "file": "labml_nn/adaptive_computation/ponder_net/experiment.py",
    "license": "MIT",
    "language": "python"
}