# ğŸ¤– Yapay Zeka Kod YazarlÄ±ÄŸÄ± Tespit AracÄ±

## ğŸ¯ Proje AmacÄ±

Bu dÃ¶nem projesi, verilen bir kod parÃ§asÄ±nÄ±n (snippet) bir **Ä°nsan GeliÅŸtirici** tarafÄ±ndan mÄ± yoksa bir **Yapay Zeka (LLM)** asistanÄ± tarafÄ±ndan mÄ± yazÄ±ldÄ±ÄŸÄ±nÄ± belirlemek Ã¼zere tasarlanmÄ±ÅŸ bir Makine Ã–ÄŸrenimi (ML) sÄ±nÄ±flandÄ±rma modeli uygulamaktadÄ±r. Temel amaÃ§, yapay zeka tarafÄ±ndan Ã¼retilen kodlarÄ±n yaygÄ±nlaÅŸmasÄ±yla ortaya Ã§Ä±kan otantiklik sorununa Ã§Ã¶zÃ¼m getirmektir.

---

## âœ¨ Temel Ã–zellikler

* **Kod GiriÅŸi:** Web uygulamasÄ± arayÃ¼zÃ¼ndeki bir metin alanÄ± aracÄ±lÄ±ÄŸÄ±yla kod parÃ§alarÄ±nÄ± kabul eder.
* **Ã‡oklu Model Tahmini:** Proje gereksinimine uygun olarak, Ã¼Ã§ farklÄ± ML algoritmasÄ±ndan (**Naive Bayes, SVM, Random Forest**) elde edilen oransal olasÄ±lÄ±k skorlarÄ±nÄ± (% Yapay Zeka / % Ä°nsan) kullanÄ±cÄ±ya sunar.
* **Ã–zel Veri Seti:** Model, hazÄ±r veri seti kullanmama kÄ±sÄ±tlamasÄ±na uyarak, sÄ±fÄ±rdan oluÅŸturulmuÅŸ 5.000 (2.500 Ä°nsan, 2.500 Yapay Zeka) kod Ã¶rneÄŸi Ã¼zerinde eÄŸitilmiÅŸtir.
* **YÃ¼ksek Kaliteli Veri AyrÄ±ÅŸtÄ±rma:** Ä°nsan kodunu fonksiyon/sÄ±nÄ±f seviyesinde parÃ§alamak iÃ§in **Python'Ä±n AST (Abstract Syntax Tree) modÃ¼lÃ¼** kullanÄ±lmÄ±ÅŸtÄ±r.

---

## ğŸ—ƒï¸ Ã–zel Veri Seti OluÅŸturma (5.000 Ã–rnek)

Proje isterlerine uygun olarak, veri setimiz tamamen Ã¶zel scriptler kullanÄ±larak toplanmÄ±ÅŸ ve etiketlenmiÅŸtir.

* **Ä°nsan YazÄ±mÄ± Kod (2.500 Ã–rnek):**
    * **Kaynak:** PopÃ¼ler, aÃ§Ä±k kaynaklÄ± **MIT LisanslÄ± GitHub depolarÄ±ndan** Ã§ekilmiÅŸtir.
    * **Toplama YÃ¶ntemi:** GitHub API kullanÄ±larak depolardaki `.py` uzantÄ±lÄ± dosyalar alÄ±nmÄ±ÅŸ ve kod bloklarÄ±na (**fonksiyon/sÄ±nÄ±f**) ayrÄ±lmÄ±ÅŸtÄ±r.
* **Yapay Zeka YazÄ±mÄ± Kod (2.500 Ã–rnek):**
    * **Kaynak:** Gemini gibi gÃ¼Ã§lÃ¼ bir BÃ¼yÃ¼k Dil Modeli (LLM) kullanÄ±larak Ã¼retilmiÅŸtir.
    * **Ãœretim YÃ¶ntemi:** Ã‡eÅŸitli algoritmalar, veri yapÄ±larÄ± ve iÅŸlevsel gÃ¶revleri kapsayan geniÅŸ bir prompt (istem) yelpazesi kullanÄ±lmÄ±ÅŸtÄ±r.

**Veri FormatÄ±:** TÃ¼m Ã¶rnekler, kod iÃ§eriÄŸi, kaynak, lisans ve dil gibi meta verileri iÃ§eren JSON formatÄ±nda kaydedilmiÅŸtir.

---

## âš™ï¸ KullanÄ±lan Teknolojiler

* **Programlama Dili:** Python
* **Veri Toplama & Ã–n Ä°ÅŸleme:** Python (`requests`, `ast`, `json`, `base64`)
* **Makine Ã–ÄŸrenimi (Modeller):** Scikit-learn (Naive Bayes, Support Vector Machines, Random Forest)

---

## ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

### Ã–n KoÅŸullar

1.  Python 3.x yÃ¼klÃ¼ olmalÄ±dÄ±r.
2.  Bir Sanal Ortam (`venv`) oluÅŸturulmalÄ±dÄ±r.
3.  GitHub Personal Access Token (PAT) gereklidir.

### Projeyi BaÅŸlatma AdÄ±mlarÄ±

1.  **BaÄŸÄ±mlÄ±lÄ±klarÄ± Kurma:**
    ```bash
    pip install requests scikit-learn [Web Ã‡erÃ§evesi AdÄ±, Ã¶rn: flask]
    ```

2.  **Veri Toplama:** Proje klasÃ¶rÃ¼ndeki `data_collector.py` scriptini Ã§alÄ±ÅŸtÄ±rÄ±n. (Token'Ä±nÄ±zÄ±n doÄŸru ayarlandÄ±ÄŸÄ±ndan emin olun.)
    ```bash
    python data_collector.py
    ```


3.  **Model EÄŸitimi:** Model eÄŸitim scriptini Ã§alÄ±ÅŸtÄ±rÄ±n.
    ```bash
    python model_trainer.py
    ```

4.  **Web UygulamasÄ±nÄ± BaÅŸlatma:** Ana uygulama dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n.
    ```bash
    python app.py  # veya main.py
    ```

---

## ğŸ¤ KatkÄ±da Bulunma ve Lisans

Bu proje, Makine Ã–ÄŸrenmesi dersi iÃ§in bir dÃ¶nem projesi olarak geliÅŸtirilmiÅŸtir.

**Lisans:** MIT LisansÄ±

---
---

# ğŸ¤– AI Code Authorship Detector

### ğŸ¯ Project Goal

This project implements a Machine Learning (ML) classification model designed to determine the authorship of a given code snippet. The primary goal is to distinguish between code written by a Human developer and code generated by a Large Language Model (LLM) such as Gemini, GPT, or Copilot.

---

### âœ¨ Features

* **Code Input:** Accepts code snippets via a text field in a web application.
* **Multi-Model Prediction:** Provides proportional likelihood scores (e.g., 65% AI / 35% Human) from **three distinct ML algorithms** (e.g., Naive Bayes, SVM, Random Forest).
* **Custom Data Collection:** The model is trained on a meticulously curated dataset of 5,000 code samples (2,500 Human, 2,500 AI), manually collected to comply with project requirements and avoid public dataset penalties.

---

### ğŸ—ƒï¸ Custom Dataset Collection

To ensure the integrity and originality required by the project specifications, the dataset was collected and labeled entirely from scratch.

* **Human-Authored Code (2,500 samples):** Collected from popular, open-source **MIT-licensed GitHub repositories** using the GitHub API and parsed into meaningful code blocks (functions/classes) using Python's **Abstract Syntax Tree (AST)** module.
* **AI-Generated Code (2,500 samples):** Generated using a powerful LLM (e.g., Gemini) by prompting it with a diverse array of coding problems (algorithms, utility functions, bug fixes).

**Data Format:** All samples are stored in JSON format with metadata (code, source, license, language).

---

### âš™ï¸ Technology Stack

* **Programming Language:** Python
* **Data Collection & Preprocessing:** Python (`requests`, `ast`, `json`, `base64`)
* **Machine Learning (Models):** Scikit-learn (Naive Bayes, Support Vector Machines, Random Forest)

---

### ğŸš€ Getting Started

#### Prerequisites

1.  Python 3.x
2.  A virtual environment (`venv`)
3.  Install dependencies: `pip install requests scikit-learn [Your Web Framework]`

#### Running the Project

1.  **Data Collection:** Run the provided `data_collector.py` script to generate the 5,000 code samples (Ensure your GitHub Personal Access Token is configured).
3.  **Model Training:** Run the training script (`model_trainer.py`) to train the three ML classifiers.
4.  **Launch Web App:** Run the main application file to start the web service.

```bash
python app.py  # or main.py
